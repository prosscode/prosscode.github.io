<!DOCTYPE HTML><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>HDFS核心API编程案例 | 博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="author" content="pross"><meta name="description" content="删除HDFS集群中所有的空文件和空目录
使用流的方式上传下载文件
统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比
统计出HDFS文件系统中平均副本数"><meta name="description" content="删除HDFS集群中所有的空文件和空目录 使用流的方式上传下载文件 统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比 统计出HDFS文件系统中平均副本数"><meta property="og:type" content="article"><meta property="og:title" content="HDFS核心API编程案例"><meta property="og:url" content="https://pross.space/blog/2018/hdfs-core-api-programming-case/index.html"><meta property="og:site_name" content="博客"><meta property="og:description" content="删除HDFS集群中所有的空文件和空目录 使用流的方式上传下载文件 统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比 统计出HDFS文件系统中平均副本数"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2018-04-10T14:57:38.000Z"><meta property="article:modified_time" content="2021-12-05T03:13:46.774Z"><meta property="article:author" content="pross"><meta property="article:tag" content="技术"><meta property="article:tag" content="Hadoop"><meta name="twitter:card" content="summary"><link rel="alternate" href="/atom.xml" title="博客" type="application/atom+xml"><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><script>let HEXO_MMEDIA_DATA={js:[],css:[],aplayerData:[],metingData:[],artPlayerData:[],dplayerData:[]}</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="wrapper"><header id="header"><div class="title"><h1><a href="/">博客</a></h1><p><a href="/"></a></p></div><nav class="nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li><li><a href="/about">About</a></li><li><a href="/daysmatter">Daysmatter</a></li><li><a href="/atom.xml">RSS</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div class="content"><article class="post"><header><div class="icon"></div><a href="/blog/2018/hdfs-core-api-programming-case/"><time datetime="2018-04-10T14:57:38.000Z">2018-04-10</time></a><h1 class="title">HDFS核心API编程案例</h1></header><div class="entry"><ul><li>删除HDFS集群中所有的空文件和空目录</li><li>使用流的方式上传下载文件</li><li>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</li><li>统计出HDFS文件系统中平均副本数</li></ul><span id="more"></span><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p><a target="_blank" rel="noopener" href="https://prosscode.github.io/2018/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">Hadoop集群环境搭建</a>–&gt;将”windows平台编译hadoop安装包”解压，并配置环境变量–&gt;准备hadoop-eclipse-plugin.jar插件，配置到eclipse–&gt;eclipse中进入Map/Reduce Locations配置集群信息–&gt;Add User Library–&gt;添加common、hdfs、mapreduce、yarn相关依赖库–&gt;新建Java项目开始编写代码</p><p>做那么多操作，无非是要做到<strong>在本地eclipse中编写的程序能够操作HDFS集群中的文件</strong></p><h4 id="公共工具类"><a href="#公共工具类" class="headerlink" title="公共工具类"></a>公共工具类</h4><p><code>HDFSUtils.java</code>：初始化FileSystem对象和关闭FileSystem</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * create time：2018年3月14日</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * content：初始化FileSystem对象和关闭FileSystem</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSUtils</span> </span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> FileSystem fs=<span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 初始化FileSystem对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> Exception </span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">		Configuration conf=<span class="keyword">new</span> Configuration();</span><br><span class="line">		conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://hadoop02:9000&quot;</span>);</span><br><span class="line">		System.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;hadoop&quot;</span>);</span><br><span class="line">		fs=FileSystem.get(conf);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 关闭FileSystem的连接</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所需要的依赖包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br></pre></td></tr></table></figure><h4 id="删除HDFS集群中所有空文件和空目录"><a href="#删除HDFS集群中所有空文件和空目录" class="headerlink" title="删除HDFS集群中所有空文件和空目录"></a>删除HDFS集群中所有空文件和空目录</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 删除HDFS集群中的所有空文件和空目录</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteEmptyDir</span><span class="params">(Path path)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	HDFSUtils.initFileSystem();</span><br><span class="line">	<span class="comment">// 当前路径就是空目录时</span></span><br><span class="line">	FileStatus[] listFile = HDFSUtils.fs.listStatus(path);</span><br><span class="line">	<span class="keyword">if</span> (listFile.length == <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="comment">//删除空目录</span></span><br><span class="line">		HDFSUtils.fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//如果不是空文件，先获取指定目录下的文件和子目录</span></span><br><span class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = HDFSUtils.fs.listLocatedStatus(path);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> (listLocatedStatus.hasNext()) &#123;</span><br><span class="line">		LocatedFileStatus next = listLocatedStatus.next();</span><br><span class="line">         	 <span class="comment">//获取当前目录和其父目录</span></span><br><span class="line">		Path currentPath = next.getPath();</span><br><span class="line">		Path parentPath=next.getPath().getParent();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 如果是文件夹，继续往下遍历</span></span><br><span class="line">		<span class="keyword">if</span> (next.isDirectory()) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 如果是空目录，删除</span></span><br><span class="line">			<span class="keyword">if</span> (HDFSUtils.fs.listStatus(currentPath).length == <span class="number">0</span>) &#123;</span><br><span class="line">				HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</span><br><span class="line">				<span class="keyword">if</span>(HDFSUtils.fs.listStatus(parentPath).length==<span class="number">0</span>)&#123;</span><br><span class="line">					HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="comment">// 不是空目录，那么则重新遍历</span></span><br><span class="line">				<span class="keyword">if</span> (HDFSUtils.fs.exists(currentPath)) &#123;</span><br><span class="line">					AchieveClass.deleteEmptyDir(currentPath);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 如果是文件</span></span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 获取文件的长度</span></span><br><span class="line">			<span class="keyword">long</span> fileLength = next.getLen();</span><br><span class="line">			<span class="comment">// 当文件是空文件时， 删除</span></span><br><span class="line">			<span class="keyword">if</span> (fileLength == <span class="number">0</span>) &#123;</span><br><span class="line">				HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹，这里需要判断一下</span></span><br><span class="line">		<span class="keyword">int</span> length = HDFSUtils.fs.listStatus(parentPath).length;</span><br><span class="line">		<span class="keyword">if</span>(length == <span class="number">0</span>)&#123;</span><br><span class="line">			HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">     	HDFSUtils.closeFileSystem();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用流的方式上传下载文件"><a href="#使用流的方式上传下载文件" class="headerlink" title="使用流的方式上传下载文件"></a>使用流的方式上传下载文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用流的方式上传文件</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath  上传的本地路径</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> desPath  上传到HDFS上后的文件名称路径</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putFileByStream</span><span class="params">(String srcPath,String desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">	HDFSUtils.initFileSystem();</span><br><span class="line">	InputStream in = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(srcPath));</span><br><span class="line">     	 <span class="comment">//Path是HDFS上的文件路径</span></span><br><span class="line">	FSDataOutputStream out = HDFSUtils.fs.create(<span class="keyword">new</span> Path(desPath));</span><br><span class="line">	IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</span><br><span class="line">	System.out.println(<span class="string">&quot;put successfully&quot;</span>);</span><br><span class="line">	HDFSUtils.closeFileSystem();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用流的方式下载文件 </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath HDFS上的下载文件的路径</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> desPath 下载到本地的文件路径</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getFileByStream</span><span class="params">(Path srcPath,File desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">	HDFSUtils.initFileSystem();</span><br><span class="line">	FSDataInputStream in=HDFSUtils.fs.open(srcPath);</span><br><span class="line">	OutputStream out=<span class="keyword">new</span> FileOutputStream(desPath);</span><br><span class="line">	IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</span><br><span class="line">	HDFSUtils.closeFileSystem();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"><a href="#统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比" class="headerlink" title="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"></a>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 统计出 HDFS文件系统中文件大小小于 HDFS集群中的默认块大小的文件占比 </span></span><br><span class="line"><span class="comment"> * 默认块大小为128MB</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">lessBlockSizeOfFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	HDFSUtils.initFileSystem();</span><br><span class="line">	FileStatus[] listStatus = HDFSUtils.fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">	<span class="comment">// 文件总数</span></span><br><span class="line">	<span class="keyword">int</span> count = listStatus.length;</span><br><span class="line">	<span class="comment">// 小于block大小的文件数个数</span></span><br><span class="line">	<span class="keyword">int</span> lessBlock = <span class="number">0</span>;</span><br><span class="line">     	 <span class="comment">// 遍历</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</span><br><span class="line">         	<span class="comment">//如果文件大小小于128M，这lessBlock+1</span></span><br><span class="line">		<span class="keyword">if</span> (listStatus[i].getLen() &lt;= <span class="number">134217728</span>) &#123;</span><br><span class="line">			lessBlock += <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.println(<span class="string">&quot;文件总数量为：&quot;</span> + count + <span class="string">&quot;个\n小于默认block的文件数量为：&quot;</span> + lessBlock + <span class="string">&quot;个&quot;</span> + <span class="string">&quot;\n文件大小小于默认块大小的文件占比:&quot;</span></span><br><span class="line">			+ (lessBlock*<span class="number">1D</span> / count) * <span class="number">100</span> + <span class="string">&quot;%&quot;</span>);</span><br><span class="line">	HDFSUtils.closeFileSystem();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="统计出HDFS文件系统中平均副本数"><a href="#统计出HDFS文件系统中平均副本数" class="headerlink" title="统计出HDFS文件系统中平均副本数"></a>统计出HDFS文件系统中平均副本数</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HDFS文件系统中的平均副本数（副本总数/总数据块数）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">avgRepofBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	HDFSUtils.initFileSystem();</span><br><span class="line">	<span class="comment">// 副本总数</span></span><br><span class="line">	<span class="keyword">int</span> repCount = <span class="number">0</span>;</span><br><span class="line">	<span class="comment">// 数据块总数</span></span><br><span class="line">	<span class="keyword">int</span> blockCount = <span class="number">0</span>;</span><br><span class="line">     </span><br><span class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HDFSUtils.fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">	<span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">		LocatedFileStatus next = listFiles.next();</span><br><span class="line">		<span class="keyword">int</span> BlockNum = next.getBlockLocations().length;</span><br><span class="line">		<span class="keyword">if</span> (BlockNum != <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">int</span> repNum = next.getReplication();</span><br><span class="line">			<span class="keyword">int</span> oneRepCount = BlockNum * repNum;</span><br><span class="line">			repCount += oneRepCount;</span><br><span class="line">			blockCount += BlockNum;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.println(<span class="string">&quot;副本总数：&quot;</span> + repCount + <span class="string">&quot;\n数据块总数：&quot;</span> + blockCount + <span class="string">&quot;\n平均副本数：&quot;</span> + repCount*<span class="number">1D</span> / blockCount);</span><br><span class="line">	HDFSUtils.closeFileSystem();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><footer><div class="tags"><a class="tags-none-link" href="/tags/Hadoop/" rel="tag">Hadoop</a>, <a class="tags-none-link" href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></div><div class="clearfix"></div></footer></article></div></div><footer id="footer"><div class="copyright">&copy; 2025 <a href="/">pross</a> <span>,</span> Theme by <a href="https://github.com/orderedlist" target="_blank">orderedlist</a></div><div class="clearfix"></div></footer><script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script><script src="/js/scale.fix.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script><script src="/assets/mmedia/mmedia-loader.js"></script></body></html>