<!DOCTYPE HTML><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>SparkStreaming之解析updateStateByKey | 博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="author" content="pross"><meta name="description" content="说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。
状态管理的需求举一个最简单的需求例子来解释状态（state）管理，现在有这样的一个需求：计算从数据流开始到目前为止单词出现的次数。是不是看起来很眼熟，这其实就是一个升级版的wordcount，只不过需要在每个batchInterval计算当前ba"><meta name="description" content="说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。 状态管理的需求举一个最简单的需求例子来解释状态（state）管理，现在有这样的一个需求：计算从数据流开始到目前为止单词出现的次数。是不是看起来很眼熟，这其实就是一个升级版的wordcount，只不过需要在每个batchInterval计算当前ba"><meta property="og:type" content="article"><meta property="og:title" content="SparkStreaming之解析updateStateByKey"><meta property="og:url" content="https://pross.space/blog/2019/analysis-of-sparkstreaming-updatestatebykey/index.html"><meta property="og:site_name" content="博客"><meta property="og:description" content="说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。 状态管理的需求举一个最简单的需求例子来解释状态（state）管理，现在有这样的一个需求：计算从数据流开始到目前为止单词出现的次数。是不是看起来很眼熟，这其实就是一个升级版的wordcount，只不过需要在每个batchInterval计算当前ba"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2019-10-19T10:52:15.000Z"><meta property="article:modified_time" content="2021-01-30T07:11:56.000Z"><meta property="article:author" content="pross"><meta property="article:tag" content="技术"><meta property="article:tag" content="Spark"><meta name="twitter:card" content="summary"><link rel="alternate" href="/atom.xml" title="博客" type="application/atom+xml"><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><meta name="generator" content="Hexo 5.4.0"></head><body><div class="wrapper"><header id="header"><div class="title"><h1><a href="/">博客</a></h1><p><a href="/"></a></p></div><nav class="nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li><li><a href="/about">About</a></li><li><a href="/daysmatter">Daysmatter</a></li><li><a href="/atom.xml">RSS</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div class="content"><article class="post"><header><div class="icon"></div><a href="/blog/2019/analysis-of-sparkstreaming-updatestatebykey/"><time datetime="2019-10-19T10:52:15.000Z">2019-10-19</time></a><h1 class="title">SparkStreaming之解析updateStateByKey</h1></header><div class="entry"><p>说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。</p><h4 id="状态管理的需求"><a href="#状态管理的需求" class="headerlink" title="状态管理的需求"></a>状态管理的需求</h4><p>举一个最简单的需求例子来解释状态（state）管理，现在有这样的一个需求：计算从数据流开始到目前为止单词出现的次数。是不是看起来很眼熟，这其实就是一个升级版的wordcount，只不过需要在每个batchInterval计算当前batch的单词计数，然后对各个批次的计数进行累加。每一个批次的累积的计数就是当前的一个状态值。我们需要把这个状态保存下来，和后面批次单词的计数结果来进行计算，这样我们就能不断的在历史的基础上进行次数的更新。</p><p>SparkStreaming提供了两种方法来解决这个问题：updateStateByKey和mapWithState。mapWithState是1.6版本新增的功能，官方说性能较updateStateByKey提升10倍。</p><span id="more"></span><h4 id="updateStateByKey概述"><a href="#updateStateByKey概述" class="headerlink" title="updateStateByKey概述"></a>updateStateByKey概述</h4><p>updateStateByKey，统计全局的Key的状态，就算没有数据输入，也会在每一个批次的时候返回之前的key的状态。假设5s产生一个批次的数据，那么就是5s的时候就更新一次key的值，然后返回。如果数据量又比较大，又需要不断的更新每个Key的state， 那么就一定会涉及到状态的保存和容错。所以，要使用updateStateByKey就需要设置一个checkpoint目录，开启checkpoint机制。因为key的State是在内存中维护的，如果宕机，则重启之后之前维护的状态就没有了，所以要长期保存的话则需要启用<code>checkpoint</code>，以便于恢复数据。</p><h4 id="updateStateByKey代码例子"><a href="#updateStateByKey代码例子" class="headerlink" title="updateStateByKey代码例子"></a>updateStateByKey代码例子</h4><p>现在我们来看看怎么用的，首先看一个updateStateByKey使用的简单例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Author: shawn pross</span></span><br><span class="line"><span class="comment">  * Date: 2018/06/18</span></span><br><span class="line"><span class="comment">  * Description: test updateStateByKey op</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestUpdateStateByKey</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">		conf.setAppName(<span class="string">s&quot;<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>&quot;</span>)</span><br><span class="line">		conf.setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        	* 创建context,3 second batch size</span></span><br><span class="line"><span class="comment">			* 创建一个接收器(ReceiverInputDStream),接收从机器上的端口，通过socket发过来的数据</span></span><br><span class="line"><span class="comment">			*/</span></span><br><span class="line">		<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">		<span class="keyword">val</span> bathLine = ssc.socketTextStream(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">		<span class="comment">/**</span></span><br><span class="line"><span class="comment">			* 传入updateStateByKey的函数</span></span><br><span class="line"><span class="comment">			*</span></span><br><span class="line"><span class="comment">			* 源码定义：</span></span><br><span class="line"><span class="comment">			* 	def updateStateByKey[S: ClassTag](</span></span><br><span class="line"><span class="comment">			* 	updateFunc: (Seq[V], Option[S]) =&gt; Option[S]): DStream[(K, S)] = ssc.withScope &#123;</span></span><br><span class="line"><span class="comment">			* 	updateStateByKey(updateFunc, defaultPartitioner())</span></span><br><span class="line"><span class="comment">			* &#125;</span></span><br><span class="line"><span class="comment">			*/</span></span><br><span class="line">		<span class="keyword">val</span> updateFunc = (currValues: <span class="type">Seq</span>[<span class="type">Int</span>], prevValueState: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">			<span class="comment">//通过Spark内部的reduceByKey按key规约，然后这里传入某key当前批次的Seq/List,再计算当前批次的总和</span></span><br><span class="line">			<span class="keyword">val</span> currentCount = currValues.sum</span><br><span class="line">			<span class="comment">// 已累加的值</span></span><br><span class="line">			<span class="keyword">val</span> previousCount = prevValueState.getOrElse(<span class="number">0</span>)</span><br><span class="line">			<span class="comment">// 返回累加后的结果，是一个Option[Int]类型</span></span><br><span class="line">			<span class="type">Some</span>(currentCount + previousCount)</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//先聚合成键值对的形式</span></span><br><span class="line">		bathLine.map(x=&gt;(x,<span class="number">1</span>)).updateStateByKey(updateFunc).print()</span><br><span class="line">        </span><br><span class="line">		ssc.start()</span><br><span class="line">		ssc.awaitTermination()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码很简单，注释也比较详细。其中要说明的是<code>updateStateByKey</code>的参数还有几个可选项：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    <span class="comment">//状态更新功能</span></span><br><span class="line">    updateFunc: (<span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">S</span>],</span><br><span class="line">    <span class="comment">//用于控制新RDD中每个RDD的分区的分区程序</span></span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    <span class="comment">//是否记住生成的RDD中的分区对象</span></span><br><span class="line">    rememberPartitioner: <span class="type">Boolean</span>,  </span><br><span class="line">    <span class="comment">//每个键的初始状态值</span></span><br><span class="line">    initialRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)],</span><br><span class="line">  ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="updateStateByKey源码分析"><a href="#updateStateByKey源码分析" class="headerlink" title="updateStateByKey源码分析"></a>updateStateByKey源码分析</h4><p>通过上面简单的小例子可以知道，使用updateStateByKey是需要先转换为键值对的形式的，而map返回的是<code>MappedDStream</code>，而进入<code>MappedDStream</code>中也没有updateStateByKey方法，然后其父类DStream中也没有。但是DStream的半生对象中有一个隐式的转换函数<code>toPairDStreamFunctions</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// map实现，返回MappedDStream</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(mapFunc))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MappedDStream父类是DStream</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappedDStream</span>[<span class="type">T</span>: <span class="type">ClassTag</span>, <span class="type">U</span>: <span class="type">ClassTag</span>] (<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    parent: <span class="type">DStream</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">    mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span></span></span></span><br><span class="line"><span class="params"><span class="class">  </span>) <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">U</span>](<span class="params">parent.ssc</span>) </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DStream中隐式的转换函数</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">toPairDStreamFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](stream: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">      (<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>):</span><br><span class="line">    <span class="type">PairDStreamFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">PairDStreamFunctions</span>[<span class="type">K</span>, <span class="type">V</span>](stream)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>看到<code>new PairDStreamFunctions</code>就不陌生了。<code>PairDStreamFunctions</code>中存在updateStateByKey方法，Seq[V]表示当前key对应的所有值，Option[S] 是当前key的历史状态，返回的是新的状态。也就是绕了一个圈子又回到原地。最后updateStateByKey最终会在这里面new出了一个<code>StateDStream</code>对象。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</span><br><span class="line">     updateFunc: (<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>])]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">S</span>)],</span><br><span class="line">     partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">     rememberPartitioner: <span class="type">Boolean</span>,</span><br><span class="line">     initialRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedFunc = ssc.sc.clean(updateFunc)</span><br><span class="line">   <span class="keyword">val</span> newUpdateFunc = (_: <span class="type">Time</span>, it: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>])]) =&gt; &#123;</span><br><span class="line">     cleanedFunc(it)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StateDStream</span>(self, newUpdateFunc, partitioner, rememberPartitioner, <span class="type">Some</span>(initialRDD))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>继续进去<code>StateDStream</code>看看，在其<code>compute</code>方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中<code>StateDStream</code>的父类计算出的RDD（本次batch的单词计数）分别是<code>prevStateRDD</code>和<code>parentRDD</code>，然后在调用 <code>computeUsingPreviousRDD</code> 方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> [<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">computeUsingPreviousRDD</span></span>(</span><br><span class="line">    batchTime: <span class="type">Time</span>,</span><br><span class="line">    parentRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</span><br><span class="line">    prevStateRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)]) = &#123;</span><br><span class="line">  <span class="comment">// Define the function for the mapPartition operation on cogrouped RDD;</span></span><br><span class="line">  <span class="comment">// first map the cogrouped tuple to tuples of required type,</span></span><br><span class="line">  <span class="comment">// and then apply the update function</span></span><br><span class="line">  <span class="keyword">val</span> updateFuncLocal = updateFunc</span><br><span class="line">  <span class="keyword">val</span> finalFunc = (iterator: <span class="type">Iterator</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">S</span>]))]) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> i = iterator.map &#123; t =&gt;</span><br><span class="line">      <span class="keyword">val</span> itr = t._2._2.iterator</span><br><span class="line">      <span class="keyword">val</span> headOption = <span class="keyword">if</span> (itr.hasNext) <span class="type">Some</span>(itr.next()) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">      (t._1, t._2._1.toSeq, headOption)</span><br><span class="line">    &#125;</span><br><span class="line">    updateFuncLocal(batchTime, i)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</span><br><span class="line">  <span class="keyword">val</span> stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</span><br><span class="line">  <span class="type">Some</span>(stateRDD)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后返回<code>stateRDD</code>结果。至此，updateStateByKey方法源码执行过程水落石出。</p></div><footer><div class="tags"><a class="tags-none-link" href="/tags/Spark/" rel="tag">Spark</a>, <a class="tags-none-link" href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></div><div class="clearfix"></div></footer></article></div></div><footer id="footer"><div class="copyright">&copy; 2022 <a href="/">pross</a> <span>,</span> Theme by <a href="https://github.com/orderedlist" target="_blank">orderedlist</a></div><div class="clearfix"></div></footer><script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script><script src="/js/scale.fix.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>