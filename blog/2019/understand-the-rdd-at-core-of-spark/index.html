<!DOCTYPE HTML><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>理解Spark核心之RDD | 博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="author" content="pross"><meta name="description" content="Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集）
理解RDD如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。"><meta name="description" content="Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集） 理解RDD如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。"><meta property="og:type" content="article"><meta property="og:title" content="理解Spark核心之RDD"><meta property="og:url" content="https://pross.space/blog/2019/understand-the-rdd-at-core-of-spark/index.html"><meta property="og:site_name" content="博客"><meta property="og:description" content="Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集） 理解RDD如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://pross.space/blog/2019/understand-the-rdd-at-core-of-spark/narrowAndwide.jpg"><meta property="og:image" content="https://pross.space/blog/2019/understand-the-rdd-at-core-of-spark/rdd-dependencies.png"><meta property="article:published_time" content="2019-05-29T09:04:41.000Z"><meta property="article:modified_time" content="2021-12-05T02:43:57.035Z"><meta property="article:author" content="pross"><meta property="article:tag" content="技术"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pross.space/blog/2019/understand-the-rdd-at-core-of-spark/narrowAndwide.jpg"><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="博客" type="application/atom+xml"></head><body><div class="wrapper"><header id="header"><div class="title"><h1><a href="/">博客</a></h1><p><a href="/"></a></p></div><nav class="nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li><li><a href="/about">About</a></li><li><a href="/daysmatter">Daysmatter</a></li></ul><div class="clearfix"></div></nav><div class="clearfix"></div></header><div class="content"><article class="post"><header><div class="icon"></div><a href="/blog/2019/understand-the-rdd-at-core-of-spark/"><time datetime="2019-05-29T09:04:41.000Z">2019-05-29</time></a><h1 class="title">理解Spark核心之RDD</h1></header><div class="entry"><p>Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集）</p><h4 id="理解RDD"><a href="#理解RDD" class="headerlink" title="理解RDD"></a>理解RDD</h4><p>如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。</p><p>RDD是一个容错的，并行的数据结构，可以让用户显示地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，比如Map、flatMap、filter等转换操作实现了monad模式（Monad是一种设计模式，表示将一个运算过程，通过函数拆解成互相连接的多个步骤；你只要提供下一步运算所需的函数，整个运算就会自动进行下去。），很好的切合了Scala的集合操作。另外，RDD还提供了比如join，groupBy，reduceByKey（action操作）等更为方便的操作，用来支持常见的数据运算。</p><p>RDD是一系列只读分区的集合，它只能从文件中读取并创建，或者从旧的RDD生成新的RDD。RDD的每一次变换操作都会生成新的RDD，而不是在原来的基础上进行修改，这种粗粒度的数据操作方式为RDD带来了容错和数据共享方面的优势，但是在面对大数据集中频繁的小操作的时候，显得效率比较低下。</p><span id="more"></span><h4 id="RDD原理"><a href="#RDD原理" class="headerlink" title="RDD原理"></a>RDD原理</h4><p>RDD实际上是一个类（sc.textFile()方法返回一个RDD对象，然后用line接收这个对象），而这个RDD类中也定义了一系列的用于操作的方法，也就是一些算子操作。</p><p>这个类为了实现对数据的操作，里面有分区信息，用于记录特定RDD的分区情况；依赖关系，指向其父RDD；一个函数，用于记录父RDD到自己的转换操作；划分策略和数据位置的元数据。在DAG中这样的RDD就可以看成一个个节点，RDD中的存储的依赖关系就是DAG的边。在Spark中，数据在物理上被划分为一个个的block，这些block由blockmanager统一管理的。</p><p>在设计RDD之间的依赖关系时，设计者将RDD之间的依赖关系分为两类：窄依赖和宽依赖。RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个DataSet片段。RDD可以相互依赖，如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之位narrow dependency（窄依赖）；若多个Child RDD分区都可以依赖，则称为wide dependency（宽依赖），而join操作则会产生wide dependency。</p><p><img src="./narrowAndwide.jpg" alt="narrow和wide"></p><p><img src="./rdd-dependencies.png" alt="narrow和wide的区别"></p><blockquote><p>Spark之所以将依赖分为narrow与wide，基于以下两点原因：</p><p>narrow dependecies可以支持在同一个cluster node上，并且以管道形式执行多行命令，例如在执行了map操作后，紧接着执行filter。相反，wide dependencies需要所有的父分区都是可用的，可能还需要调用类似MapReduce之类的操作进行跨节点传递。</p><p>其次从失败恢复的角度考虑，narrow dependencies的失败恢复更有效，因为它只需要重新计算丢失的parent partition即可，而且可以并行的在不同节点进行重计算。而wide dependencies牵涉到RDD各级的多个parent partitions。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>RDD是Spark的核心，也是整个Spark架构的基础，特性总结如下：</p><ul><li>不变的数据结构存储</li><li>支持跨集群的分布式数据结构</li><li>可以根据数据记录的Key对结构进行分区</li><li>提供了粗粒度的操作，且这些操作支持分区</li><li>它将数据存储在内存中，从而提供了低延迟性</li></ul></div><footer><div class="tags"><a class="tags-none-link" href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag">技术</a></div><div class="clearfix"></div></footer></article></div></div><footer id="footer"><div class="copyright">&copy; 2021 <a href="/">pross</a> <span>,</span> Theme by <a href="https://github.com/orderedlist" target="_blank">orderedlist</a></div><div class="clearfix"></div></footer><script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script><script src="/js/scale.fix.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">jQuery(".fancybox").fancybox()</script></body></html>