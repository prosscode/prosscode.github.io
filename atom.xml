<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PROSS</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://pross.space/"/>
  <updated>2018-10-27T16:53:44.136Z</updated>
  <id>https://pross.space/</id>
  
  <author>
    <name>RukiapR0ss</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>你所忽视的排序算法（上）</title>
    <link href="https://pross.space/archives/2018/10/27/"/>
    <id>https://pross.space/archives/2018/10/27/</id>
    <published>2018-10-27T15:12:58.000Z</published>
    <updated>2018-10-27T16:53:44.136Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><blockquote><p>算法（algorithm），在数学（算学）和计算机科学之中，为任何良定义的具体计算步骤的一个序列，常用语计算，数据处理和自动推理。精确而言，算法是一个表示为有限长列表的有效方法。算法应包含清晰定义的指令用于计算函数。</p><p>——<a href="https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95" target="_blank" rel="external">维基百科</a></p></blockquote><a id="more"></a><p>程序猿圈子里似乎都默然这样的一个等式：程序=数据结构+算法。思考起来，就感觉相当于：作文=语法+词语。这句话相当出名，因为这是1986年尼古拉斯赵四（逃）在获得图灵奖时说的一句话，现在听起来，似乎没有什么不正确的。当然，这就好比当年牛顿在1687年提出万有引力一样，现在看起来是废话一样，但是当时这句话确定奠定了程序的基础概念。</p><p>所以我们就来讲讲算法，那么问题来了，什么是算法呢？喏，开头已经引用了维基百科的对算法的定义，下面接而来了一段解释：</p><blockquote><p>算法中的指令描述的是一个<a href="https://zh.wikipedia.org/wiki/%E8%A8%88%E7%AE%97" target="_blank" rel="external">计算</a>，当其运行时能从一个初始状态和初始输入（可能为空）开始，经过一系列<strong>有限</strong>而清晰定义的状态最终产生<strong>输出</strong>并<strong>停止</strong>于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入。</p></blockquote><p>看懂了吗？别急别急，没看懂没关系，我也不是很理解；不过没关系，我们用一句通俗的话来阐释一下：<strong>算法（Algorithm）就是解决问题的方法</strong>。所以我们就这样来理解就好了。</p><p>在InfoQ上有一篇文章《计算机科学最重要的32个算法》，介绍了二分查找法（Binary Search），快速傅里叶变换（Fast Fourier transform，FFT），哈希算法（Hashing），堆排序（Heaps）等等，有兴趣的可以看看。当然，我也是在学习中。今天这篇文章是算法系列的第一篇，我们先从排序算法（Sort）开始。</p><p>计划是来介绍十大常见的排序算法，为了控制文章篇幅，分上下两篇来介绍，一些算法定义解释和动态图过程分析展示大多来自于自由全面的维基百科。此文为上篇，介绍算法引入排序以及介绍部分简单的排序算法。下篇介绍剩余的排序算法和下一章预告。</p><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>排序算法大致可以分为两大类：<strong>非线性时间比较类排序和线性时间非比较类</strong>。前者是通过比较来决定元素间的次序，其时间复杂度不能突破O（nlogn）；后者不通过比较来决定元素间的次序，但是可以突破基于比较排序的时间下界，以线性时间运行。有点数学基础的各位看官很好理解线性和非线性的区别，以及过程中会提到复杂度，这里不在赘述，以后有机会单开一篇介绍。</p><h4 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h4><p>冒泡排序（Bubble Sort）是一种简单的排序算法，常作为程序设计入门算法介绍。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p><p><strong>逻辑描述</strong></p><ul><li>比较相邻的元素。如果第一个比第二个大，就交换他们两个。</li><li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。</li><li>针对所有的元素重复以上的步骤，除了最后一个。</li><li>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</li></ul><p><strong>代码实现</strong></p><p>比较常见的算法，代码略。</p><h4 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h4><p>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p><p><strong>逻辑描述</strong></p><p>N个记录的直接选择排序可经过N-1趟直接选择排序得到有序结果。</p><ul><li>初始状态：无序区为R[1…n]，有序区为空</li><li>第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R[i..n]。该趟排序从当前无序区中选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区</li><li>n-1趟结束，数组有序化了</li></ul><p><strong>代码实现</strong></p><p>比较常见的算法，代码略。</p><h4 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h4><p><strong>插入排序</strong>（Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。</p><p><strong>逻辑描述</strong></p><p>插入排序在实现上，通常采用in-place排序（即只需用到<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e66384bc40452c5452f33563fe0e27e803b0cc21" alt="{\displaystyle O(1)}">的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p><ol><li>从第一个元素开始，该元素可以认为已经被排序</li><li>取出下一个元素，在已经排序的元素序列中从后向前扫描</li><li>如果该元素（已排序）大于新元素，将该元素移到下一位置</li><li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置</li><li>将新元素插入到该位置后</li><li>重复步骤2~5</li></ol><p><strong>代码实现</strong></p><p>比较常见的算法，代码略。</p><h4 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h4><p>希尔排序（Shell Sort），也称递减增量排序算法，是插入排序的一种更高效的改进版本。</p><p><strong>逻辑描述</strong></p><p>先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述：</p><ul><li>选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1</li><li>按增量序列个数k，对序列进行k趟排序</li><li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度</li></ul><p><strong>代码实现</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.sort;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Map;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@describe</span>:</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span>:彭爽pross</span></div><div class="line"><span class="comment"> * <span class="doctag">@date</span>: 2018/10/27</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShellSort</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] shellSort(<span class="keyword">int</span>[] array) &#123;</div><div class="line"><span class="keyword">int</span> i;</div><div class="line"><span class="keyword">int</span> j;</div><div class="line"><span class="keyword">int</span> temp;</div><div class="line"><span class="comment">//自定义间隔序列</span></div><div class="line"><span class="keyword">int</span> number = array.length / <span class="number">2</span>;</div><div class="line"><span class="keyword">while</span> (number &gt;= <span class="number">1</span>) &#123;</div><div class="line"><span class="keyword">for</span> (i = number; i &lt; array.length; i++) &#123;</div><div class="line">temp = array[i];</div><div class="line">j = i - number;</div><div class="line"><span class="keyword">while</span> (j &gt;= <span class="number">0</span> &amp;&amp; array[j] &lt; temp) &#123;</div><div class="line">array[j + number] = array[j];</div><div class="line">j = j - number;</div><div class="line">&#125;</div><div class="line">array[j + number] = temp;</div><div class="line">&#125;</div><div class="line">number = number / <span class="number">2</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">return</span> array;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>算法分析</strong></p><p>希尔排序是基于插入排序的以下两点性质而提出改进方法的：1.插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率；2.但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位。</p><p>希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。</p><h4 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h4><p>归并排序（Merge sort，或mergesort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。</p><p><strong>逻辑描述</strong></p><ol><li>将序列每相邻两个数字进行归并操作，形成<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/284284713ad8f1ba13458b896c87efc4b9b7df9c" alt="{\displaystyle ceil(n/2)}">个序列，排序后每个序列包含两/一个元素</li><li>若此时序列数不是一个则将上述序列再次归并，形成<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0f7b6be8e0c402e981a78d573dc3072c3d24a3c4" alt="{\displaystyle ceil(n/4)}">个序列，每个序列包含四/三个元素</li><li>重复第2步操作，直到所有元素排序完毕，即序列数为1</li></ol><p><strong>代码实现</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.sort;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@describe</span>: 归并排序</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span>:彭爽pross</span></div><div class="line"><span class="comment"> * <span class="doctag">@date</span>: 2018/10/27</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MergeSort</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] mergeSort(<span class="keyword">int</span>[] arr) &#123;</div><div class="line"><span class="keyword">int</span>[] orderedArr = <span class="keyword">new</span> <span class="keyword">int</span>[arr.length];</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; arr.length * <span class="number">2</span>; i *= <span class="number">2</span>) &#123;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; (arr.length + i - <span class="number">1</span>) / i; j++) &#123;</div><div class="line"><span class="keyword">int</span> left = i * j;</div><div class="line"><span class="keyword">int</span> mid = left + i / <span class="number">2</span> &gt;= arr.length ? (arr.length - <span class="number">1</span>) : (left + i / <span class="number">2</span>);</div><div class="line"><span class="keyword">int</span> right = i * (j + <span class="number">1</span>) - <span class="number">1</span> &gt;= arr.length ? (arr.length - <span class="number">1</span>) : (i * (j + <span class="number">1</span>) - <span class="number">1</span>);</div><div class="line"><span class="keyword">int</span> start = left, l = left, m = mid;</div><div class="line"><span class="keyword">while</span> (l &lt; mid &amp;&amp; m &lt;= right) &#123;</div><div class="line"><span class="keyword">if</span> (arr[l] &lt; arr[m]) &#123;</div><div class="line">orderedArr[start++] = arr[l++];</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">orderedArr[start++] = arr[m++];</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">while</span> (l &lt; mid)</div><div class="line">orderedArr[start++] = arr[l++];</div><div class="line"><span class="keyword">while</span> (m &lt;= right)</div><div class="line">orderedArr[start++] = arr[m++];</div><div class="line">System.arraycopy(orderedArr, left, arr, left, right - left + <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> orderedArr;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>上篇介绍了，冒泡排序，选择排序，插入排序，希尔排序和归并排序。</p><p>所有代码可以在<a href="https://github.com/prosscode/java-learning-case/tree/master/algorithm/src/main/java/org/pross/sort" target="_blank" rel="external">我的Github</a>查阅。</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;blockquote&gt;&lt;p&gt;算法（algorithm），在数学（算学）和计算机科学之中，为任何良定义的具体计算步骤的一个序列，常用语计算，数据处理和自动推理。精确而言，算法是一个表示为有限长列表的有效方法。算法应包含清晰定义的指令用于计算函数。&lt;/p&gt;&lt;p&gt;——&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;维基百科&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://pross.space/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>相见</title>
    <link href="https://pross.space/archives/2018/09/28/"/>
    <id>https://pross.space/archives/2018/09/28/</id>
    <published>2018-09-28T13:36:25.000Z</published>
    <updated>2018-09-28T13:41:19.720Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>分享几个简短的思考和观点。</p><a id="more"></a><p><strong>关于代码</strong></p><blockquote><p>「为了理解一个简单函数的运行过程，今天的软件工程师可能需要追踪25个文件。因为每个文件都包含一个 Java 方法，它会向另一个文件的另一个方法发消息。为了方便查看20层的堆栈，人们发明了 Eclipse 那样的复杂工具。实际做事的那一行代码，埋藏在数百行胶水代码、无数个接口和其他冗余代码之下。」</p><p>– <a href="http://blogs.harvard.edu/philg/2018/09/18/is-data-scientist-the-new-programmer/" target="_blank" rel="external">Philip Greenspun</a>，麻省理工学院的计算机教授</p></blockquote><p>插嘴一句，现在还是习惯使用JetBrains家的IDEA，风格简约，爽心悦目。看到很多关于集成开发环境工具好坏的争论，Eclipse什么什么不好，IDEA哪里哪里特别方便，无聊之者，还不如老老实实把BUG解决掉，赶快回家睡觉。</p><p>由Phillip Greenspun的一段话引出，只是非常印证最近的工作。为了理解这个方法函数的功能是什么，不停的追踪发现，不停的向下挖掘，在经历层层封锁之后终于找到了真相，心里带着窃喜，还是要骂咧咧一句：原来你就在这里，原来你是这样实现的，原来你干着这样的事。然后带着些许满足感返回去，继续下一行代码的探索与发现。</p><p><strong>关于改变观点</strong></p><p>亚马逊的老板贝佐斯说，「如果一个人经常改变自己的看法，更可能得到正确的观点。」以前没有从这方面角度思考过这个问题，经常改变自己的看法和观点不是立场不确定么？你都不确定自己的观点是否确定怎么值得别人相信你？</p><p>仔细思考贝佐斯的话，还是有一点道理。如今世界变化的太快了，聪明的人都会不断的修改自己对世界的理解，重新考虑自己以前考虑的问题，不断的用汲取的新的知识，新的思维方式来完善自己的观点和看法，然后得出不一样的结论。这样说来，不是意味之前的观点就是错误的，而应该说自己的观点是暂时的，因为我们都在成长，今天的你和昨天的你都会不一样。</p><p>这样理解贝佐斯的话可能更有建造性，「如果一个人经常改变自己过去事情的看法，更可能得到正确的观点。」另外，写完这句话，我对这个观点看法已经改变了。: )</p><p><strong>关于读书</strong></p><p>池老师的<a href="https://book.douban.com/subject/26663519/" target="_blank" rel="external">MacTalk·跨越边界</a>，已经断断续续在通勤之余已经阅读完。感受还是有一些，另外开篇文章再来说一说。</p><p>我觉得读书是由点及面的成长，现在阅读的这本书为点，书中涉及到的知识，介绍的人物，作者阅读书籍为扩展的面，这是一个循环辐射学习的过程。比如我读完跨越边界，对软件项目的思考，持续学习的观点，时代的技术发展（毕竟池老师是70后程序员）都有一定的感悟和自己的理解；对高晓松的《如丧》，《鱼羊野史》，冯大辉的博客文集，一生只为欢笑的linus，传奇工程师沃兹大神都抱着浓厚的兴趣，这样就是面的展开。</p><p>读书和写博客都引用同一句话，「读书会带来很多好处，却没有任何明显的坏处。」</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;分享几个简短的思考和观点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活杂记" scheme="https://pross.space/tags/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>你需要多久才能变成一个&quot;傻瓜&quot;</title>
    <link href="https://pross.space/archives/2018/09/24/"/>
    <id>https://pross.space/archives/2018/09/24/</id>
    <published>2018-09-24T10:41:47.000Z</published>
    <updated>2018-09-24T12:35:11.190Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>金出武雄（Kanade Takeo）是美国卡内基·梅隆大学（CMU）的机器人视觉研究所所长，也是世界上最重要的计算机是视觉研究人员之一。</p><p><img src="/archives/2018/09/24/kanade_takeo_2014.jpg" alt=""></p><a id="more"></a><p>微信之父张小龙在《微信背后的产品观》一文中讲到：“产品经理要有傻瓜心态。”这里的傻瓜并不是真正的傻掉，而是需要有一种外行的心态，保持“stay foolish”。张小龙说，自己需要经过5~10分钟的酝酿才能达到傻瓜状态，马化腾需要1分钟，功力最深的乔布斯可以在专家和傻瓜之间随意切换，来去自如。</p><p>最早听到类似的说法是通过池老师在博客中介绍的金出武雄教授。他出版了一本书，名字叫做《像外行一样思考，像专家一样实践》，关于这本书的创作，金教授如是说：</p><blockquote><p>听过我的演讲或言论之后，有很多人表示：“你的话，简直就是谎话，几乎都是谎话、是玩笑、像是真话，是真话，是自吹自擂、虽然很有建设性但……杂七杂八还有点意思。”于是我想，要不要把这些收集起来，写一本书呢？</p></blockquote><p>于是这本书就诞生了，这是一个小插曲。这本书是金教授对其日常研究、生活和学习的经验进行收集整理而成的一本小册子。但其中的内容远不止这些，无论是学术、技术、产品、演讲、写作、互联网、教育、思考的本质等，书中都有所涉猎，并且观点奇特，思路新颖，适合各个创造性领域的人群阅读。</p><p><img src="/archives/2018/09/24/book.jpg" alt=""></p><p>虽然今天主题是这本书，我还没有看完这本书，但是池老师读了一遍并在<code>mactalk·跨越边界</code>上总结了，我就站在巨人的肩膀上，挑几个思考性的观点，做一下学习思考的笔记。</p><h4 id="Best-First"><a href="#Best-First" class="headerlink" title="Best First"></a>Best First</h4><p>金教授有个观点叫做”Best First“，意为最好东西一定要放在最前面。无论是演讲还是写书，金教授都遵循这个规则。观众或读者都希望开始的时候就看到最好的东西，很多演讲或图书都喜欢做一些冗长的铺垫才进入主题，岂不知那些铺垫已经耗尽了我们观众的耐心。</p><p>这个观点非常的有趣，基于此，我们就知道，书中的核心内容毫无疑问就是第一章：像外行一样思考，像专家一样实践。</p><h4 id="关于反对"><a href="#关于反对" class="headerlink" title="关于反对"></a>关于反对</h4><blockquote><p>明斯基教授，您总是能在各种领域中想出很多创造性的，引人入胜且能够引导新方向的构思。请问您的诀窍是什么呢？</p><p>他回答说：这个很简单，只要反对大家的所说的就可以了。大家都认同的好想法基本上都不太令人满意。</p></blockquote><h4 id="关于迷茫"><a href="#关于迷茫" class="headerlink" title="关于迷茫"></a>关于迷茫</h4><blockquote><p>金教授说：越能干的人越迷茫。</p><p>就算是卡内基·梅隆大学的计算机科学系和机器人研究所的博士研究生，他们出类拔萃无所不能，也避免不了这种感觉。不，应该说，正是这种人，才更容易陷入不安和迷茫。</p></blockquote><p>如果你工作时，经常在”能不能行呢？“的不安感和“啊，成功了！”的成就感之间往复行走，那么恭喜你，离成功已经没有几公里了。交织着这两种感觉的体验将成为你智慧和体力的强有力基石。这也从某种程度上解释了我为什么会经常处于一种迷茫的状态么？</p><h4 id="关于记忆力"><a href="#关于记忆力" class="headerlink" title="关于记忆力"></a>关于记忆力</h4><blockquote><p>日常生活中，对于人的知觉、思考、行动等，追本溯源，最终都会落在记忆上，如果头脑中没有知识和信息作为工具、材料，是不可能发挥规划能力和创造能力的。构思就是通过重组脑海中的记忆而产生的。如果没有良好而广博的记忆内容做基础，根本产生不了什么好的构思。因此，最有效的学习方法就是记忆。把他人长时间思考总结出来的成果记忆下来，不仅高效快捷，也能为自身的思考扩展基础。当然，这里所谓的记忆，是指”经过理解的记忆”，这一点无需多言。</p></blockquote><h4 id="关于颠覆"><a href="#关于颠覆" class="headerlink" title="关于颠覆"></a>关于颠覆</h4><blockquote><p>计算机的发展日新月异。20世纪60年代，计算机像竞赛一样指数发展，但到了20世纪80年代，发展速度减缓，甚至有人说计算机不会再进步了，还举了很多例子：硅晶体上不能画再细的线了，不能制造出更小的晶体管了，硬盘的存储密度不能再增大了，等等。根据这些说法他们得出的结论是：发展瓶颈终将到来。</p></blockquote><p>科学的进步就是不断突破极限和开辟新的领域，每个人的奋斗就是突破自己。</p><h4 id="关于不可能原则"><a href="#关于不可能原则" class="headerlink" title="关于不可能原则"></a>关于不可能原则</h4><blockquote><p>第一条：科学工作者声明某件事情是可行的时候，基本上他不会错。但当他说不可能的时候，他很可能错了。第二条：发现极限在哪里的唯一方法就是超越极限，尝试向稍微超越这个极限的领域迈进、冒险。第三条：无论是哪种技术，只要它是非常先进的，那看起来都跟魔术没什么区别。</p></blockquote><p>如果出现一个想法和创意，看起来不可能实现，实际上你可能错了。如果发现自己的所做的事情已经做到了极限，可以试试超越极限迈进一步。</p><h4 id="关于演讲和交流"><a href="#关于演讲和交流" class="headerlink" title="关于演讲和交流"></a>关于演讲和交流</h4><blockquote><p>在与他人交流的过程中完善自己的想法。无论什么样的构想，最初大都只是个偶然的想法。锤炼构想的方法就是跟他人交流，在交谈中验证是不是一个有价值的想法，并且获取相关知识，修正不完备的地方。升华构想的关键是“交流”，因为他人有很多自己角度的认识和想法，借鉴过来才能完善自己的构想。</p></blockquote><h4 id="创造的基础是模仿"><a href="#创造的基础是模仿" class="headerlink" title="创造的基础是模仿"></a>创造的基础是模仿</h4><blockquote><p>模仿、相似，这样不是很好吗？最初的想法的确是相同的，但在此基础之上添加东西、使之升华的水平高低才是决定胜负的关键。因此，大部分的创造都是在模仿的基础之上增加其附加价值的东西。独创、创造，不是无中生有的魔术。</p></blockquote><p>有一段时间自己非常厌恶<code>CV战士</code>式的工作，总想造点新的轮子或工具。想要创造出的新的东西，那一定是在原有的基础上或思维格局上进行新的改造和发现。</p><p>最后结尾引用池老师的总结。回到最初的问题，如何在专家和傻瓜之间进行自由的切换呢？其实金教授已经写在书里了：</p><p>思考的时候，要像外行一样单纯直接，实践的时候则要像专家一样严密细致，并且要有以专业知识和方法武装起来的“我做得到”的乐观主义精神。要记住，独特的、好的创意和好的结尾，不管是对研究而言，还是对商业运营而言，都不是自己突然冒出来的东西，那一定是刻苦的努力和长期的思考带来的。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;金出武雄（Kanade Takeo）是美国卡内基·梅隆大学（CMU）的机器人视觉研究所所长，也是世界上最重要的计算机是视觉研究人员之一。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/archives/2018/09/24/kanade_takeo_2014.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活杂记" scheme="https://pross.space/tags/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming之解析mapWithState</title>
    <link href="https://pross.space/archives/2018/09/10/"/>
    <id>https://pross.space/archives/2018/09/10/</id>
    <published>2018-09-10T01:56:59.000Z</published>
    <updated>2018-09-16T12:45:01.485Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>最近经历挫折教育，今天闲得时间，整理状态管理之解析mapWithState。今天说道的mapWithState是从Spark1.6开始引入的一种新的状态管理机制，支持输出全量的状态和更新的状态，支持对状态超时的管理，和自主选择需要的输出。</p><a id="more"></a><h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> stateParse</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* Author: shawn pross</span></div><div class="line"><span class="comment">* Date: 2018/09/10</span></div><div class="line"><span class="comment">* Description: </span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMapWithState</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">conf.setAppName(<span class="string">s"<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>"</span>)</div><div class="line">conf.setMaster(<span class="string">"local[2]"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">3</span>))</div><div class="line">ssc.checkpoint(<span class="string">"/checkpoint/"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> line = ssc.socketTextStream(<span class="string">"127.0.0.1"</span>,<span class="number">9999</span>)</div><div class="line"><span class="keyword">val</span> wordDStream = line.flatMap(_.split(<span class="string">","</span>)).map(x=&gt;(x,<span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment">//状态更新函数，output是输出，state是状态</span></div><div class="line"><span class="keyword">val</span> mappingFunc = (userId:<span class="type">String</span>,value:<span class="type">Option</span>[<span class="type">Int</span>],state:<span class="type">State</span>[<span class="type">Int</span>])=&gt;&#123;</div><div class="line"><span class="keyword">val</span> sum= value.getOrElse(<span class="number">0</span>) + state.getOption().getOrElse(<span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> output = (userId,sum)</div><div class="line">state.update(sum)</div><div class="line">output</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//通过mapWithState更新状态，设置状态超时时间为1小时</span></div><div class="line"><span class="keyword">val</span> stateDStream = wordDStream.mapWithState(<span class="type">StateSpec</span>.function(mappingFunc).timeout(<span class="type">Minutes</span>(<span class="number">60</span>))).print()</div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><code>mapWithState</code>接收的参数是一个<code>StateSpec</code>对象，在StateSpec中封装了状态管理的函数。我们定义了一个状态更新函数<code>mappingFunc</code>，该函数会更新指定用户的状态，同时会返回更新后的状态，将该函数传给<code>mapWithState</code>，并设置状态超时时间。SparkStreaming通过根据我们定义的更新函数，在每个计算时间间隔内更新内部维护的状态，同时返回经过<code>mappingFunc</code>处理后的结果数据流。</p><p>我们来看看它的实现方式。</p><h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>代码例子中可以看到，我们是在<code>StateSpec</code>中封装状态管理的函数，深入研究，看看怎么实现的。我们发现<code>mapWithState</code>函数会创建了<code>MapWithStateDStreamImpl</code>对象；继续往下寻找，其实<code>MapWithStateDStreamImp</code>是继承了<code>MapWithStateDStream</code>，而<code>MapWithStateDStream</code>的父类实际上是<code>DStream</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// mapWithState函数中创建了MapWithStateDStreamImpl对象</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapWithState</span></span>[<span class="type">StateType</span>: <span class="type">ClassTag</span>, <span class="type">MappedType</span>: <span class="type">ClassTag</span>](</div><div class="line">      spec: <span class="type">StateSpec</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>]</div><div class="line">    ): <span class="type">MapWithStateDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>] = &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">MapWithStateDStreamImpl</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>](</div><div class="line">      self,</div><div class="line">      spec.asInstanceOf[<span class="type">StateSpecImpl</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>]]</div><div class="line">    )</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">  * MapWithStateDStreamImpl继承MapWithStateDStream</span></div><div class="line"><span class="comment">  * 创建InternalMapWithStateDStream类型对象的internalStream</span></div><div class="line"><span class="comment">  */</span></div><div class="line"><span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">MapWithStateDStreamImpl</span>[</span></div><div class="line"><span class="class">    <span class="type">KeyType</span>: <span class="type">ClassTag</span>, <span class="type">ValueType</span>: <span class="type">ClassTag</span>, <span class="type">StateType</span>: <span class="type">ClassTag</span>, <span class="type">MappedType</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></div><div class="line"><span class="class"><span class="params">    dataStream: <span class="type">DStream</span>[(<span class="type">KeyType</span>, <span class="type">ValueType</span></span>)],</span></div><div class="line"><span class="class">    <span class="title">spec</span></span>: <span class="type">StateSpecImpl</span>[<span class="type">KeyType</span>, <span class="type">ValueType</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>])</div><div class="line">  <span class="keyword">extends</span> <span class="type">MapWithStateDStream</span>[<span class="type">KeyType</span>, <span class="type">ValueType</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>](dataStream.context) &#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> internalStream =</div><div class="line">    <span class="keyword">new</span> <span class="type">InternalMapWithStateDStream</span>[<span class="type">KeyType</span>, <span class="type">ValueType</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>](dataStream, spec)</div><div class="line">      </div><div class="line">  ...</div><div class="line"> </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(validTime: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">RDD</span>[<span class="type">MappedType</span>]] = &#123;</div><div class="line">    internalStream.getOrCompute(validTime).map &#123; _.flatMap[<span class="type">MappedType</span>] &#123; _.mappedData &#125; &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">      </div><div class="line"><span class="comment">//MapWithStateDStream的父类实际上是DStream</span></div><div class="line"><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MapWithStateDStream</span>[<span class="type">KeyType</span>, <span class="type">ValueType</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></div><div class="line"><span class="class"><span class="params">    ssc: <span class="type">StreamingContext</span></span>) <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">MappedType</span>](<span class="params">ssc</span>) </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">/** Return a pair DStream where each RDD is the snapshot of the state of all the keys. */</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stateSnapshots</span></span>(): <span class="type">DStream</span>[(<span class="type">KeyType</span>, <span class="type">StateType</span>)]</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>我们可以看到，MapWithStateDStreamImpl 中创建了一个<code>InternalMapWithStateDStream</code>类型对象<code>internalStream</code>，在MapWithStateDStreamImpl的<code>compute</code>方法中调用了internalStream的<code>getOrCompute</code>方法。</p><p>实际上，InternalMapWithStateDStream中没有getOrCompute方法，最终是调用父类 DStream 的getOrCpmpute方法，该方法中最终会调用InternalMapWithStateDStream中重写的Compute方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/** Method that generates an RDD for the given time */</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(validTime: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">RDD</span>[<span class="type">MapWithStateRDDRecord</span>[<span class="type">K</span>, <span class="type">S</span>, <span class="type">E</span>]]] = &#123;</div><div class="line">    <span class="comment">// Get the previous state or create a new empty state RDD</span></div><div class="line">    <span class="keyword">val</span> prevStateRDD = getOrCompute(validTime - slideDuration) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(rdd) =&gt;</div><div class="line">        <span class="keyword">if</span> (rdd.partitioner != <span class="type">Some</span>(partitioner)) &#123;</div><div class="line">          <span class="comment">// If the RDD is not partitioned the right way, let us repartition it using the</span></div><div class="line">          <span class="comment">// partition index as the key. This is to ensure that state RDD is always partitioned</span></div><div class="line">          <span class="comment">// before creating another state RDD using it</span></div><div class="line">          <span class="type">MapWithStateRDD</span>.createFromRDD[<span class="type">K</span>, <span class="type">V</span>, <span class="type">S</span>, <span class="type">E</span>](</div><div class="line">            rdd.flatMap &#123; _.stateMap.getAll() &#125;, partitioner, validTime)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          rdd</div><div class="line">        &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="type">MapWithStateRDD</span>.createFromPairRDD[<span class="type">K</span>, <span class="type">V</span>, <span class="type">S</span>, <span class="type">E</span>](</div><div class="line">          spec.getInitialStateRDD().getOrElse(<span class="keyword">new</span> <span class="type">EmptyRDD</span>[(<span class="type">K</span>, <span class="type">S</span>)](ssc.sparkContext)),</div><div class="line">          partitioner,</div><div class="line">          validTime</div><div class="line">        )</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">// Compute the new state RDD with previous state RDD and partitioned data RDD</span></div><div class="line">    <span class="comment">// Even if there is no data RDD, use an empty one to create a new state RDD</span></div><div class="line">    <span class="keyword">val</span> dataRDD = parent.getOrCompute(validTime).getOrElse &#123;</div><div class="line">      context.sparkContext.emptyRDD[(<span class="type">K</span>, <span class="type">V</span>)]</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> partitionedDataRDD = dataRDD.partitionBy(partitioner)</div><div class="line">    <span class="keyword">val</span> timeoutThresholdTime = spec.getTimeoutInterval().map &#123; interval =&gt;</div><div class="line">      (validTime - interval).milliseconds</div><div class="line">    &#125;</div><div class="line">    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">MapWithStateRDD</span>(</div><div class="line">      prevStateRDD, partitionedDataRDD, mappingFunction, validTime, timeoutThresholdTime))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>InternalMapWithStateDStream中重写的Compute方法是关键。根据给定的时间生成一个<code>MapWithStateRDD</code>，首先获取了先前状态的RDD：preStateRDD和当前时间的RDD：dataRDD，然后对dataRDD基于先前状态RDD的分区器进行重新分区获取<code>partitionedDataRDD</code>，最后将<code>preStateRDD</code>，<code>partitionedDataRDD</code>，用户定义的函数<code>mappingFunction</code>和时间戳传给新生成的<code>MapWithStateRDD</code>对象，得出结果并返回。</p><p>我们再来看一下关键的MapWithStateRDD的<code>compute</code>方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</div><div class="line">      partition: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">MapWithStateRDDRecord</span>[<span class="type">K</span>, <span class="type">S</span>, <span class="type">E</span>]] = &#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> stateRDDPartition = partition.asInstanceOf[<span class="type">MapWithStateRDDPartition</span>]</div><div class="line">    <span class="keyword">val</span> prevStateRDDIterator = prevStateRDD.iterator(</div><div class="line">      stateRDDPartition.previousSessionRDDPartition, context)</div><div class="line">    <span class="keyword">val</span> dataIterator = partitionedDataRDD.iterator(</div><div class="line">      stateRDDPartition.partitionedDataRDDPartition, context)</div><div class="line">    <span class="comment">//preRecord代表一个分区的数据</span></div><div class="line">    <span class="keyword">val</span> prevRecord = <span class="keyword">if</span> (prevStateRDDIterator.hasNext) <span class="type">Some</span>(prevStateRDDIterator.next()) <span class="keyword">else</span> <span class="type">None</span></div><div class="line">    <span class="keyword">val</span> newRecord = <span class="type">MapWithStateRDDRecord</span>.updateRecordWithData(</div><div class="line">      prevRecord,</div><div class="line">      dataIterator,</div><div class="line">      mappingFunction,</div><div class="line">      batchTime,</div><div class="line">      timeoutThresholdTime,</div><div class="line">      removeTimedoutData = doFullScan <span class="comment">// remove timed-out data only when full scan is enabled</span></div><div class="line">    )</div><div class="line">    <span class="type">Iterator</span>(newRecord)</div><div class="line">  &#125;</div></pre></td></tr></table></figure><p><code>MapWithStateRDDRecord</code> 对应MapWithStateRDD 的一个分区，<code>StateMap</code>存储了key的状态，mappedData存储了mappingFunc函数的返回值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[streaming] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MapWithStateRDDRecord</span>[<span class="type">K</span>, <span class="type">S</span>, <span class="type">E</span>](<span class="params"></span></span></div><div class="line"><span class="class"><span class="params">    var stateMap: <span class="type">StateMap</span>[<span class="type">K</span>, <span class="type">S</span>], var mappedData: <span class="type">Seq</span>[<span class="type">E</span>]</span>)</span></div></pre></td></tr></table></figure><p>我们再来看下<code>MapWithStateRDDRecord.updateRecordWithData</code>方法，<code>newStateMap</code>是创建一个<code>StateMap</code>，从过去的Record中复制（如果存在），否则就创建新的StateMap对象。最终返回<code>MapWithSateRDDRecord</code>对象交给MapWithStateRDD的<code>compute</code>函数，MapWithStateRDD的compute函数将其封装成Iterator返回出去。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateRecordWithData</span></span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">S</span>: <span class="type">ClassTag</span>, <span class="type">E</span>: <span class="type">ClassTag</span>](</div><div class="line">    prevRecord: <span class="type">Option</span>[<span class="type">MapWithStateRDDRecord</span>[<span class="type">K</span>, <span class="type">S</span>, <span class="type">E</span>]],</div><div class="line">    dataIterator: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)],</div><div class="line">    mappingFunction: (<span class="type">Time</span>, <span class="type">K</span>, <span class="type">Option</span>[<span class="type">V</span>], <span class="type">State</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">E</span>],</div><div class="line">    batchTime: <span class="type">Time</span>,</div><div class="line">    timeoutThresholdTime: <span class="type">Option</span>[<span class="type">Long</span>],</div><div class="line">    removeTimedoutData: <span class="type">Boolean</span></div><div class="line">  ): <span class="type">MapWithStateRDDRecord</span>[<span class="type">K</span>, <span class="type">S</span>, <span class="type">E</span>] = &#123;</div><div class="line">    <span class="comment">// Create a new state map by cloning the previous one (if it exists) or by creating an empty one</span></div><div class="line">    <span class="keyword">val</span> newStateMap = prevRecord.map &#123; _.stateMap.copy() &#125;. getOrElse &#123; <span class="keyword">new</span> <span class="type">EmptyStateMap</span>[<span class="type">K</span>, <span class="type">S</span>]() &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> mappedData = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">E</span>]</div><div class="line">    <span class="keyword">val</span> wrappedState = <span class="keyword">new</span> <span class="type">StateImpl</span>[<span class="type">S</span>]()</div><div class="line"></div><div class="line">    <span class="comment">// Call the mapping function on each record in the data iterator, and accordingly</span></div><div class="line">    <span class="comment">// update the states touched, and collect the data returned by the mapping function</span></div><div class="line">    dataIterator.foreach &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">        <span class="comment">// 获取key对应的状态</span></div><div class="line">      wrappedState.wrap(newStateMap.get(key))</div><div class="line">        <span class="comment">// 调用mappingFunc获取返回值</span></div><div class="line">      <span class="keyword">val</span> returned = mappingFunction(batchTime, key, <span class="type">Some</span>(value), wrappedState)</div><div class="line">        <span class="comment">//维护newStateMap的值</span></div><div class="line">      <span class="keyword">if</span> (wrappedState.isRemoved) &#123;</div><div class="line">        newStateMap.remove(key)</div><div class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (wrappedState.isUpdated</div><div class="line">          || (wrappedState.exists &amp;&amp; timeoutThresholdTime.isDefined)) &#123;</div><div class="line">        newStateMap.put(key, wrappedState.get(), batchTime.milliseconds)</div><div class="line">      &#125;</div><div class="line">      mappedData ++= returned</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// Get the timed out state records, call the mapping function on each and collect the</span></div><div class="line">    <span class="comment">// data returned</span></div><div class="line">    <span class="keyword">if</span> (removeTimedoutData &amp;&amp; timeoutThresholdTime.isDefined) &#123;</div><div class="line">      newStateMap.getByTime(timeoutThresholdTime.get).foreach &#123; <span class="keyword">case</span> (key, state, _) =&gt;</div><div class="line">        wrappedState.wrapTimingOutState(state)</div><div class="line">        <span class="keyword">val</span> returned = mappingFunction(batchTime, key, <span class="type">None</span>, wrappedState)</div><div class="line">        mappedData ++= returned</div><div class="line">        newStateMap.remove(key)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="type">MapWithStateRDDRecord</span>(newStateMap, mappedData)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>至此，mapWithState方法源码执行过程水落石出。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;最近经历挫折教育，今天闲得时间，整理状态管理之解析mapWithState。今天说道的mapWithState是从Spark1.6开始引入的一种新的状态管理机制，支持输出全量的状态和更新的状态，支持对状态超时的管理，和自主选择需要的输出。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://pross.space/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>无题</title>
    <link href="https://pross.space/archives/2018/08/25/"/>
    <id>https://pross.space/archives/2018/08/25/</id>
    <published>2018-08-25T14:36:25.000Z</published>
    <updated>2018-09-24T12:29:48.146Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>岁月还在眼眸打坐，你的凝望</p><p>也许在苦等，有不朽的诗句</p><p>说完这些话后我的梦想已经有拔节之声</p><p>我蜷缩起双腿</p><p>似乎是在为每一段时光致敬</p><a id="more"></a><p><img src="/archives/2018/08/25/Authony.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;岁月还在眼眸打坐，你的凝望&lt;/p&gt;&lt;p&gt;也许在苦等，有不朽的诗句&lt;/p&gt;&lt;p&gt;说完这些话后我的梦想已经有拔节之声&lt;/p&gt;&lt;p&gt;我蜷缩起双腿&lt;/p&gt;&lt;p&gt;似乎是在为每一段时光致敬&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活杂记" scheme="https://pross.space/tags/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming之解析updateStateByKey</title>
    <link href="https://pross.space/archives/2018/06/19/"/>
    <id>https://pross.space/archives/2018/06/19/</id>
    <published>2018-06-19T10:52:15.000Z</published>
    <updated>2018-09-16T12:45:17.683Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。</p><a id="more"></a><h4 id="状态管理的需求"><a href="#状态管理的需求" class="headerlink" title="状态管理的需求"></a>状态管理的需求</h4><p>举一个最简单的需求例子来解释状态（state）管理，现在有这样的一个需求：计算从数据流开始到目前为止单词出现的次数。是不是看起来很眼熟，这其实就是一个升级版的wordcount，只不过需要在每个batchInterval计算当前batch的单词计数，然后对各个批次的计数进行累加。每一个批次的累积的计数就是当前的一个状态值。我们需要把这个状态保存下来，和后面批次单词的计数结果来进行计算，这样我们就能不断的在历史的基础上进行次数的更新。</p><p>SparkStreaming提供了两种方法来解决这个问题：updateStateByKey和mapWithState。mapWithState是1.6版本新增的功能，官方说性能较updateStateByKey提升10倍。</p><h4 id="updateStateByKey概述"><a href="#updateStateByKey概述" class="headerlink" title="updateStateByKey概述"></a>updateStateByKey概述</h4><p>updateStateByKey，统计全局的Key的状态，就算没有数据输入，也会在每一个批次的时候返回之前的key的状态。假设5s产生一个批次的数据，那么就是5s的时候就更新一次key的值，然后返回。如果数据量又比较大，又需要不断的更新每个Key的state， 那么就一定会涉及到状态的保存和容错。所以，要使用updateStateByKey就需要设置一个checkpoint目录，开启checkpoint机制。因为key的State是在内存中维护的，如果宕机，则重启之后之前维护的状态就没有了，所以要长期保存的话则需要启用<code>checkpoint</code>，以便于恢复数据。</p><h4 id="updateStateByKey代码例子"><a href="#updateStateByKey代码例子" class="headerlink" title="updateStateByKey代码例子"></a>updateStateByKey代码例子</h4><p>现在我们来看看怎么用的，首先看一个updateStateByKey使用的简单例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">  * Author: shawn pross</span></div><div class="line"><span class="comment">  * Date: 2018/06/18</span></div><div class="line"><span class="comment">  * Description: test updateStateByKey op</span></div><div class="line"><span class="comment">  */</span></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestUpdateStateByKey</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">conf.setAppName(<span class="string">s"<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>"</span>)</div><div class="line">conf.setMaster(<span class="string">"local[2]"</span>)</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">        <span class="comment">/**</span></div><div class="line"><span class="comment">        * 创建context,3 second batch size</span></div><div class="line"><span class="comment">* 创建一个接收器(ReceiverInputDStream),接收从机器上的端口，通过socket发过来的数据</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">3</span>))</div><div class="line"><span class="keyword">val</span> bathLine = ssc.socketTextStream(<span class="string">"127.0.0.1"</span>, <span class="number">9999</span>)</div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment">* 传入updateStateByKey的函数</span></div><div class="line"><span class="comment">*</span></div><div class="line"><span class="comment">* 源码定义：</span></div><div class="line"><span class="comment">* def updateStateByKey[S: ClassTag](</span></div><div class="line"><span class="comment">* updateFunc: (Seq[V], Option[S]) =&gt; Option[S]): DStream[(K, S)] = ssc.withScope &#123;</span></div><div class="line"><span class="comment">* updateStateByKey(updateFunc, defaultPartitioner())</span></div><div class="line"><span class="comment">* &#125;</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">val</span> updateFunc = (currValues: <span class="type">Seq</span>[<span class="type">Int</span>], prevValueState: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</div><div class="line"><span class="comment">//通过Spark内部的reduceByKey按key规约，然后这里传入某key当前批次的Seq/List,再计算当前批次的总和</span></div><div class="line"><span class="keyword">val</span> currentCount = currValues.sum</div><div class="line"><span class="comment">// 已累加的值</span></div><div class="line"><span class="keyword">val</span> previousCount = prevValueState.getOrElse(<span class="number">0</span>)</div><div class="line"><span class="comment">// 返回累加后的结果，是一个Option[Int]类型</span></div><div class="line"><span class="type">Some</span>(currentCount + previousCount)</div><div class="line">&#125;</div><div class="line"><span class="comment">//先聚合成键值对的形式</span></div><div class="line">bathLine.map(x=&gt;(x,<span class="number">1</span>)).updateStateByKey(updateFunc).print()</div><div class="line">        </div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>代码很简单，注释也比较详细。其中要说明的是<code>updateStateByKey</code>的参数还有几个可选项：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</div><div class="line">    <span class="comment">//状态更新功能</span></div><div class="line">    updateFunc: (<span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">S</span>],</div><div class="line">    <span class="comment">//用于控制新RDD中每个RDD的分区的分区程序</span></div><div class="line">    partitioner: <span class="type">Partitioner</span>,</div><div class="line">    <span class="comment">//是否记住生成的RDD中的分区对象</span></div><div class="line">    rememberPartitioner: <span class="type">Boolean</span>,  </div><div class="line">    <span class="comment">//每个键的初始状态值</span></div><div class="line">    initialRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)],</div><div class="line">  ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="updateStateByKey源码分析"><a href="#updateStateByKey源码分析" class="headerlink" title="updateStateByKey源码分析"></a>updateStateByKey源码分析</h4><p>通过上面简单的小例子可以知道，使用updateStateByKey是需要先转换为键值对的形式的，而map返回的是<code>MappedDStream</code>，而进入<code>MappedDStream</code>中也没有updateStateByKey方法，然后其父类DStream中也没有。但是DStream的半生对象中有一个隐式的转换函数<code>toPairDStreamFunctions</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// map实现，返回MappedDStream</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">MappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(mapFunc))</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="comment">// MappedDStream父类是DStream</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappedDStream</span>[<span class="type">T</span>: <span class="type">ClassTag</span>, <span class="type">U</span>: <span class="type">ClassTag</span>] (<span class="params"></span></span></div><div class="line"><span class="class"><span class="params">    parent: <span class="type">DStream</span>[<span class="type">T</span>],</span></span></div><div class="line"><span class="class"><span class="params">    mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span></span></span></div><div class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">U</span>](<span class="params">parent.ssc</span>) </span>&#123;</div><div class="line">    ...</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// DStream中隐式的转换函数</span></div><div class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">toPairDStreamFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](stream: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)])</div><div class="line">      (<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>):</div><div class="line">    <span class="type">PairDStreamFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">PairDStreamFunctions</span>[<span class="type">K</span>, <span class="type">V</span>](stream)</div><div class="line">  &#125;</div></pre></td></tr></table></figure><p>看到<code>new PairDStreamFunctions</code>就不陌生了。<code>PairDStreamFunctions</code>中存在updateStateByKey方法，Seq[V]表示当前key对应的所有值，Option[S] 是当前key的历史状态，返回的是新的状态。也就是绕了一个圈子又回到原地。最后updateStateByKey最终会在这里面new出了一个<code>StateDStream</code>对象。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</div><div class="line">     updateFunc: (<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>])]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">S</span>)],</div><div class="line">     partitioner: <span class="type">Partitioner</span>,</div><div class="line">     rememberPartitioner: <span class="type">Boolean</span>,</div><div class="line">     initialRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</div><div class="line">   <span class="keyword">val</span> cleanedFunc = ssc.sc.clean(updateFunc)</div><div class="line">   <span class="keyword">val</span> newUpdateFunc = (_: <span class="type">Time</span>, it: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>])]) =&gt; &#123;</div><div class="line">     cleanedFunc(it)</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">new</span> <span class="type">StateDStream</span>(self, newUpdateFunc, partitioner, rememberPartitioner, <span class="type">Some</span>(initialRDD))</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>继续进去<code>StateDStream</code>看看，在其<code>compute</code>方法中，会先获取上一个batch计算出的RDD（包含了至程序开始到上一个batch单词的累计计数），然后在获取本次batch中<code>StateDStream</code>的父类计算出的RDD（本次batch的单词计数）分别是<code>prevStateRDD</code>和<code>parentRDD</code>，然后在调用 <code>computeUsingPreviousRDD</code> 方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> [<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">computeUsingPreviousRDD</span></span>(</div><div class="line">    batchTime: <span class="type">Time</span>,</div><div class="line">    parentRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</div><div class="line">    prevStateRDD: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">S</span>)]) = &#123;</div><div class="line">  <span class="comment">// Define the function for the mapPartition operation on cogrouped RDD;</span></div><div class="line">  <span class="comment">// first map the cogrouped tuple to tuples of required type,</span></div><div class="line">  <span class="comment">// and then apply the update function</span></div><div class="line">  <span class="keyword">val</span> updateFuncLocal = updateFunc</div><div class="line">  <span class="keyword">val</span> finalFunc = (iterator: <span class="type">Iterator</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">S</span>]))]) =&gt; &#123;</div><div class="line">    <span class="keyword">val</span> i = iterator.map &#123; t =&gt;</div><div class="line">      <span class="keyword">val</span> itr = t._2._2.iterator</div><div class="line">      <span class="keyword">val</span> headOption = <span class="keyword">if</span> (itr.hasNext) <span class="type">Some</span>(itr.next()) <span class="keyword">else</span> <span class="type">None</span></div><div class="line">      (t._1, t._2._1.toSeq, headOption)</div><div class="line">    &#125;</div><div class="line">    updateFuncLocal(batchTime, i)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">val</span> cogroupedRDD = parentRDD.cogroup(prevStateRDD, partitioner)</div><div class="line">  <span class="keyword">val</span> stateRDD = cogroupedRDD.mapPartitions(finalFunc, preservePartitioning)</div><div class="line">  <span class="type">Some</span>(stateRDD)</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>最后返回<code>stateRDD</code>结果。至此，updateStateByKey方法源码执行过程水落石出。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;说到Spark Streaming的状态管理，就会想到updateStateByKey，还有mapWithState。今天整理了一下，着重了解一下前者。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://pross.space/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>理解Spark核心之RDD</title>
    <link href="https://pross.space/archives/2018/05/29/"/>
    <id>https://pross.space/archives/2018/05/29/</id>
    <published>2018-05-29T09:04:41.000Z</published>
    <updated>2018-08-31T16:30:19.437Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集）</p><a id="more"></a><h4 id="理解RDD"><a href="#理解RDD" class="headerlink" title="理解RDD"></a>理解RDD</h4><p>如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。</p><p>RDD是一个容错的，并行的数据结构，可以让用户显示地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，比如Map、flatMap、filter等转换操作实现了monad模式（Monad是一种设计模式，表示将一个运算过程，通过函数拆解成互相连接的多个步骤；你只要提供下一步运算所需的函数，整个运算就会自动进行下去。），很好的切合了Scala的集合操作。另外，RDD还提供了比如join，groupBy，reduceByKey（action操作）等更为方便的操作，用来支持常见的数据运算。</p><p>RDD是一系列只读分区的集合，它只能从文件中读取并创建，或者从旧的RDD生成新的RDD。RDD的每一次变换操作都会生成新的RDD，而不是在原来的基础上进行修改，这种粗粒度的数据操作方式为RDD带来了容错和数据共享方面的优势，但是在面对大数据集中频繁的小操作的时候，显得效率比较低下。</p><h4 id="RDD原理"><a href="#RDD原理" class="headerlink" title="RDD原理"></a>RDD原理</h4><p>RDD实际上是一个类（sc.textFile()方法返回一个RDD对象，然后用line接收这个对象），而这个RDD类中也定义了一系列的用于操作的方法，也就是一些算子操作。</p><p>这个类为了实现对数据的操作，里面有分区信息，用于记录特定RDD的分区情况；依赖关系，指向其父RDD；一个函数，用于记录父RDD到自己的转换操作；划分策略和数据位置的元数据。在DAG中这样的RDD就可以看成一个个节点，RDD中的存储的依赖关系就是DAG的边。在Spark中，数据在物理上被划分为一个个的block，这些block由blockmanager统一管理的。</p><p>在设计RDD之间的依赖关系时，设计者将RDD之间的依赖关系分为两类：窄依赖和宽依赖。RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个DataSet片段。RDD可以相互依赖，如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之位narrow dependency（窄依赖）；若多个Child RDD分区都可以依赖，则称为wide dependency（宽依赖），而join操作则会产生wide dependency。</p><p><img src="/archives/2018/05/29/narrowAndwide.jpg" alt="narrow和wide"></p><p><img src="/archives/2018/05/29/rdd-dependencies.png" alt="narrow和wide的区别"></p><blockquote><p>Spark之所以将依赖分为narrow与wide，基于以下两点原因：</p><p>narrow dependecies可以支持在同一个cluster node上，并且以管道形式执行多行命令，例如在执行了map操作后，紧接着执行filter。相反，wide dependencies需要所有的父分区都是可用的，可能还需要调用类似MapReduce之类的操作进行跨节点传递。</p><p>其次从失败恢复的角度考虑，narrow dependencies的失败恢复更有效，因为它只需要重新计算丢失的parent partition即可，而且可以并行的在不同节点进行重计算。而wide dependencies牵涉到RDD各级的多个parent partitions。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>RDD是Spark的核心，也是整个Spark架构的基础，特性总结如下：</p><ul><li>不变的数据结构存储</li><li>支持跨集群的分布式数据结构</li><li>可以根据数据记录的Key对结构进行分区</li><li>提供了粗粒度的操作，且这些操作支持分区</li><li>它将数据存储在内存中，从而提供了低延迟性</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://pross.space/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的运行模式</title>
    <link href="https://pross.space/archives/2018/05/15/"/>
    <id>https://pross.space/archives/2018/05/15/</id>
    <published>2018-05-15T12:56:12.000Z</published>
    <updated>2018-08-31T16:18:36.785Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>Spark是新一代基于内存的计算框架，是用于大规模数据处理的同意分析引擎。相比于Hadoop MapReduce计算框架，Spark将中间计算结果保留在内存中，速度提升10~100倍；同时采用弹性分布式数据集（RDD）实现迭代计算，更好的适用于数据挖掘、机器学习，极大的提升开发效率。</p><p>Spark的运行模式，它不仅支持单机模式，同时支持集群模式运行；这里具体的总结一下Spark的各种运行模式的区分。</p><a id="more"></a><h4 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h4><p>Local模式又称本地模式，通过Local模式运行非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker进程（只有集群的Standalone模式运行时，才需要这两个角色），也不用启动Hadoop的服务，除非你需要用到HDFS。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--class org.apache.spark.examples.SparkPi \</div><div class="line">--master local[2] \</div><div class="line">lib/spark-examples-1.0.0-hadoop2.2.0.jar  \</div><div class="line">100</div><div class="line"></div><div class="line"># 1）--master local 就可以确定是单机的local模式了，[2]意思是分配2个cores运行</div><div class="line"># 2）--lib/spark-examples-1.0.0-hadoop2.2.0.jar：jar包的路径（application-jar）</div><div class="line"># 3）--100：传递给主类的主要方法的可选参数（application-arguments）</div></pre></td></tr></table></figure><p>这里的spark-submit进程既是客户端提交任务的Client进程，又是Spark的Driver程序，还充当着Spark执行Task的Executor角色。所有的程序都运行在一个JVM中，主要用于开发时测试。</p><h4 id="本地伪集群运行模式（单机模拟集群）"><a href="#本地伪集群运行模式（单机模拟集群）" class="headerlink" title="本地伪集群运行模式（单机模拟集群）"></a>本地伪集群运行模式（单机模拟集群）</h4><p>这种模式，和Local[N]很像，不同的是它会在单机的环境下启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委曲求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有出现问题，或者想使用Spark计算框架而没有太多资源的情况下。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--master local-cluster[2,3,1024]</div><div class="line"></div><div class="line"># --master local-cluster[2,3,1024]：在本地模拟集群下使用2个Executor进程，每个进程分配3个cores和1024M的内存来运行程序</div></pre></td></tr></table></figure><p>这里的spark-submit依然充当全能角色，又是Client进程，又是Driver程序，也负责资源管理。运行该模式很简单，只需要把Spark安装包解压后，修改一些常用的配置，不用启动Spark的Master、Worker守护进程，也不用启动Hadoop的服务，除非你是需要用到HDFS。</p><h4 id="Standalone模式（集群）"><a href="#Standalone模式（集群）" class="headerlink" title="Standalone模式（集群）"></a>Standalone模式（集群）</h4><p>Standalone是集群模式，这里就需要在执行应用程序前，先启动Spark的Master和Worker守护进程，不用启动Hadoop的服务，除非你需要使用HDFS。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --executor-memory 8G \</div><div class="line">  --total-executor-cores 10 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># 1) --master spark://207.184.161.138:7077:采用Standalone模式运行，后面是集群地址和端口</div><div class="line"># 2) --executor-memory 20G:配置executor进程内存为8G</div><div class="line"># 3）--total-executor-cores 100：配置cores数为10个</div></pre></td></tr></table></figure><p>Master进程作为cluster manager，用来对应用程序申请的资源进行管理；spark-submit做为Client端和运行Driver程序。Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点，Master节点和Worker节点。其中Driver既可以运行在Master节点上，也可以运行在本地的Client端。</p><p>当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclipse、IDEA等开发平台上使用<code>new SparkConf.setManager(&quot;spark://master:7077&quot;)</code>方式运行Spark任务时，Driver是运行在本地Client端上的。</p><p><strong>运行流程</strong></p><ul><li>SparkContext连接到Master，向Master注册并申请资源（CPU Core和Memory）</li><li>Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend</li><li>StandaloneExecutorBackend向SparkContext注册</li><li>SparkContext将Application代码发给StandaloneExecutorBackend；并且SparkContext解析Application代码，构建DAG图，提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会产生Job；每个Job中含有一个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后Stage（又称为TaskSet）提交Task Scheduler，负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行</li><li>StandaloneExecutorBackend会构建Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成</li><li>所有Task完成后，SparkContext向Master注销，释放资源</li></ul><h4 id="on-yarn-client模式（集群）"><a href="#on-yarn-client模式（集群）" class="headerlink" title="on yarn client模式（集群）"></a>on yarn client模式（集群）</h4><p>越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--class org.apache.spark.examples.SparkPi \</div><div class="line">--master yarn \</div><div class="line">--deploy-mode client \</div><div class="line">lib/spark-examples-1.0.0-hadoop2.2.0.jar </div><div class="line"></div><div class="line"># 1) --master yarn：采用yarn进行资源调度</div><div class="line"># 2) --deploy-mode client：client环境运行</div></pre></td></tr></table></figure><p>在执行Spark应用程序前，要启动Hadoop的各种服务，由于已经有了资源管理器，所以不需要启动Spark的Master，Worker守护进程。</p><p><strong>运行流程</strong></p><ul><li>Spark Yarn Client向Yarn的ResourceManager申请启动Application Master，同时在SparkContext初始化中创建DAG Scheduler和Task Scheduler，由于我们选择是Yarn Client模式，程序会选择启动YarnClientClusterScheduler和YarnClientSchedulerBackend</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster；与YarnCluster区别是在该ApplicationMaster中不运行SparkContext，只与SparkContext进行联系进行资源的分配</li><li>Client中的SparkContext初始化完成后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便于对应的NodeManager通信，要求它在获得的Container中开始向SparkContext注册并申请执行Task任务</li><li>Client中的SparkContext分配给Container的Task开始执行，并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而可以在任务失败时重启任务</li><li>应用程序完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</li></ul><h4 id="on-yarn-cluster模式（集群）"><a href="#on-yarn-cluster模式（集群）" class="headerlink" title="on yarn cluster模式（集群）"></a>on yarn cluster模式（集群）</h4><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">--master yarn \</div><div class="line">--deploy-mode cluster \</div><div class="line">--driver-memory 4g \</div><div class="line">--executor-memory 2g \</div><div class="line">--executor-cores 1 \</div><div class="line">examples/jars/spark-examples*.jar \</div><div class="line">10</div><div class="line"></div><div class="line"># --driver-memory 4g：集群模式下Yarn Application Master的内存大小</div></pre></td></tr></table></figure><p><strong>运行流程</strong></p><ul><li>SparkYarnClient向YARN中ResourceManager提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster中进行SparkContext的初始化</li><li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便于对应的NodeManager通信，要求它在获得的Container中开始向SparkContext注册并申请执行Task任务</li><li>SparkContext分配给Container的Task开始执行，并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而可以在任务失败时重启任务</li><li>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己</li></ul><h4 id="Mesos模式"><a href="#Mesos模式" class="headerlink" title="Mesos模式"></a>Mesos模式</h4><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核。Mesos最初是由加州大学伯克利分校的AMPLab开发的，后在Twitter得到广泛使用。Apache Mesos是一个通用的集群管理器，起源于 Google 的数据中心资源管理系统Borg。</p><p>Mesos模式接触较少，这里不作为展开。</p><h4 id="Kubernetes模式（K8S）"><a href="#Kubernetes模式（K8S）" class="headerlink" title="Kubernetes模式（K8S）"></a>Kubernetes模式（K8S）</h4><p>Kubernetes调度器目前是实验性的。在未来的版本中，可能会出现配置，容器图像和入口点的行为变化。 （Spark2.3.0）</p><p>Kubernetes模式接触较少，这里不作为展开。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Spark是新一代基于内存的计算框架，是用于大规模数据处理的同意分析引擎。相比于Hadoop MapReduce计算框架，Spark将中间计算结果保留在内存中，速度提升10~100倍；同时采用弹性分布式数据集（RDD）实现迭代计算，更好的适用于数据挖掘、机器学习，极大的提升开发效率。&lt;/p&gt;&lt;p&gt;Spark的运行模式，它不仅支持单机模式，同时支持集群模式运行；这里具体的总结一下Spark的各种运行模式的区分。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://pross.space/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HDFS NameNode内存全景</title>
    <link href="https://pross.space/archives/2018/05/07/"/>
    <id>https://pross.space/archives/2018/05/07/</id>
    <published>2018-05-07T00:37:22.000Z</published>
    <updated>2018-08-31T16:37:19.626Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><blockquote><p>在HDFS系统架构中，NameNode管理着整个文件系统的元数据，维护整个集群的机架感知信息和DataNode和Block的信息，Lease管理以及集中式缓存引入的缓存管理等等。从整个HDFS系统架构上来看，NameNode是最重要、最复杂也是最容易出现问题的地方。</p></blockquote><a id="more"></a><h4 id="NameNode概述"><a href="#NameNode概述" class="headerlink" title="NameNode概述"></a>NameNode概述</h4><p>NameNode管理的HDFS文件系统的元数据分为两个层次：NameSpace管理层，负责管理文件系统中的树状目录结构以及文件与数据块之间的映射关系；块管理层，负责管理文件系统中文件的物理块与实际存储位置的映射关系（BlockMap）。</p><p>NameSpace管理的元数据除内存常驻外，也会周期Flush到持久化设备fsimage文件上（core-site.xml中配置<code>hadoop.tmp.dir</code>目录下的dfs/name/current中）；BlockMap元数据只存在于内存中；当NameNode发生重启，首先从持久化设备中读取fsimage，构建NameSpace的元数据信息，之后根据DataNode的汇报信息重新构造BlockMap，这两部分数据是占据NamNode大部分JVM Heap空间。</p><h4 id="NameNode内存结构"><a href="#NameNode内存结构" class="headerlink" title="NameNode内存结构"></a>NameNode内存结构</h4><p><img src="/archives/2018/05/07/namenode.png" alt="namenode"></p><p>NameNode常驻内存主要被NameSpace和BlockManager使用，二者使用占比分别接近50%。其它部分内存开销较小且相对固定，与NameSpace和BlockManager相比基本可以忽略。</p><h4 id="NameNode内存分析"><a href="#NameNode内存分析" class="headerlink" title="NameNode内存分析"></a>NameNode内存分析</h4><h5 id="NameSpace"><a href="#NameSpace" class="headerlink" title="NameSpace"></a>NameSpace</h5><p><img src="/archives/2018/05/07/namespace.png" alt="namespace"></p><p>HDFS文件系统的目录结构也是按照树状结构维护，NameSpace保存了目录树以及每个目录/文件节点的属性。在整个NameSpace目录树中存在两种不同类型的INode数据结构：<code>INodeDirectory</code>和<code>INodeFile</code>。其中INodeDirectory表示的是目录树中的目录，INodeFile表示的是目录树中的文件。<br><img src="/archives/2018/05/07/inode.png" alt="inode"></p><p>INodeDirectory和INodeFile均继承自INode，所以具备大部分相同的公共信息INodeWithAddititionalFields，除常用基础属性外，其中还提供了扩展属性features（如Quota，Snapshot等），如果以后出现新属性也可以通过feature扩展。不同的是，INodeFile特有的标识副本数和数据块大小组合的<code>header</code>（2.61之后新增了标识存储策略ID的信息）以及该文件包含的有序Block的数组；INodeDirectory特有的是列表<code>children</code>，children默认是大小为5的ArrayList类型，按照子节点name有序存储，在插入时会损失一部分写入的性能，但是可以方便后续快速二分查找提高读的性能，对于一般的存储系统，读操作比写操作占比要高。</p><h5 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h5><p>NameNode概述中介绍的负责管理文件系统中文件的物理块与实际存储位置的映射关系（BlockMap）就是由BlockManager来统一管理。NameSpace和BlockMap之间通过前面提到的INodeFile有序Blocks数组关联到一起。<br><img src="/archives/2018/05/07/blockinfo.png" alt="blockinfo"></p><p>每一个INodeFile都会包含数量不等的Block，具体数量由文件大小及每一个Block的大小比值决定，这些Block按照所在的文件的先后顺序组成BlockInfo数组，BlockInfo维护的是Block的元数据，数据本身是由DataNode管理，所以BlockInfo需要包含实际数据且由DataNode管理的信息是名为triplets的Object数组，大小为3*replicas（replicas是Block副本数量，默认为3），从图中可以知道，BlockInfo包含了哪些Block，这些Block分别存储在哪些DataNode上。</p><p>如何快速的通过BlockId快速定位到Block，这里还需要BlocksMap。</p><p>BlocksMap底层通过LightWeightGSet实现，本质是一个链式解决冲突的Hash表。事实上，BlocksMap里所有的BlockInfo就是INodeFile中对应BlockInfo的引用，通过Block查找对应BlockInfo时，也是先对Block计算HashCode，根据结果快速定位到对应的BlockInfo信息。</p><p><img src="/archives/2018/05/07/blocksMap.png" alt="blocksMap"></p><p>这里还涉及到几个核心的数据结构：excessReplicateMap（多余的副本存放） ，neededReplications（需要补充Block的副本存放处，是一个优先级队列，缺少副本数越多的Block会优先处理），invalidateBlocks（删除的副本存放处） ，corruptReplicas（某些Block不可用暂存处）等。</p><p>相比Namespace，BlockManager管理的数据要复杂的多。</p><h5 id="NetworkTopology"><a href="#NetworkTopology" class="headerlink" title="NetworkTopology"></a>NetworkTopology</h5><p>Hadoop在设计考虑到数据的安全和高效，默认存放三份副本。存储策略是本地一份，同机架内其他某一个节点上一份，不同机架的某一节点上一份，这样如果本地数据损坏了，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿到的数据要快；为了降低整体的带宽消耗和读取延时时间，HDFS会尽快让读取程序读取离它最近的副本。那么Hadoop确定任意两个节点是位于同一机架还是不同的机架呢？这就需要机架拓扑NetworkTopology，也叫作机架感知。<br>默认情况下，NameNode启动时日志信息是这样的：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-05-09 19:27:26,423 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /default-rack/ 192.168.123.102:50010</div></pre></td></tr></table></figure><p>每个IP对应的机架ID都是/default-rack，说明Hadoop的机架感知没有被启用。<br><strong>配置机架感知</strong><br>配置机架感知也很简单，NameNode所在的节点中，在<code>core-site.xml</code>文件中配置<code>topology.script.name</code>，value通常是一个shell脚本，该脚本接受一个参数，输出一个值。接受的参数通常为某台DataNode的IP地址，而输出的值通常为该IP地址对应的DataNode所在的rack。<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.script.file.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apps/hadoop-2.6.5/etc/hadoop/topology.sh<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p></p><p><code>topology.sh</code><br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>!/bin/bash  </div><div class="line">HADOOP_CONF=/home/bigdata/apps/hadoop/etc/hadoop  </div><div class="line">while [ $# -gt 0 ] ; do  </div><div class="line">  nodeArg=$1  </div><div class="line">  exec&lt;$&#123;HADOOP_CONF&#125;/topology.data</div><div class="line">  result=""  </div><div class="line">  while read line ; do  </div><div class="line">    ar=( $line )  </div><div class="line">    if [ "$&#123;ar[0]&#125;" = "$nodeArg" ]||[ "$&#123;ar[1]&#125;" = "$nodeArg" ]; then  </div><div class="line">      result="$&#123;ar[2]&#125;"  </div><div class="line">    fi  </div><div class="line">  done  </div><div class="line">  shift  </div><div class="line">  if [ -z "$result" ] ; then  </div><div class="line">    echo -n "/default-rack"  </div><div class="line">  else  </div><div class="line">    echo -n "$result"  </div><div class="line">  fi  </div><div class="line">  done</div></pre></td></tr></table></figure><p></p><p><code>topology.data</code>格式为：节点（IP或主机名） /交换机xx/机架xx<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">192.168.123.102 hadoop02 /switch/rack2</div><div class="line">...</div></pre></td></tr></table></figure><p></p><p>配置后，NameNode启动时日志信息是这样的：<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-05-09 19:27:26,423 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /switch/rack2/ 192.168.123.102:50010</div></pre></td></tr></table></figure><p></p><p>说明Hadoop的机架感知已经被启用了。查看Hadoop机架信息的命令<code>hadoop dfsadmin -printTopology</code></p><h5 id="LeaseManager"><a href="#LeaseManager" class="headerlink" title="LeaseManager"></a>LeaseManager</h5><p>Lease 机制是重要的分布式协议，广泛应用于各种实际的分布式系统中。HDFS支持Write-Once-Read-Many，对文件写操作的互斥同步靠Lease实现。<br>Lease实际上是时间约束锁，其主要特点是排他性。客户端写文件时需要先申请一个Lease，一旦有客户端持有了某个文件的Lease，其它客户端就不可能再申请到该文件的Lease，这就保证了同一时刻对一个文件的写操作只能发生在一个客户端。<br>NameNode的LeaseManager是Lease机制的核心，维护了文件与Lease、客户端与Lease的对应关系，这类信息会随写数据的变化实时发生对应改变。</p><blockquote><p>本文是根据<a href="https://tech.meituan.com/namenode.html?_blank" target="_blank" rel="external">美团点评技术团队中《HDFS NameNode内存全景》</a>整理总结</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;blockquote&gt;&lt;p&gt;在HDFS系统架构中，NameNode管理着整个文件系统的元数据，维护整个集群的机架感知信息和DataNode和Block的信息，Lease管理以及集中式缓存引入的缓存管理等等。从整个HDFS系统架构上来看，NameNode是最重要、最复杂也是最容易出现问题的地方。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop HA集群搭建</title>
    <link href="https://pross.space/archives/2018/04/25/"/>
    <id>https://pross.space/archives/2018/04/25/</id>
    <published>2018-04-25T00:58:34.000Z</published>
    <updated>2018-08-31T16:29:47.492Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>HA：High Available，高可用。为什么需要HA机制？怎么配置HA？</p><a id="more"></a><h3 id="为什么会有Hadoop-HA机制"><a href="#为什么会有Hadoop-HA机制" class="headerlink" title="为什么会有Hadoop HA机制"></a>为什么会有Hadoop HA机制</h3><p>在HDFS集群中NameNode会存在单点故障（SPOF：A Single Point of Failure）问题：对于只有一个NameNode的集群，如果唯一的NameNode机器出现故障，比如宕机、软件硬件升级等。那么整个集群将无法使用，直到NameNode重新启动才会恢复。</p><p>所以在hadoop2.0之前，出现这种单节点故障问题是无法解决的；但是Hadoop HA机制的出现就很好的解决了这个问题，在一个典型的Hadoop HA集群中，使用两台单独的机器配置为NameNodes节点。在任何时间点，确保NameNodes中只有一个处于Active状态，另一个处在Standby状态。其中ActiveNameNode负责集群中所有的客户端的操作，StandbyNameNode仅仅充当备机，保证一旦ActiveNameNode出现问题能够快速切换。</p><p>准备两台NameNode解决单节点故障问题是完全可行，但是又出现一个问题：Active和Standby两个 NameNodes的元数据信息(editlog)如何同步才能让Standby节点切换后”无缝对接工作”？那么就需要一个共享存储系统，这个系统就是Zookeeper，Active NameNode会将数据写入共享存储系统，而Standby便可以快速切为Active NameNode接替其工作。为了实现这一目标，DataNode需要配置NameNodes的位置，并同时给他们发送文件块信息以及心跳检测。</p><p><img src="/archives/2018/04/25/zookeeper.jpg" alt=""></p><h3 id="集群规划和准备"><a href="#集群规划和准备" class="headerlink" title="集群规划和准备"></a>集群规划和准备</h3><p>基于Hadoop集群环境搭建的准备条件，集群规划：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">hadoop02</th><th style="text-align:center">hadoop03</th><th style="text-align:center">hadoop04</th><th style="text-align:center">hadoop05</th></tr></thead><tbody><tr><td style="text-align:center">namenode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">datanode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">resourcemanager</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">nodemanager</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">zookeeper</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">journalnode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">zkfc</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr></tbody></table><h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><p><code>hadoop-env.sh</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME= /usr/local/jdk1.8.0_73</div></pre></td></tr></table></figure><p><code>core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 hdfs 的 nameservice 为 myha01 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://myha01/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 hadoop 工作目录 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 zookeeper 集群访问地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定副本数 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!--指定 hdfs 的 nameservice 为 myha01，需要和 core-site.xml 中保持一致--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>myha01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- myha01 下面有两个 NameNode，分别是 nn1，nn2 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.myha01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn1 的 RPC 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.myha01.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn1 的 http 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.myha01.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn2 的 RPC 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.myha01.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn2 的 http 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.myha01.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 NameNode 的 edits 元数据在 JournalNode 上的存放位置 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/myha01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 JournalNode 在本地磁盘存放数据的位置 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 NameNode 失败自动切换 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 value值不要换行--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.myha01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></div><div class="line">          sshfence</div><div class="line">          shell(/bin/true)</div><div class="line">      <span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 使用 sshfence 隔离机制时需要 ssh 免登陆 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置 sshfence 隔离机制超时时间 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 mr 框架为 yarn 方式 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 RM 高可用 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 RM 的 cluster id --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 RM 的名字 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 分别指定 RM 的地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop04<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop05<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 zk 集群地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 要运行 MapReduce 程序必须配置的附属服务 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 YARN 集群的日志聚合功能 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- YARN 集群的聚合日志最长保留时长 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 启用自动恢复 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 制定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>slaves</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop02</div><div class="line">hadoop03</div><div class="line">hadoop04</div><div class="line">hadoop05</div></pre></td></tr></table></figure><p><strong>配置环境变量和分发安装包</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vim ~/.bashrc  #如果没找到，先执行 ll -a ~/</div><div class="line">添加两行：</div><div class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div><div class="line"></div><div class="line">分发安装包和环境变量：</div><div class="line">scp -r ~/apps/hadoop-2.7.5 hadoop@hadoop03:~/apps/</div><div class="line">...</div><div class="line">scp ~/.bashrc hadoop@hadoop03:~/</div><div class="line">...</div></pre></td></tr></table></figure><h3 id="集群初始化和启动"><a href="#集群初始化和启动" class="headerlink" title="集群初始化和启动"></a>集群初始化和启动</h3><p>HA集群的启动是有一定的顺序的，第一次启动严格按照以下步骤:</p><p><strong>启动zookeeper集群</strong></p><p>配置zookeeper集群的节点都需要启动</p><p>启动命令：<code>zkServer.sh start</code></p><p>查看状态：<code>zkServer.sh status</code></p><p><strong>启动journalnode进程：</strong></p><p>配置journalnode集群的节点都需要启动</p><p>启动命令：<code>hadoop-daemon.sh start journalnode</code></p><p><strong>格式化NameNode：</strong></p><p>在hadoop02主节点上格式化操作</p><p>命令：<code>hadoop namenode -format</code></p><p>在<code>~/data/hadoopdata/</code>中会生成集群信息，把hadoopdata文件发送到hadoop03上</p><p><code>scp -r ~/data/hadoopdata/ hadoop@hadoop03:~/data/</code></p><p><strong>格式化ZKFC</strong></p><p>在第一台机器上操作即可</p><p>命令：<code>hdfs zkfc -formatZK</code></p><p><strong>启动HDFS和YARN集群</strong></p><p>命令：<code>start-dfs.sh</code> <code>start-yarn.sh</code></p><p><strong>查看进程情况</strong></p><p>命令：<code>jps</code></p><h3 id="验证和测试"><a href="#验证和测试" class="headerlink" title="验证和测试"></a>验证和测试</h3><p><strong>验证</strong></p><p>访问HDFSWeb页面：<code>http://hadoop02:50070</code> 、<code>http://hadoop03:50070</code></p><p>hadoop02是Actice状态，hadoop03是standby状态</p><p>访问YARNweb页面：<code>http://hadoop04:8088</code> 、<code>http://hadoop05:8088</code></p><p>访问hadoop05 是 standby resourcemanager，会自动跳转到 hadoop04</p><p><strong>测试</strong></p><p>干掉active namenode的进程，看看集群变化</p><p>在上传文件的时候干掉 active namenode， 看看集群有什么变化</p><p>干掉 active resourcemanager， 看看集群有什么变化</p><p>在执行任务的时候干掉 active resourcemanager，看看集群有什么变化</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;HA：High Available，高可用。为什么需要HA机制？怎么配置HA？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Decorator Pattern（装饰者模式）</title>
    <link href="https://pross.space/archives/2018/04/16/"/>
    <id>https://pross.space/archives/2018/04/16/</id>
    <published>2018-04-16T14:59:59.000Z</published>
    <updated>2018-08-31T16:20:50.919Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>所为”设计模式”，全称是”面对对象设计模式”的简称。其次需要明白，设计模式是与语言无关，意思就是说它并不是属于某种编程语言。最后，因为Java对面向对象支持的比较好，并且在JDK当中有很多实现就是依靠装饰者模式，所以下面举例采用Java代码来说明。</p><a id="more"></a><h4 id="装饰者模式"><a href="#装饰者模式" class="headerlink" title="装饰者模式"></a>装饰者模式</h4><p>装饰者模式是在不改变原类文件和使用继承的情况下，动态的扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象。其目的是：对原类的功能增强。</p><p>在装饰模式中的各个角色有：</p><p>抽象构件（Component）：给出一个抽象接口，以规范准备接收附加责任的对象。</p><p>具体构件（Concrete Component）：定义一个将要接收附加责任的类</p><p>装饰（Decorator）角色：持有一个构建对象的实例，并实现一个与抽象构件接口一致的接口</p><p>具体装饰（Concrete Decorator）：负责给构件对象添加上附加的责任</p><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>需求：定义一个字符串数组，让字符串数组中的元素进行大小写转换。</p><p>分析：字符串数组中是没有直接可以实现大小写转换的方法，当然，字符串数组遍历出来转化为单个字符进行大小写的转换是可以，但是用这样的写法就和装饰者模式没啥事了。返回之前，因为没有直接转换方法，所以需要对其进行功能增强。</p><p>代码一现，其义自见：</p><p><code>ListTest.java</code></p><p><img src="/archives/2018/04/16/ListTest.png" alt=""></p><p><code>MyList.java</code></p><p><img src="/archives/2018/04/16/MyList.png" alt=""></p><p><code>Operator.java</code></p><p><img src="/archives/2018/04/16/Operator.png" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;所为”设计模式”，全称是”面对对象设计模式”的简称。其次需要明白，设计模式是与语言无关，意思就是说它并不是属于某种编程语言。最后，因为Java对面向对象支持的比较好，并且在JDK当中有很多实现就是依靠装饰者模式，所以下面举例采用Java代码来说明。&lt;/p&gt;
    
    </summary>
    
    
      <category term="设计模式" scheme="https://pross.space/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程案例（下）</title>
    <link href="https://pross.space/archives/2018/04/05/"/>
    <id>https://pross.space/archives/2018/04/05/</id>
    <published>2018-04-05T14:18:53.000Z</published>
    <updated>2018-05-29T13:29:04.569Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><ul><li>Versions变动版本记录</li><li>数值累加</li></ul><a id="more"></a><h4 id="Version变动版本记录"><a href="#Version变动版本记录" class="headerlink" title="Version变动版本记录"></a>Version变动版本记录</h4><p>在所有有版本变动的记录后面追加一条字段信息：该信息就是上一个版本的版本号，只限同用户</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">数据格式：</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">8</span>,<span class="number">360</span>手机助手,<span class="number">0.1</span>版本,北京</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">22</span>,<span class="number">360</span>手机助手,<span class="number">0.2</span>版本,北京</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">14</span>,<span class="number">360</span>手机助手,<span class="number">0.3</span>版本,北京</div><div class="line">...</div><div class="line">字段信息：</div><div class="line">用户ID，用户名，游戏名，小时，数据来源，游戏版本，用户所在地</div><div class="line">id, name, game, hour, source, version, city</div></pre></td></tr></table></figure><p>思路：把id、name、game、hours、source、version、city封装成<code>Version</code>对象，实现<code>WritableComparable</code>接口，并根据id、name、version先后排序。排序后判断相邻的数据版本号是否一致来进行版本号的添加。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.version;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月19日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：Version实体类</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Version</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">Version</span>&gt;</span>&#123;</div><div class="line"><span class="keyword">private</span> String id;</div><div class="line"><span class="keyword">private</span> String name;</div><div class="line"><span class="keyword">private</span> String game;</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> hour;</div><div class="line"><span class="keyword">private</span> String source;</div><div class="line"><span class="keyword">private</span> String version;</div><div class="line"><span class="keyword">private</span> String city;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> id;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.id = id;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> name;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getGame</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> game;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setGame</span><span class="params">(String game)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.game = game;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getHour</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> hour;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHour</span><span class="params">(<span class="keyword">int</span> hour)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.hour = hour;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getSource</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> source;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSource</span><span class="params">(String source)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.source = source;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getVersion</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> version;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setVersion</span><span class="params">(String version)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.version = version;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getCity</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCity</span><span class="params">(String city)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.city = city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Version</span><span class="params">(String id, String name, String game, <span class="keyword">int</span> hour, String source, String version, String city)</span> </span>&#123;</div><div class="line"><span class="keyword">super</span>();</div><div class="line"><span class="keyword">this</span>.id = id;</div><div class="line"><span class="keyword">this</span>.name = name;</div><div class="line"><span class="keyword">this</span>.game = game;</div><div class="line"><span class="keyword">this</span>.hour = hour;</div><div class="line"><span class="keyword">this</span>.source = source;</div><div class="line"><span class="keyword">this</span>.version = version;</div><div class="line"><span class="keyword">this</span>.city = city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Version</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">super</span>();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> id + <span class="string">","</span> + name + <span class="string">","</span> + game + <span class="string">","</span> + hour + <span class="string">","</span> + source+ <span class="string">","</span> + version + <span class="string">","</span> + city;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 序列化与反序列化</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">out.writeUTF(id);</div><div class="line">out.writeUTF(name);</div><div class="line">out.writeUTF(game);</div><div class="line">        out.writeInt(hour);  </div><div class="line">        out.writeUTF(source);  </div><div class="line">        out.writeUTF(version);  </div><div class="line">        out.writeUTF(city);  </div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line"><span class="keyword">this</span>.id = in.readUTF();</div><div class="line"><span class="keyword">this</span>.name = in.readUTF();</div><div class="line"><span class="keyword">this</span>.game = in.readUTF();</div><div class="line"><span class="keyword">this</span>.hour = in.readInt();</div><div class="line"><span class="keyword">this</span>.source = in.readUTF();</div><div class="line"><span class="keyword">this</span>.version = in.readUTF();</div><div class="line"><span class="keyword">this</span>.city = in.readUTF();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 比较器 排序</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Version version)</span> </span>&#123;</div><div class="line"><span class="keyword">int</span> resultID=<span class="keyword">this</span>.id.compareTo(version.getId());</div><div class="line"><span class="keyword">if</span>(resultID==<span class="number">0</span>)&#123;</div><div class="line"><span class="keyword">int</span> resultName=<span class="keyword">this</span>.name.compareTo(version.getName());</div><div class="line"><span class="keyword">if</span>(resultName==<span class="number">0</span>)&#123;</div><div class="line"><span class="keyword">return</span> <span class="keyword">this</span>.version.compareTo(version.getVersion());</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="keyword">return</span> resultName;</div><div class="line">&#125;</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="keyword">return</span> resultID;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在MR程序中，map阶段拆分数据后把数据封装到<code>Version</code>对象中，在reduce阶段进行数据迭代，判断版本号是否一致，当id和name不一致，证明不是两个不同的用户；如果一致，则进行版本号的拼接，放入Key中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.version;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月19日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：在所有有版本变动的记录后面追加一条字段信息：该信息就是上一个版本的版本号，只限同用户</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * 文件路径：E:\BigData\7_Hadoop\hadoop-mapreduce-day5\data\input_version</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMR</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span></span>&#123;</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="keyword">int</span> run=ToolRunner.run(<span class="keyword">new</span> VersionMR(), args);</div><div class="line">System.exit(run);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="comment">//指定HDFS相关参数</span></div><div class="line">Configuration conf=<span class="keyword">new</span> Configuration();</div><div class="line">FileSystem fs=FileSystem.get(conf);</div><div class="line"> </div><div class="line">Job job=Job.getInstance(conf);</div><div class="line">job.setJarByClass(VersionMR.class);</div><div class="line"></div><div class="line"><span class="comment">//指定mapper类和reduce类</span></div><div class="line">job.setMapperClass(VersionMRMapper.class);</div><div class="line">job.setReducerClass(VersionMRReduce.class);</div><div class="line"></div><div class="line"><span class="comment">//指定输出类型</span></div><div class="line">        job.setMapOutputKeyClass(Version.class);  </div><div class="line">        job.setMapOutputValueClass(NullWritable.class);  </div><div class="line">        job.setOutputKeyClass(Text.class);  </div><div class="line">        job.setOutputValueClass(NullWritable.class); </div><div class="line">        </div><div class="line">        <span class="comment">//指定路径</span></div><div class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day5\\data\\input_version"</span>);  </div><div class="line">        Path outputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day5\\data\\output_version"</span>); </div><div class="line">        <span class="comment">//如果输出路径存在则删除</span></div><div class="line">       <span class="keyword">if</span>(fs.exists(outputPath))&#123;</div><div class="line">       fs.delete(outputPath,<span class="keyword">true</span>);</div><div class="line">       &#125;</div><div class="line">       FileInputFormat.setInputPaths(job, inputPath);</div><div class="line">       FileOutputFormat.setOutputPath(job, outputPath);</div><div class="line">       </div><div class="line">       <span class="comment">//提交任务</span></div><div class="line">       <span class="keyword">boolean</span> bool = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">       <span class="keyword">return</span> bool? <span class="number">0</span> : <span class="number">1</span> ;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * map组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Version</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 数据格式：</span></div><div class="line"><span class="comment"> * 20170308,黄渤,光环斗地主,8,360手机助手,0.1版本,北京</span></div><div class="line"><span class="comment"> * 20170308,黄渤,光环斗地主,5,360手机助手,0.1版本,北京</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * 字段信息：</span></div><div class="line"><span class="comment"> * 用户ID，用户名，游戏名，小时，数据来源，游戏版本，用户所在地</span></div><div class="line"><span class="comment"> * id, name, game, hour, source, version, city </span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException ,InterruptedException </span>&#123;</div><div class="line"><span class="comment">//拆分数据</span></div><div class="line">String[] split = value.toString().split(<span class="string">","</span>);</div><div class="line"><span class="comment">//把数据封装到对象中</span></div><div class="line">Version version = <span class="keyword">new</span> Version(split[<span class="number">0</span>], split[<span class="number">1</span>],split[<span class="number">2</span>],Integer.parseInt(split[<span class="number">3</span>]), split[<span class="number">4</span>], split[<span class="number">5</span>], split[<span class="number">6</span>]);</div><div class="line">context.write(version, NullWritable.get());</div><div class="line">&#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * reduce组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMRReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Version</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line">String lastID=<span class="keyword">null</span>;</div><div class="line">String lastName=<span class="keyword">null</span>;</div><div class="line">String lastVersion=<span class="keyword">null</span>;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Version key, Iterable&lt;NullWritable&gt; values,Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">            <span class="keyword">for</span>(NullWritable nvl:values)&#123;</div><div class="line"><span class="keyword">if</span>(lastVersion==<span class="keyword">null</span>)&#123;</div><div class="line"><span class="comment">//第一次进入程序，lastVersion为空，直接打印，因为相当于没有上一条数据</span></div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()), NullWritable.get());</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="comment">//当ID和Name一致时</span></div><div class="line"><span class="keyword">if</span>(lastID.equals(key.getId()) &amp;&amp; lastName.equals(key.getName()))&#123;</div><div class="line"><span class="comment">//判断上个版本号和当前版本号是否一致</span></div><div class="line"><span class="keyword">if</span>(!lastVersion.equals(key.getVersion()))&#123;</div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()+<span class="string">"-"</span>+lastVersion), NullWritable.get());</div><div class="line">&#125;</div><div class="line"><span class="comment">//当ID和Name有一个不一致时，证明是两个不同的用户</span></div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()), NullWritable.get());</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">//进行数据迭代处理，方便本次和下次的数据进行对比</span></div><div class="line">lastID=key.getId();</div><div class="line">lastName=key.getName();</div><div class="line">lastVersion=key.getVersion();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="数值累加"><a href="#数值累加" class="headerlink" title="数值累加"></a>数值累加</h4><p>求所有数对应位置的叠加和</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">举例：</div><div class="line"><span class="number">0001</span>.txt文件有数据：</div><div class="line"><span class="number">1</span></div><div class="line"><span class="number">2</span></div><div class="line"><span class="number">3</span></div><div class="line"><span class="number">4</span></div><div class="line">...</div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">0002</span>.txt文件有数据：</div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line">...</div><div class="line">返回结果是：</div><div class="line"><span class="number">1</span><span class="number">1</span></div><div class="line"><span class="number">2</span><span class="number">3</span></div><div class="line"><span class="number">3</span><span class="number">6</span></div><div class="line"><span class="number">4</span><span class="number">10</span></div><div class="line">...</div><div class="line"><span class="number">10</span><span class="number">55</span></div><div class="line">也就是，每一行数字后面都追加一个累加到该数字的和值</div></pre></td></tr></table></figure><p>思路：先求出每个文件的总和，然后按照文件的顺序进行叠加</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;Versions变动版本记录&lt;/li&gt;&lt;li&gt;数值累加&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程框架之Shuffle详述</title>
    <link href="https://pross.space/archives/2018/03/28/"/>
    <id>https://pross.space/archives/2018/03/28/</id>
    <published>2018-03-28T10:43:32.000Z</published>
    <updated>2018-06-03T07:16:47.878Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p><img src="/archives/2018/03/28/shuffle.png" alt=""></p><a id="more"></a><h3 id="Shuffle是什麽"><a href="#Shuffle是什麽" class="headerlink" title="Shuffle是什麽"></a>Shuffle是什麽</h3><p>Shuffle：数据混洗，核心机制有数据分区、排序、局部聚合、缓存、拉取、再合并排序。</p><p>Shuffle是MapReduce处理流程中的一个核心过程，由封面图可以看出，它的每一个处理步骤是分散在各个mapTask和ReduceTask节点上完成的，整体分为3个操作：Partitioner（分区，NumReduceTask只有一个或者没有分区操作将不会起作用）、Sort（排序，根据key排序，没有reducer阶段，那么就不会对key排序）、Combiner（合并，局部value的合并，可选组件）</p><h3 id="MapReuce中的Shuffle"><a href="#MapReuce中的Shuffle" class="headerlink" title="MapReuce中的Shuffle"></a>MapReuce中的Shuffle</h3><p>我们先捋一捋MapReduce的执行流程：</p><p><img src="/archives/2018/03/28/mapreduce.png" alt=""></p><p>我们知道，一个大文件需要处理，在HDFS上是以block块的形式存放，Hadop2.x之后的每个block默认为128M，默认副本为3份，运行每个map任务会处理一个split，一般split大小和block相同，那么有多少block就有多少个map任务。</p><p>每个map任务处理完输入的split后会把结果写入到内存的一个环形缓冲区，写入过程中会进行简单的排序，默认大小为100M，当缓冲区的大小使用超过一定的阀值（默认为80%），一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续向环形缓冲区中写入数据。</p><p>数据溢写入到磁盘之前，首先会根据Reducer的数量划分成同数量的分区（默认为HashPartition），每个分区中的数据都会有后台线程根据map任务的输出结果进行内排序（字典数序，自然数顺序或自定义顺序comparator），如果有combiner（作用是使map输出更紧凑，写到本地磁盘和传给reducer的数据更少），Mapper会在溢写到磁盘之前排好序的输出上运行，最后在本地生成分好区排好序的小文件；如果小文件数量达到默认值10（mapreduce.task.io.sort.factor），则会合并成一个大文件，这个结果文件的分区存在一个映射关系，比如 0~1024 字节内容为 0 号分区内容，1025~2048 字节内容为 1 号分区内容；如果map向环形缓冲区写入数据的速度大于向本地写入数据，环形缓冲区就会被写满，向环形缓冲区写入的数据的线程就会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件。</p><p>Reduce任务启动，Reducer个数由mapred-site.xml的mapreduce.job.reducers配置决定，或者初始化job时调用Job.setNumReduceTask来设置；Reducer中的一个线程定期向MRAppMastrer询问Mapper输出结果文件位置，mapper结束后会向MRAppMaster汇报信息；从而Reducer得知Mapper状态，得到map结果文件目录。</p><p>当有一个Mapper结束时，reduce任务进入复制阶段，通过http协议（hadoop内置了netty容器）把所有Mapper结果文件的对应的分区数据复制过来，比如：编号为0的reduce复制maop结果文件中0号分区数据，1号reduce复制map结果文件中1号分区的数据等；Reducer可以并行复制Mappere的结果，默认线程数为5（mapred-site.xml：mapreduce.reduce.shuffle.parallelcoopies）；</p><p>另外：如果map结果文件相当小，则会被直接复制到reduceNodeManager的内存中（缓冲区大小由mapred.site.xml：mapreduce.shuffle.input.buffer.percent指定，默认为0.7），一旦缓冲区达到reduce的阀值大小0.66或写入到reduceNodeManager内存中文件个数达到map输出阀值1000（mapred-site.xml：mapreduce.reduce.merge.inmen.threshold），reduce就会把map结果文件合并溢写到本地。</p><p>复制阶段完成后，Ruducer进入到Merge阶段，循环的合并map结果文件，维持其顺序排列，合并因子默认为10（mapred-site.xml：mapreduce.task.io.start.factor），经过不断的Merge后得到一个”最终文件”，可能存储在磁盘也可能存在内存中。</p><p>“最终文件”输入到reduce进行计算，计算结果输入到HDFS文件系统中存储。</p><p><strong>shuffle流程</strong></p><p><img src="/archives/2018/03/28/shuffle-rim.png" alt=""></p><p>回到map阶段，mapTask 是收集 map()方法输出的 K-V 对，放到内存缓冲区 kvbuffer中（环形缓冲区：内存中的一种首尾相连的数据结构，kvbuffer 包含数据区和索引区）</p><p>从内存缓冲区中的数据区的数据不断溢出本地磁盘文件 file.out，可能会溢出多次，则会有多个文件，相应的内存缓冲区中的索引区数据溢出为磁盘索引文件 file.out.index</p><p>多个溢出文件会被合并成大的溢出文件</p><p>在溢出过程中，及合并的过程中，都要调用 partitoner 进行分区和针对 key 进行排序</p><p>在数据量大的时候，可以对 mapTask 结果启用压缩，将 mapreduce.map.output.compress设为 true，并使用 mapreduce.map.output.compress.codec 设置使用的压缩算法，可以提高数据传输到 reducer 端的效率</p><p>reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区数据，取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文件再进行合并（归并排序）</p><p>合并成大文件后，shuffle 的过程也就结束了，后面进入 reduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法）</p><p>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快</p><blockquote><p>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M</p><p>缓冲区的溢写比也可以通过参数调整，参数：mapreduce.map.sort.spill.percent 默认 0.8</p></blockquote><p><strong>带上shuffle后mapreduce执行流程图</strong></p><p><img src="/archives/2018/03/28/mapreduce-shuffle.png" alt=""></p><p><strong>为什么需要环形缓冲区</strong></p><p>Map过程中环形缓冲区是指数据被map处理之后会先放入内存，内存中的这片区域就是环形缓冲区。数据从内存要写入磁盘中时，数据会被先写入到磁盘缓冲区，磁盘缓冲区满了再把数据写入磁盘。</p><blockquote><p>磁盘缓冲区是为了平滑不同I/O设备的速度差。</p></blockquote><p>磁盘是分区分块存储的。如果是机械硬盘，是分磁道和扇区的。当磁头转到一个扇区的某磁道时，开始读取数据，如果只读取了 100KB 的数据，这时操作系统就想，磁头转到这儿看不容易啊，反正来都来了，顺带多读点数据吧，万一用的着呢。</p><p>所以，读取数据的时候也是通过缓冲区的。</p><blockquote><p>如果应用的数据存放在不同的磁道，不同的扇区，那么读取的效率是很低的，这被称为磁盘碎片，所以 windows 有个操作叫“整理磁盘碎片”。</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;&lt;img src=&quot;/archives/2018/03/28/shuffle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程案例（上）</title>
    <link href="https://pross.space/archives/2018/03/18/"/>
    <id>https://pross.space/archives/2018/03/18/</id>
    <published>2018-03-18T04:01:05.000Z</published>
    <updated>2018-05-29T13:29:14.280Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><ul><li>单词计数WordCount</li><li>数组排序并加序号</li><li>共同好友</li></ul><a id="more"></a><h4 id="MapRedeuce相关概念"><a href="#MapRedeuce相关概念" class="headerlink" title="MapRedeuce相关概念"></a>MapRedeuce相关概念</h4><p>MapReduce是一个分布式运算程序的编程框架，是用户开发”基于Hadoop的数据分析应用”的核心框架</p><p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上</p><p>FileInputFormat中默认的切片机制：切片大小默认等于block的大小</p><p>用户编写MR程序分为三个部分：Mapper、Reducer、Driver（提交运行MR程序的客户端）</p><p>Mapper的输入数据和输出数据是K-V对的形式，Reducer的输入数据类型对应Mapper的输出类型</p><p>ReduceTask进程对每一组相同K的K-V组调用一次reduce()方法</p><h4 id="单词计数WordCount"><a href="#单词计数WordCount" class="headerlink" title="单词计数WordCount"></a>单词计数WordCount</h4><p>思路：逐行读取文本内容–&gt;把读取到的一行文本内容切割为一个一个的单词–&gt;把每个单词出现一次的信息记录为一个key-value，key=word，value=1–&gt;收集所有相同的单词，然后统计value值的总和</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：例子程序 wordcount</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMR</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="comment">//指定hdfs相关的参数,没有涉及集群文件可以把set信息注释</span></div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line"><span class="comment">//conf.set("fs.defaultFS", "hdfs://hadoop02:9000");</span></div><div class="line"><span class="comment">//System.setProperty("HADOOP_USER_NAME", "hadoop");</span></div><div class="line">FileSystem fs = FileSystem.get(conf);</div><div class="line"></div><div class="line"><span class="comment">//通过Configuration对象获取job对象，job对象会组织所有的该mapreduce程序所有的各种组件</span></div><div class="line">Job job = Job.getInstance(conf);</div><div class="line"><span class="comment">//设置jar包所在路径，一般即为类名的反射</span></div><div class="line">job.setJarByClass(WordCountMR.class);</div><div class="line"></div><div class="line"><span class="comment">//指定mapper和reducer类</span></div><div class="line">job.setMapperClass(WordCountMRMapper.class);</div><div class="line">job.setReducerClass(WordCountMRReducer.class);</div><div class="line"><span class="comment">//指定mapper和reduce的输入输出类型，如果reducer的输入输出类型和mapper一致可以省略</span></div><div class="line">job.setMapOutputKeyClass(Text.class);</div><div class="line">job.setMapOutputValueClass(IntWritable.class);</div><div class="line">job.setOutputKeyClass(Text.class);</div><div class="line">job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line"><span class="comment">//设置程序的输入路径和输出路径</span></div><div class="line">Path inputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day2\\data\\score\\input"</span>);</div><div class="line">Path outputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day2\\data\\score\\output_wordcount"</span>);</div><div class="line">FileInputFormat.setInputPaths(job, inputPath);</div><div class="line"><span class="comment">//判断输出路径是否存在，存在则删除</span></div><div class="line"><span class="keyword">if</span>(fs.exists(outputPath))&#123;</div><div class="line">fs.delete(outputPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job, outputPath);</div><div class="line"></div><div class="line"><span class="comment">//提交任务，布尔值决定要不要将运行进度信息输出给用户</span></div><div class="line"><span class="keyword">boolean</span> isDone = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line"><span class="comment">//主线程根据mapreduce程序的运行结果成功与否决定是否退出</span></div><div class="line">System.exit(isDone ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * mapper组件</span></div><div class="line"><span class="comment"> * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量 </span></div><div class="line"><span class="comment"> * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本 </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">//定义输出类型，避免重复实例化其对象</span></div><div class="line">Text outKey=<span class="keyword">new</span> Text();</div><div class="line">IntWritable outValue=<span class="keyword">new</span> IntWritable();</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">//切分单词</span></div><div class="line">String[] words=value.toString().split(<span class="string">" "</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span>(String word:words)&#123;</div><div class="line"><span class="comment">//每个单词计数一次，也就是把单词组织成&lt;hello，1&gt;这样的K-V</span></div><div class="line">outKey.set(word);</div><div class="line">outValue.set(<span class="number">1</span>);</div><div class="line">context.write(outKey, outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * reduce组件</span></div><div class="line"><span class="comment"> * 输入类型就是Map阶段的处理结果，输出类型就是Reduce最后的输出 </span></div><div class="line"><span class="comment"> * reducetask将这些收到K-V数据拿来处理时，是这样调用我们的reduce方法的： </span></div><div class="line"><span class="comment"> * 先将自己收到的所有的K-V对按照K分组（根据K是否相同） 将某一组K-V中的第一个K-V中的K传给reduce方法的key变量</span></div><div class="line"><span class="comment"> * 把这一组kv中所有的v用一个迭代器传给reduce方法的变量values </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMRReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">//结果汇总</span></div><div class="line"><span class="keyword">int</span> sum=<span class="number">0</span>;</div><div class="line"><span class="keyword">for</span>(IntWritable v:values)&#123;</div><div class="line">sum+=v.get();</div><div class="line">&#125;</div><div class="line"><span class="comment">//输出</span></div><div class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="数组排序并加序号"><a href="#数组排序并加序号" class="headerlink" title="数组排序并加序号"></a>数组排序并加序号</h4><p>思路：利用shuffle阶段会把K-V对中的key值自动排序功能，先把数组元素放到key中进行排序，然后根据排序的顺序在reduce阶段为元素编上序号</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：数字排序并加序号</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySort</span> </span>&#123;</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span>  number=<span class="number">0</span>;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">Job job = Job.getInstance(conf);</div><div class="line">job.setJarByClass(ArraySort.class);</div><div class="line"></div><div class="line">job.setMapperClass(ArraySortMapper.class);</div><div class="line">job.setReducerClass(ArraySortReducer.class);</div><div class="line"></div><div class="line">job.setOutputKeyClass(IntWritable.class);</div><div class="line">job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/array/input"</span>));</div><div class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/array/output2"</span>));</div><div class="line"></div><div class="line"><span class="keyword">boolean</span> isDone = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">System.exit(isDone ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * Mapper组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">//定义输出类型，避免重复实例化其对象</span></div><div class="line">IntWritable outKey=<span class="keyword">new</span> IntWritable();</div><div class="line">IntWritable outValue=<span class="keyword">new</span> IntWritable();</div><div class="line">      </div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">           <span class="comment">//去除到元素的前后空格</span></div><div class="line">String tempNumber = value.toString().trim();</div><div class="line"><span class="keyword">int</span> outKeyTemp=Integer.parseInt(tempNumber);</div><div class="line">           outKey.set(outKeyTemp);</div><div class="line">           outValue.set(<span class="number">0</span>);</div><div class="line">context.write(outKey,outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * Ruduce组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>,<span class="title">IntWritable</span>, <span class="title">IntWritable</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;IntWritable&gt; values,Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="keyword">for</span>(IntWritable t:values)&#123;</div><div class="line">number++;</div><div class="line">context.write(<span class="keyword">new</span> IntWritable(number),key);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>思考：当本地文件中的数据过大时，这样的排序并加编号的方案是否可行 ?</p><h4 id="求共同好友"><a href="#求共同好友" class="headerlink" title="求共同好友"></a>求共同好友</h4><p>数据格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">A:B,C,D,F,E,O (表示B,C,D,E,F,O是A用户的好友)</div><div class="line">B:A,C,E,K</div><div class="line">C:F,A,D,I</div><div class="line">D:A,E,F,L</div><div class="line">E:B,C,D,M,L</div><div class="line">F:A,B,C,D,E,O,M</div><div class="line">G:A,C,D,E,F</div><div class="line">H:A,C,D,E,O</div><div class="line">I:A,O</div><div class="line">J:B,O</div><div class="line">K:A,C,D</div><div class="line">L:D,E,F</div><div class="line">M:E,F,G</div><div class="line">O:A,H,I,J,K</div><div class="line">...</div></pre></td></tr></table></figure><p>输出格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">A-B:C E  (A和B的共同还有是E,C)</div><div class="line">A-C:D F</div></pre></td></tr></table></figure><p>逆推法：结果为A-B:C E，那么key:A-B；value:C E，得知A-B:C；A-B:E，意思是E是A和B的共同好友；继续可以转换A:E，B:E；那么单看A的好友可以拆分为，A:B，A:C，A:D。</p><p>我们整理来一下，从源数据格式推出结果：A:B,C,D,F,E,O / B:A,C,E,K –&gt; A:C,E / B:C,E –&gt;A-B:E / A-B:C –&gt; A-B:C E，这样推理过来就需要两个MR来解决，拆分来写，只需要后一个MR程序调用前一个MR程序的结果即可；多job串联，可以使用JobControl这个类把具有依赖关系的多个job串联起来，然后调度先后执行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.friend;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> *         create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> *         content：找出共同好友</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend</span></span>&#123;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">Configuration conf1 = <span class="keyword">new</span> Configuration();</div><div class="line">Configuration conf2 = <span class="keyword">new</span> Configuration();</div><div class="line">FileSystem fs = FileSystem.get(conf1);</div><div class="line"></div><div class="line"><span class="comment">// 第一个job任务</span></div><div class="line">Job job1 = Job.getInstance(conf1);</div><div class="line">job1.setJarByClass(CommonFriend.class);</div><div class="line">job1.setMapperClass(CommonFriend1Mapper.class);</div><div class="line">job1.setReducerClass(CommonFriend1Reduce.class);</div><div class="line"></div><div class="line">job1.setMapOutputKeyClass(Text.class);</div><div class="line">job1.setMapOutputValueClass(Text.class);</div><div class="line">job1.setOutputKeyClass(Text.class);</div><div class="line">job1.setOutputValueClass(Text.class);</div><div class="line"></div><div class="line">Path inputPath1 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/input"</span>);</div><div class="line">Path outputPath1 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_merge"</span>);</div><div class="line">FileInputFormat.setInputPaths(job1, inputPath1);</div><div class="line"><span class="keyword">if</span> (fs.exists(outputPath1)) &#123;</div><div class="line">fs.delete(outputPath1, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job1, outputPath1);</div><div class="line"></div><div class="line"><span class="comment">//第二个job任务</span></div><div class="line">Job job2 =Job.getInstance(conf2);</div><div class="line">job2.setMapperClass(CommonFriend2Mapper.class);</div><div class="line">job2.setReducerClass(CommonFriend2Reducer.class);</div><div class="line">job2.setMapOutputKeyClass(Text.class);</div><div class="line">job2.setMapOutputValueClass(Text.class);</div><div class="line">job2.setOutputKeyClass(Text.class);</div><div class="line">job2.setOutputValueClass(Text.class);</div><div class="line">Path inputPath2 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_merge"</span>);</div><div class="line">Path outputPath2 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_lastMerge"</span>);</div><div class="line">FileInputFormat.setInputPaths(job2, inputPath2);</div><div class="line"><span class="keyword">if</span>(fs.exists(outputPath2))&#123;</div><div class="line">fs.delete(outputPath2, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job2, outputPath2);</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 多个job串联</span></div><div class="line"><span class="comment"> * 使用JobControl把具有依赖的多个job串联起来</span></div><div class="line"><span class="comment"> */</span></div><div class="line">JobControl control=<span class="keyword">new</span> JobControl(<span class="string">"CommonFriend"</span>);</div><div class="line">ControlledJob ajob=<span class="keyword">new</span> ControlledJob(job1.getConfiguration());</div><div class="line">ControlledJob bjob=<span class="keyword">new</span> ControlledJob(job2.getConfiguration());</div><div class="line"><span class="comment">// bjob的执行依赖ajob</span></div><div class="line">bjob.addDependingJob(ajob);</div><div class="line"></div><div class="line">control.addJob(ajob);</div><div class="line">control.addJob(bjob);</div><div class="line"></div><div class="line">Thread t=<span class="keyword">new</span> Thread(control);</div><div class="line">t.start();</div><div class="line"></div><div class="line"><span class="comment">//等待0.5秒执行下一个</span></div><div class="line"><span class="keyword">while</span>(!control.allFinished())&#123;</div><div class="line">Thread.sleep(<span class="number">500</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line">System.exit(<span class="number">0</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 第一个MR程序</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend1Mapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 数据源格式 </span></div><div class="line"><span class="comment"> * A:B,C,D,F,E,O B:A,C,E,K</span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line">Text outkey = <span class="keyword">new</span> Text();</div><div class="line">Text outValue = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">// 拆分</span></div><div class="line">String[] user_friends = value.toString().split(<span class="string">":"</span>);</div><div class="line">String user = user_friends[<span class="number">0</span>];</div><div class="line">String[] friends = user_friends[<span class="number">1</span>].split(<span class="string">","</span>);</div><div class="line"><span class="keyword">for</span> (String friend : friends) &#123;</div><div class="line">outkey.set(friend);</div><div class="line">outValue.set(user);</div><div class="line">context.write(outkey, outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend1Reduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text keyOut = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">ArrayList&lt;String&gt; userList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"><span class="keyword">for</span> (Text t : values) &#123;</div><div class="line">userList.add(t.toString());</div><div class="line">&#125;</div><div class="line">Collections.sort(userList);</div><div class="line"><span class="keyword">int</span> size = userList.size();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size - <span class="number">1</span>; i++) &#123;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; size; j++) &#123;</div><div class="line">String outKey = userList.get(i) + <span class="string">"-"</span> + userList.get(j);</div><div class="line">keyOut.set(outKey);</div><div class="line">context.write(keyOut, key);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 第二个MR程序</span></div><div class="line"><span class="comment"> * 数据格式：</span></div><div class="line"><span class="comment"> * B-CA</span></div><div class="line"><span class="comment"> * B-DA</span></div><div class="line"><span class="comment"> * B-FA</span></div><div class="line"><span class="comment"> * B-GA</span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend2Mapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text keyOut = <span class="keyword">new</span> Text();</div><div class="line">Text valueOut = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">String[] split = value.toString().split(<span class="string">"\t"</span>);</div><div class="line">keyOut.set(split[<span class="number">0</span>]);</div><div class="line">valueOut.set(split[<span class="number">1</span>]);</div><div class="line">context.write(keyOut, valueOut);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend2Reducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text valueOut = <span class="keyword">new</span> Text();</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line"><span class="keyword">for</span> (Text t : values) &#123;</div><div class="line">sb.append(t.toString()).append(<span class="string">" "</span>);</div><div class="line">&#125;</div><div class="line">String outValueStr = sb.toString();</div><div class="line">valueOut.set(outValueStr);</div><div class="line">context.write(key, valueOut);</div><div class="line">&#125;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 输出数据：</span></div><div class="line"><span class="comment"> * A-BE C </span></div><div class="line"><span class="comment"> * A-CD F </span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;单词计数WordCount&lt;/li&gt;&lt;li&gt;数组排序并加序号&lt;/li&gt;&lt;li&gt;共同好友&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HDFS核心API编程案例</title>
    <link href="https://pross.space/archives/2018/03/17/"/>
    <id>https://pross.space/archives/2018/03/17/</id>
    <published>2018-03-17T14:57:38.000Z</published>
    <updated>2018-05-29T12:57:32.688Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><ul><li>删除HDFS集群中所有的空文件和空目录</li><li>使用流的方式上传下载文件</li><li>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</li><li>统计出HDFS文件系统中平均副本数</li></ul><a id="more"></a><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p><a href="https://prosscode.github.io/2018/Hadoop集群环境搭建/" target="_blank" rel="external">Hadoop集群环境搭建</a>–&gt;将”windows平台编译hadoop安装包”解压，并配置环境变量–&gt;准备hadoop-eclipse-plugin.jar插件，配置到eclipse–&gt;eclipse中进入Map/Reduce Locations配置集群信息–&gt;Add User Library–&gt;添加common、hdfs、mapreduce、yarn相关依赖库–&gt;新建Java项目开始编写代码</p><p>做那么多操作，无非是要做到<strong>在本地eclipse中编写的程序能够操作HDFS集群中的文件</strong></p><h4 id="公共工具类"><a href="#公共工具类" class="headerlink" title="公共工具类"></a>公共工具类</h4><p><code>HDFSUtils.java</code>：初始化FileSystem对象和关闭FileSystem</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Random;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月14日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：初始化FileSystem对象和关闭FileSystem</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSUtils</span> </span>&#123;</div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> FileSystem fs=<span class="keyword">null</span>;</div><div class="line">  </div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 初始化FileSystem对象</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">Configuration conf=<span class="keyword">new</span> Configuration();</div><div class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop02:9000"</span>);</div><div class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</div><div class="line">fs=FileSystem.get(conf);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 关闭FileSystem的连接</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">fs.close();</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>所需要的依赖包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.FileInputStream;</div><div class="line"><span class="keyword">import</span> java.io.FileOutputStream;</div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.io.OutputStream;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div></pre></td></tr></table></figure><h4 id="删除HDFS集群中所有空文件和空目录"><a href="#删除HDFS集群中所有空文件和空目录" class="headerlink" title="删除HDFS集群中所有空文件和空目录"></a>删除HDFS集群中所有空文件和空目录</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 删除HDFS集群中的所有空文件和空目录</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteEmptyDir</span><span class="params">(Path path)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line"><span class="comment">// 当前路径就是空目录时</span></div><div class="line">FileStatus[] listFile = HDFSUtils.fs.listStatus(path);</div><div class="line"><span class="keyword">if</span> (listFile.length == <span class="number">0</span>) &#123;</div><div class="line"><span class="comment">//删除空目录</span></div><div class="line">HDFSUtils.fs.delete(path, <span class="keyword">true</span>);</div><div class="line"><span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//如果不是空文件，先获取指定目录下的文件和子目录</span></div><div class="line">RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = HDFSUtils.fs.listLocatedStatus(path);</div><div class="line"></div><div class="line"><span class="keyword">while</span> (listLocatedStatus.hasNext()) &#123;</div><div class="line">LocatedFileStatus next = listLocatedStatus.next();</div><div class="line">          <span class="comment">//获取当前目录和其父目录</span></div><div class="line">Path currentPath = next.getPath();</div><div class="line">Path parentPath=next.getPath().getParent();</div><div class="line"></div><div class="line"><span class="comment">// 如果是文件夹，继续往下遍历</span></div><div class="line"><span class="keyword">if</span> (next.isDirectory()) &#123;</div><div class="line"></div><div class="line"><span class="comment">// 如果是空目录，删除</span></div><div class="line"><span class="keyword">if</span> (HDFSUtils.fs.listStatus(currentPath).length == <span class="number">0</span>) &#123;</div><div class="line">HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</div><div class="line"><span class="keyword">if</span>(HDFSUtils.fs.listStatus(parentPath).length==<span class="number">0</span>)&#123;</div><div class="line">HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line"><span class="comment">// 不是空目录，那么则重新遍历</span></div><div class="line"><span class="keyword">if</span> (HDFSUtils.fs.exists(currentPath)) &#123;</div><div class="line">AchieveClass.deleteEmptyDir(currentPath);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 如果是文件</span></div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line"><span class="comment">// 获取文件的长度</span></div><div class="line"><span class="keyword">long</span> fileLength = next.getLen();</div><div class="line"><span class="comment">// 当文件是空文件时， 删除</span></div><div class="line"><span class="keyword">if</span> (fileLength == <span class="number">0</span>) &#123;</div><div class="line">HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹，这里需要判断一下</span></div><div class="line"><span class="keyword">int</span> length = HDFSUtils.fs.listStatus(parentPath).length;</div><div class="line"><span class="keyword">if</span>(length == <span class="number">0</span>)&#123;</div><div class="line">HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">     HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="使用流的方式上传下载文件"><a href="#使用流的方式上传下载文件" class="headerlink" title="使用流的方式上传下载文件"></a>使用流的方式上传下载文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 使用流的方式上传文件</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath  上传的本地路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> desPath  上传到HDFS上后的文件名称路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putFileByStream</span><span class="params">(String srcPath,String desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">InputStream in = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(srcPath));</div><div class="line">      <span class="comment">//Path是HDFS上的文件路径</span></div><div class="line">FSDataOutputStream out = HDFSUtils.fs.create(<span class="keyword">new</span> Path(desPath));</div><div class="line">IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</div><div class="line">System.out.println(<span class="string">"put successfully"</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 使用流的方式下载文件 </span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath HDFS上的下载文件的路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> desPath 下载到本地的文件路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getFileByStream</span><span class="params">(Path srcPath,File desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">FSDataInputStream in=HDFSUtils.fs.open(srcPath);</div><div class="line">OutputStream out=<span class="keyword">new</span> FileOutputStream(desPath);</div><div class="line">IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"><a href="#统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比" class="headerlink" title="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"></a>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 统计出 HDFS文件系统中文件大小小于 HDFS集群中的默认块大小的文件占比 </span></div><div class="line"><span class="comment"> * 默认块大小为128MB</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">lessBlockSizeOfFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">FileStatus[] listStatus = HDFSUtils.fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</div><div class="line"><span class="comment">// 文件总数</span></div><div class="line"><span class="keyword">int</span> count = listStatus.length;</div><div class="line"><span class="comment">// 小于block大小的文件数个数</span></div><div class="line"><span class="keyword">int</span> lessBlock = <span class="number">0</span>;</div><div class="line">      <span class="comment">// 遍历</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         <span class="comment">//如果文件大小小于128M，这lessBlock+1</span></div><div class="line"><span class="keyword">if</span> (listStatus[i].getLen() &lt;= <span class="number">134217728</span>) &#123;</div><div class="line">lessBlock += <span class="number">1</span>;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">System.out.println(<span class="string">"文件总数量为："</span> + count + <span class="string">"个\n小于默认block的文件数量为："</span> + lessBlock + <span class="string">"个"</span> + <span class="string">"\n文件大小小于默认块大小的文件占比:"</span></div><div class="line">+ (lessBlock*<span class="number">1</span>D / count) * <span class="number">100</span> + <span class="string">"%"</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="统计出HDFS文件系统中平均副本数"><a href="#统计出HDFS文件系统中平均副本数" class="headerlink" title="统计出HDFS文件系统中平均副本数"></a>统计出HDFS文件系统中平均副本数</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * HDFS文件系统中的平均副本数（副本总数/总数据块数）</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">avgRepofBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line"><span class="comment">// 副本总数</span></div><div class="line"><span class="keyword">int</span> repCount = <span class="number">0</span>;</div><div class="line"><span class="comment">// 数据块总数</span></div><div class="line"><span class="keyword">int</span> blockCount = <span class="number">0</span>;</div><div class="line">     </div><div class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HDFSUtils.fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</div><div class="line"><span class="keyword">while</span> (listFiles.hasNext()) &#123;</div><div class="line">LocatedFileStatus next = listFiles.next();</div><div class="line"><span class="keyword">int</span> BlockNum = next.getBlockLocations().length;</div><div class="line"><span class="keyword">if</span> (BlockNum != <span class="number">0</span>) &#123;</div><div class="line"><span class="keyword">int</span> repNum = next.getReplication();</div><div class="line"><span class="keyword">int</span> oneRepCount = BlockNum * repNum;</div><div class="line">repCount += oneRepCount;</div><div class="line">blockCount += BlockNum;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">System.out.println(<span class="string">"副本总数："</span> + repCount + <span class="string">"\n数据块总数："</span> + blockCount + <span class="string">"\n平均副本数："</span> + repCount*<span class="number">1</span>D / blockCount);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;删除HDFS集群中所有的空文件和空目录&lt;/li&gt;&lt;li&gt;使用流的方式上传下载文件&lt;/li&gt;&lt;li&gt;统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比&lt;/li&gt;&lt;li&gt;统计出HDFS文件系统中平均副本数&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HDFS核心设计</title>
    <link href="https://pross.space/archives/2018/03/11/"/>
    <id>https://pross.space/archives/2018/03/11/</id>
    <published>2018-03-11T00:43:04.000Z</published>
    <updated>2018-05-29T12:57:58.601Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><ul><li>心跳机制</li><li>安全模式</li><li>副本存放策略</li><li>负载均衡</li></ul><a id="more"></a><h3 id="HDFS相关概念"><a href="#HDFS相关概念" class="headerlink" title="HDFS相关概念"></a>HDFS相关概念</h3><p>HDFS（Hadoop Distributed File System Hadoop）分布式文件系统，主要用来解决海量数据的存储问题</p><p>HDFS中的文件在物理上是分块（block）存储，块的大小可以通过配置参数（dfs.blocksize）来规定，默认在Hadoop2.x版本中是128MB。</p><p>HDFS文件的各个block的存储管理由DataNode节点承担，DataNode是HDFS集群从节点，每一个block都可以在多个DataNode上存储多个副本（副本数量可以通过参数设置dfs.replication，默认是3）。</p><p>HDFS是设计成适应一次写入，多次读出的场景，不支持文件的修改。</p><p>HDFS核心API：Configuration、FileSystem。</p><h3 id="HDFS架构解释"><a href="#HDFS架构解释" class="headerlink" title="HDFS架构解释"></a>HDFS架构解释</h3><p>在上一篇的<a href="https://prosscode.github.io/2018/Hadoop集群环境搭建/" target="_blank" rel="external">Hadoop集群环境搭建</a>中对集群规划分为：NameNode、DataNode、SecondaryNameNode以及ResourceManager和NodeManager。我们只知道需要这样去规划一个基本的分布式集群，并不知其所以然，那么这里对HDFS部分名词阐释：</p><p>主节点Namenode：掌管文件系统目录树，管理文件系统的元数据（一个block元信息消耗大约150byte的内存），负责保持和分配文件副本的数量，并处理客户端读写的请求。客户端请求访问HDFS都是通过向NameNode申请来进行的。</p><p>从节点DataNode：存储整个集群的所有的数据块，处理真正的数据读写。通过心跳机制定期汇报给NameNode有关block的信息。</p><p>SecondaryNameNode：严格说并不是NameNode备份节点，而是NameNode的助手，主要给NameNode分担压力之用。</p><p>下图可以很好的帮助理解</p><p><img src="/archives/2018/03/11/1.jpg" alt=""></p><h3 id="HDFS核心设计"><a href="#HDFS核心设计" class="headerlink" title="HDFS核心设计"></a>HDFS核心设计</h3><p><strong>心跳机制（HearBeat）</strong></p><p>在上面架构解释中说到，DataNode通过心跳机制定期汇报给NameNode有关block的信息，那么HDFS的心跳机制是什么原理呢？</p><p>我们知道，Hadoop是Master（NameNode、ResourceManager）/Slave（DataNode、NodeManager）结构，Master启动的时候会启动一个IPC（Inter-Process Comunocation，进程通信）server服务，等待Slave的连接；而Slave启动时，会主动连接master的IPC server服务，并且是每隔3秒连接一次master，这个每隔一段时间去连接一次的机智，我们形象的称为<strong>心跳</strong>。</p><p>Slave通过心跳汇报自己的信息给Master，Master也通过心跳给Slave下达命令；NameNode通过心跳得知DataNode的状态，ResourceManager 通过心跳得知 NodeManager 的状态。如果Master长时间都没有收到slave的心跳，就认为slave挂掉了，那么NameNode感知到Data挂掉死亡的时长是怎么计算的呢？</p><p>原理是这样的：DataNode启动好了之后，会专门启动一个线程来负责给NameNode发送心跳数据包，如果整个DataNode没有任何问题，但是仅仅只是当前负责发送信息的数据包线程挂掉了，那么NameNode会发送命名向这个DataNode进行确认，如果第一次没有返回结果，仅且只会检查第二次。如果发送数据包线程没有问题，是DataNode出现了某些问题，就没有DataNode的汇报；HDFS的标准： 如果连续10次没有收到DataNode的汇报，那么NameNode就会认为该DataNode存在宕机的可能。</p><p>这里需要查看hdfs-site.xml配置文件中的两个相关设置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>heartbeat.recheck.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p>那么计算公示就可以出来：<code>timeout=2*heartbeat.recheck.interval+10*dfs.heartbeat.interval</code></p><p>需要注意的是，<code>heartbeat.recheck.interval</code>时间单位是ms（毫秒，默认为5分钟），<code>dfs.heartbeat.interval</code>时间单位是s（秒，默认为3秒）。</p><p><strong>安全模式</strong></p><p>安全模式是HDFS的自我保护数据安全的措施，当NameNode发现集群中的block丢失率（默认为0.999f，可修改dfs.safemode.threshold.pct手动配置）达到一定比例时，NameNode就会进入安全模式，在安全模式下，客户端不能对HDFS上的任何数据进行操作，只能查看元数据信息。</p><p>安全模式常用操作命令：</p><p>强制NameNode退出安全模式：<code>hadoop dfsadmin -safemode leave</code></p><p>进入安全模式：<code>hadoop dfsadmin -safemode enter</code></p><p>查看安全模式状态：<code>hadoop dfsadmin -safemode get</code></p><p>等待安全模式结束：<code>hadoop dfsadmin -safemode wait</code></p><p><strong>副本存放策略</strong></p><p>数据分块存储和副本的存放是保证可靠性和高性能的关键</p><p>副本存放策略说明：HDFS默认的副本数是3，第一个Block副本放在客户端所在的Node里，如果客户端不在集群范围，则第一个Node随机选取（不会选择太满和太忙的Node），第二个副本放置到与第一个节点不同的机架中的其中一个Node，第三个副本在和第二个副本在同一个机架，随机放在不同的Node中。</p><p>下图Block1-3表示三个副本。</p><p><img src="/archives/2018/03/11/replication.png" alt=""></p><p>修改副本数：</p><p>修改集群文件hdfs-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p>命令设置：命令+副本数+文件夹路径或某个文件路径</p><p><code>bin/hadoop fs -setrep -R 2 /</code></p><p><strong>负载均衡</strong></p><p>节点与节点之间磁盘利用率不平衡是HDFS集群非常容易出现的情况（集群内新增，删除节点，某个节点机器内一个盘存储达到饱和等）当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，数据均衡过程的核心是一个数据均衡算法。进行数据的负载均衡调整必须满足以下原则：</p><ul><li>数据平衡不能导致数据块减少，数据备份的丢失</li><li>管理员可以终止数据平衡进程</li><li>每次移动的数据量以及占用的网络资源必须是可控的</li><li>数据均衡过程中，不能影响NameNode的正常工作</li></ul><p>影响Balancer的几个参数：</p><p><code>- threshold</code>：默认设置是10，参数范围0-100，判断集群是否平衡的阀值，理论上设置的越小，整个集群越平衡</p><p><code>dfs.balance.bandwidthPerSec</code>：默认值是1048576（1M/S），Balancer运行时允许占用的带宽</p><p>用命令设置：</p><p><code>hadoop dfsadmin -setBalanacerBandwidth 10485760</code></p><p><code>sbin/start-balancer.sh -t(threshold) 10%</code></p><p>在hdfs-site.xml配置文件中设置bandwidthPerSec：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.balance.bandwidthPerSec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>负载均衡时的带宽大小设置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;心跳机制&lt;/li&gt;&lt;li&gt;安全模式&lt;/li&gt;&lt;li&gt;副本存放策略&lt;/li&gt;&lt;li&gt;负载均衡&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群环境搭建</title>
    <link href="https://pross.space/archives/2018/03/03/"/>
    <id>https://pross.space/archives/2018/03/03/</id>
    <published>2018-03-03T09:33:25.000Z</published>
    <updated>2018-08-31T16:17:09.755Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>学习Hadoop第一步，从Hadoop集群环境的搭建开始。</p><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ul><li><p>虚拟机（VMware Workstation）</p></li><li><p>Xshell5</p></li><li><p>Linux系统（CentOS-6.7-x86_64-bin-iso）</p></li><li><p>Hadoop编译后的安装包（hadoop-2.7.5-centos-6.7.tar.gz）</p></li><li><p>JDK（jdk-8u73-linux-x64.tar.gz）、</p></li></ul><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th></th><th>HDFS</th><th>YRAN</th></tr></thead><tbody><tr><td>Hadoop02</td><td>NameNode+DataNode</td><td>NodeManager</td></tr><tr><td>Hadoop03</td><td>DataNode+SecondaryNameNode</td><td>NodeManager</td></tr><tr><td>Hadoop04</td><td>DataNode</td><td>NodeManager</td></tr><tr><td>Hadoop05</td><td>DataNode</td><td>ResourceManager+NodeManager</td></tr></tbody></table><p>集群共四个节点，HDFS主节点为Hadoop02，YRAN主节点为Hadoop05</p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><strong>集群搭建：Haddoop02、Hadoop03、Hadoop04、Hadoop05</strong></p><ul><li>各个节点必须固定IP地址，并互相配置集群所有的主机映射</li><li>安装JDK，配置SSH免密登录（相互持有对方的公钥，就算是自己也需要持有）</li><li>关闭防火墙，关闭防火墙自动开启（关系到web管理页面是否能访问成功）</li><li>除root用户外，统一增加用户名：hadoop</li><li>可以配置一个节点后，克隆其余三个节点</li></ul><p><strong>解压Hadoop安装包，这里指定路径：/home/hadoop/apps/hadoop-2.7.5</strong></p><ul><li><p>修改hadoop的环境变量：普通用户（~/etc/.bashrc），root用户（/etc/profile）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</div><div class="line">  </div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div></pre></td></tr></table></figure></li></ul><p><strong>配置Hadoop配置文件（见下方详情）</strong></p><ul><li>先配置一个节点中的配置文件，然后通过scp分发到其余的节点</li><li>所有节点的Hadoop安装路径和配置文件必须一致</li></ul><p><strong>分发安装包</strong></p><p><code>scp</code>命令，需要配置SSH</p><p><strong>启动Hadoop集群</strong></p><ul><li>初始化</li><li>启动HDFS</li><li>启动YARN</li></ul><p><strong>检测验证是否成功</strong></p><ul><li>JPS命令查看各个节点进程</li><li>查看集群状态：<code>hdfs dfsadmin -report</code> 、<code>hadoop dfsadmin -report</code></li><li>HDFSweb管理页面：<a href="https://hadoop02:50070" target="_blank" rel="external">https://hadoop02:50070</a></li><li>YARNweb管理页面：<a href="https://hadoop05:8088" target="_blank" rel="external">https://hadoop05:8088</a></li></ul><h3 id="修改Hadoop配置文件"><a href="#修改Hadoop配置文件" class="headerlink" title="修改Hadoop配置文件"></a>修改Hadoop配置文件</h3><p>Hadoop配置文件需要修改六个，路径在：hadoop-2.7.5/etc/hadoop/</p><p><img src="/archives/2018/03/03/conf.png" alt=""></p><p><strong>hadoop-env.sh</strong></p><ul><li><p>默认的JAVA_HOME变量，建议修改JAVA_HOME的路径为jdk的原始路径</p><p><img src="/archives/2018/03/03/java.png" alt="hadoop-env.sh"></p></li></ul><p><strong>core-site.xml</strong></p><ul><li><p>添加hdfs配置路径，文件上传端口，临时文件存放的目录等</p><p><img src="/archives/2018/03/03/core.png" alt="core-site.xml"></p></li></ul><p><strong>hdfs-site.xml</strong></p><ul><li><p>namenode、datanode数据存储的目录，数据备份副本的个数，以及第二主节点</p><p><img src="/archives/2018/03/03/hdfs.png" alt="hdfs-site.xml"></p></li></ul><p><strong>mapred-site.xml</strong></p><ul><li><p>配置名mapreduce-yarn管理</p><p><img src="/archives/2018/03/03/mapred.png" alt="mapred-site.xml"></p></li></ul><p><strong>yarn-site.xml</strong></p><ul><li><p>yarn的主机名等</p><p><img src="/archives/2018/03/03/yarn.png" alt="yarn-site.xml"></p></li></ul><p><strong>slaves</strong></p><ul><li><p>集群的节点列表。slaves文件中配置的是DataNode的所在节点服务，方便Hadoop启动时去寻找当前集群的节点，从而命令对应的服务器启动对应的进程</p><p><img src="/archives/2018/03/03/slaves.png" alt="slaves"></p></li></ul><h3 id="分发"><a href="#分发" class="headerlink" title="分发"></a>分发</h3><p>通过<code>scp</code>，命令，将hadoop-2.7.5安装包分发到各个节点的相同位置上；</p><p><code>scp local_file remote_username@remote_ip:remote_folder</code>，<code>-r</code>递归复制</p><p>例：<code>scp -r /apps/hadoop-2.7.5 hadoop@hadoo02:~/apps/</code></p><h3 id="启动Hadoop集群"><a href="#启动Hadoop集群" class="headerlink" title="启动Hadoop集群"></a>启动Hadoop集群</h3><ul><li><p>初始化只能在主节点中进行：<code>（/home/hadoop/apps/hadoop-2.7.5/）bin/hadoop namenode -format</code></p></li><li><p>哪个节点启动HDFS均可：<code>（/home/hadoop/apps/hadoop-2.7.5/）sbin/start-dfs.sh</code></p></li><li><p>YARN启动必须在主节点：<code>（/home/hadoop/apps/hadoop-2.7.5/）sbin/start-yarn.sh</code></p></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;学习Hadoop第一步，从Hadoop集群环境的搭建开始。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop生态体系</title>
    <link href="https://pross.space/archives/2018/02/27/"/>
    <id>https://pross.space/archives/2018/02/27/</id>
    <published>2018-02-27T03:49:58.000Z</published>
    <updated>2018-08-31T16:16:16.803Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>Hadoop生态体系架构图</p><p>体系演变过程</p><p>Hadoop生态系统部分组件导图</p><a id="more"></a><h4 id="Hadoop生态体系架构图"><a href="#Hadoop生态体系架构图" class="headerlink" title="Hadoop生态体系架构图"></a>Hadoop生态体系架构图</h4><p><strong>hadoop1.0</strong></p><p><img src="/archives/2018/02/27/hadoop1.png" alt="hadoop1.x"></p><p><strong>hadoop2.0</strong></p><p><img src="/archives/2018/02/27/hadoop2.png" alt="hadoop2.x"></p><h4 id="演变过程"><a href="#演变过程" class="headerlink" title="演变过程"></a>演变过程</h4><p><img src="/archives/2018/02/27/hadoop1to2.png" alt="hadoop1to2"></p><h4 id="Hadoop生态系统部分组件导图"><a href="#Hadoop生态系统部分组件导图" class="headerlink" title="Hadoop生态系统部分组件导图"></a>Hadoop生态系统部分组件导图</h4><p><img src="/archives/2018/02/27/hadoopall.png" alt="hadoopAll"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Hadoop生态体系架构图&lt;/p&gt;&lt;p&gt;体系演变过程&lt;/p&gt;&lt;p&gt;Hadoop生态系统部分组件导图&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://pross.space/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Merge Two Sorted Lists(#21)</title>
    <link href="https://pross.space/archives/2018/01/21/"/>
    <id>https://pross.space/archives/2018/01/21/</id>
    <published>2018-01-21T12:38:05.000Z</published>
    <updated>2018-04-04T08:34:57.099Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>LeetCode编号：21</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.</p><p><strong>Example</strong></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4</div><div class="line">Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4</div></pre></td></tr></table></figure><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * Definition for singly-linked list.</span></div><div class="line"><span class="comment"> * public class ListNode &#123;</span></div><div class="line"><span class="comment"> *     int val;</span></div><div class="line"><span class="comment"> *     ListNode next;</span></div><div class="line"><span class="comment"> *     ListNode(int x) &#123; val = x; &#125;</span></div><div class="line"><span class="comment"> * &#125;</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">mergeTwoLists</span><span class="params">(ListNode l1, ListNode l2)</span> </span>&#123;</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：21&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://pross.space/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Vaild Parentheses(#20)</title>
    <link href="https://pross.space/archives/2018/01/16/"/>
    <id>https://pross.space/archives/2018/01/16/</id>
    <published>2018-01-16T02:16:51.000Z</published>
    <updated>2018-08-31T16:30:15.333Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --><p>LeetCode编号：20</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Given a string containing just the characters <code>&#39;(&#39;</code>, <code>&#39;)&#39;</code>, <code>&#39;{&#39;</code>, <code>&#39;}&#39;</code>, <code>&#39;[&#39;</code> and <code>&#39;]&#39;</code>, determine if the input string is valid.</p><p>The brackets must close in the correct order, <code>&quot;( )&quot;</code> and <code>&quot;( )[ ]{ }&quot;</code> are all valid but <code>&quot;( ]&quot;</code> and <code>&quot;( [ ) ]&quot;</code> are not.</p><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">      </div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>题目要求</strong></p><p>根据LeetCode给出的题目说明：给出一个字符串包含一对小括号、中括号、大括号。需要判断是否是按照正确的的顺序排列的。即需要括号需要成对的在一起。</p><p><strong>解题思路</strong></p><p>可以通过截取字符串后压入栈中，然后做连续索引下的值是否比较进行判断。也可以通过字符串转化为字符数组，利用栈类的特性做匹配，进而做出判断。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>Solution one</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">        Stack&lt;Integer&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</div><div class="line">            <span class="keyword">int</span> q = <span class="string">"()&#123;&#125;[]"</span>.indexOf(s.substring(i, i + <span class="number">1</span>));</div><div class="line">            <span class="keyword">if</span>(q % <span class="number">2</span> == <span class="number">1</span>) &#123;</div><div class="line">                <span class="keyword">if</span>(p.isEmpty() || stack.pop() != q - <span class="number">1</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                stack.push(q);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> stack.isEmpty();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>Solution two</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">Stack&lt;Character&gt; stack = <span class="keyword">new</span> Stack&lt;Character&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">char</span> c : s.toCharArray()) &#123;</div><div class="line"><span class="keyword">if</span> (c == <span class="string">'('</span>)&#123;</div><div class="line">            stack.push(<span class="string">')'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">'&#123;'</span>)&#123;</div><div class="line">            stack.push(<span class="string">'&#125;'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">'['</span>)&#123;</div><div class="line">            stack.push(<span class="string">']'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> ((stack.isEmpty() || stack.pop() != c))&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> stack.isEmpty();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Stack中的两个方法：</p><ul><li>pop()：移除堆栈顶部的对象，并作为此函数的值返回该对象</li><li>push(E item)：将一个项目推到这个堆栈的顶部</li><li>栈是Vector的一个子类，它实现了一个标准的后进先出的栈</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Oct 28 2018 00:54:31 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：20&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://pross.space/tags/LeetCode/"/>
    
  </entry>
  
</feed>
