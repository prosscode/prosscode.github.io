<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PROSS</title>
  
  <subtitle>Great minds have purpose, others have only wishes.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://prosscode.github.io/"/>
  <updated>2018-08-26T14:41:36.498Z</updated>
  <id>https://prosscode.github.io/</id>
  
  <author>
    <name>RukiapR0ss</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>无题;</title>
    <link href="https://prosscode.github.io/archives/2018/08/26/"/>
    <id>https://prosscode.github.io/archives/2018/08/26/</id>
    <published>2018-08-26T14:36:25.000Z</published>
    <updated>2018-08-26T14:41:36.498Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>岁月还在眼眸打坐，你的凝望</p><p>也许在苦等，有不朽的诗句</p><p>说完这些话后我的梦想已经有拔节之声</p><p>我蜷缩起双腿</p><p>似乎是在为每一段时光致敬</p><p><img src="/archives/2018/08/26/Authony.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;岁月还在眼眸打坐，你的凝望&lt;/p&gt;&lt;p&gt;也许在苦等，有不朽的诗句&lt;/p&gt;&lt;p&gt;说完这些话后我的梦想已经有拔节之声&lt;/p&gt;&lt;p&gt;我蜷缩起双腿&lt;/p&gt;&lt;p
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://prosscode.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>理解Spark核心之RDD</title>
    <link href="https://prosscode.github.io/archives/2018/05/29/"/>
    <id>https://prosscode.github.io/archives/2018/05/29/</id>
    <published>2018-05-29T09:04:41.000Z</published>
    <updated>2018-06-26T14:38:00.116Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性分布式数据集）</p><h4 id="理解RDD"><a href="#理解RDD" class="headerlink" title="理解RDD"></a>理解RDD</h4><p>如果你在Spark集群中加载了一个很大的文本数据，Spark就会将该文本抽象为一个RDD，这个RDD根据你定义的分区策略（比如HashKey）可以分为数个Partition，这样就可以对各个分区进行并行处理，从而提高效率。</p><p>RDD是一个容错的，并行的数据结构，可以让用户显示地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，比如Map、flatMap、filter等转换操作实现了monad模式（Monad是一种设计模式，表示将一个运算过程，通过函数拆解成互相连接的多个步骤；你只要提供下一步运算所需的函数，整个运算就会自动进行下去。），很好的切合了Scala的集合操作。另外，RDD还提供了比如join，groupBy，reduceByKey（action操作）等更为方便的操作，用来支持常见的数据运算。</p><p>RDD是一系列只读分区的集合，它只能从文件中读取并创建，或者从旧的RDD生成新的RDD。RDD的每一次变换操作都会生成新的RDD，而不是在原来的基础上进行修改，这种粗粒度的数据操作方式为RDD带来了容错和数据共享方面的优势，但是在面对大数据集中频繁的小操作的时候，显得效率比较低下。</p><h4 id="RDD原理"><a href="#RDD原理" class="headerlink" title="RDD原理"></a>RDD原理</h4><p>RDD实际上是一个类（sc.textFile()方法返回一个RDD对象，然后用line接收这个对象），而这个RDD类中也定义了一系列的用于操作的方法，也就是一些算子操作。</p><p>这个类为了实现对数据的操作，里面有分区信息，用于记录特定RDD的分区情况；依赖关系，指向其父RDD；一个函数，用于记录父RDD到自己的转换操作；划分策略和数据位置的元数据。在DAG中这样的RDD就可以看成一个个节点，RDD中的存储的依赖关系就是DAG的边。在Spark中，数据在物理上被划分为一个个的block，这些block由blockmanager统一管理的。</p><p>在设计RDD之间的依赖关系时，设计者将RDD之间的依赖关系分为两类：窄依赖和宽依赖。RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个DataSet片段。RDD可以相互依赖，如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之位narrow dependency（窄依赖）；若多个Child RDD分区都可以依赖，则称为wide dependency（宽依赖），而join操作则会产生wide dependency。</p><p><img src="/archives/2018/05/29/narrowAndwide.jpg" alt="narrow和wide"></p><p><img src="/archives/2018/05/29/rdd-dependencies.png" alt="narrow和wide的区别"></p><blockquote><p>Spark之所以将依赖分为narrow与wide，基于以下两点原因：</p><p>narrow dependecies可以支持在同一个cluster node上，并且以管道形式执行多行命令，例如在执行了map操作后，紧接着执行filter。相反，wide dependencies需要所有的父分区都是可用的，可能还需要调用类似MapReduce之类的操作进行跨节点传递。</p><p>其次从失败恢复的角度考虑，narrow dependencies的失败恢复更有效，因为它只需要重新计算丢失的parent partition即可，而且可以并行的在不同节点进行重计算。而wide dependencies牵涉到RDD各级的多个parent partitions。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>RDD是Spark的核心，也是整个Spark架构的基础，特性总结如下：</p><ul><li>不变的数据结构存储</li><li>支持跨集群的分布式数据结构</li><li>可以根据数据记录的Key对结构进行分区</li><li>提供了粗粒度的操作，且这些操作支持分区</li><li>它将数据存储在内存中，从而提供了低延迟性</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Spark是围绕RDD的概念展开的，RDD是可以并行操作的容错元素集合。RDD全称是Resilient Distributed Datasets（弹性
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://prosscode.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的运行模式</title>
    <link href="https://prosscode.github.io/archives/2018/05/15/"/>
    <id>https://prosscode.github.io/archives/2018/05/15/</id>
    <published>2018-05-15T12:56:12.000Z</published>
    <updated>2018-06-14T09:57:31.066Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>Spark是新一代基于内存的计算框架，是用于大规模数据处理的同意分析引擎。相比于Hadoop MapReduce计算框架，Spark将中间计算结果保留在内存中，速度提升10~100倍；同时采用弹性分布式数据集（RDD）实现迭代计算，更好的适用于数据挖掘、机器学习，极大的提升开发效率。</p><p>Spark的运行模式，它不仅支持单机模式，同时支持集群模式运行；这里具体的总结一下Spark的各种运行模式的区分。</p><h4 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h4><p>Local模式又称本地模式，通过Local模式运行非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker进程（只有集群的Standalone模式运行时，才需要这两个角色），也不用启动Hadoop的服务，除非你需要用到HDFS。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--class org.apache.spark.examples.SparkPi \</div><div class="line">--master local[2] \</div><div class="line">lib/spark-examples-1.0.0-hadoop2.2.0.jar  \</div><div class="line">100</div><div class="line"></div><div class="line"># 1）--master local 就可以确定是单机的local模式了，[2]意思是分配2个cores运行</div><div class="line"># 2）--lib/spark-examples-1.0.0-hadoop2.2.0.jar：jar包的路径（application-jar）</div><div class="line"># 3）--100：传递给主类的主要方法的可选参数（application-arguments）</div></pre></td></tr></table></figure><p>这里的spark-submit进程既是客户端提交任务的Client进程，又是Spark的Driver程序，还充当着Spark执行Task的Executor角色。所有的程序都运行在一个JVM中，主要用于开发时测试。</p><h4 id="本地伪集群运行模式（单机模拟集群）"><a href="#本地伪集群运行模式（单机模拟集群）" class="headerlink" title="本地伪集群运行模式（单机模拟集群）"></a>本地伪集群运行模式（单机模拟集群）</h4><p>这种模式，和Local[N]很像，不同的是它会在单机的环境下启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委曲求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有出现问题，或者想使用Spark计算框架而没有太多资源的情况下。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--master local-cluster[2,3,1024]</div><div class="line"></div><div class="line"># --master local-cluster[2,3,1024]：在本地模拟集群下使用2个Executor进程，每个进程分配3个cores和1024M的内存来运行程序</div></pre></td></tr></table></figure><p>这里的spark-submit依然充当全能角色，又是Client进程，又是Driver程序，也负责资源管理。运行该模式很简单，只需要把Spark安装包解压后，修改一些常用的配置，不用启动Spark的Master、Worker守护进程，也不用启动Hadoop的服务，除非你是需要用到HDFS。</p><h4 id="Standalone模式（集群）"><a href="#Standalone模式（集群）" class="headerlink" title="Standalone模式（集群）"></a>Standalone模式（集群）</h4><p>Standalone是集群模式，这里就需要在执行应用程序前，先启动Spark的Master和Worker守护进程，不用启动Hadoop的服务，除非你需要使用HDFS。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">  --class org.apache.spark.examples.SparkPi \</div><div class="line">  --master spark://207.184.161.138:7077 \</div><div class="line">  --executor-memory 8G \</div><div class="line">  --total-executor-cores 10 \</div><div class="line">  /path/to/examples.jar \</div><div class="line">  1000</div><div class="line"></div><div class="line"># 1) --master spark://207.184.161.138:7077:采用Standalone模式运行，后面是集群地址和端口</div><div class="line"># 2) --executor-memory 20G:配置executor进程内存为8G</div><div class="line"># 3）--total-executor-cores 100：配置cores数为10个</div></pre></td></tr></table></figure><p>Master进程作为cluster manager，用来对应用程序申请的资源进行管理；spark-submit做为Client端和运行Driver程序。Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点，Master节点和Worker节点。其中Driver既可以运行在Master节点上，也可以运行在本地的Client端。</p><p>当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclipse、IDEA等开发平台上使用<code>new SparkConf.setManager(&quot;spark://master:7077&quot;)</code>方式运行Spark任务时，Driver是运行在本地Client端上的。</p><p><strong>运行流程</strong></p><ul><li>SparkContext连接到Master，向Master注册并申请资源（CPU Core和Memory）</li><li>Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend</li><li>StandaloneExecutorBackend向SparkContext注册</li><li>SparkContext将Application代码发给StandaloneExecutorBackend；并且SparkContext解析Application代码，构建DAG图，提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会产生Job；每个Job中含有一个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后Stage（又称为TaskSet）提交Task Scheduler，负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行</li><li>StandaloneExecutorBackend会构建Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成</li><li>所有Task完成后，SparkContext向Master注销，释放资源</li></ul><h4 id="on-yarn-client模式（集群）"><a href="#on-yarn-client模式（集群）" class="headerlink" title="on yarn client模式（集群）"></a>on yarn client模式（集群）</h4><p>越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。</p><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit \</div><div class="line">--class org.apache.spark.examples.SparkPi \</div><div class="line">--master yarn \</div><div class="line">--deploy-mode client \</div><div class="line">lib/spark-examples-1.0.0-hadoop2.2.0.jar </div><div class="line"></div><div class="line"># 1) --master yarn：采用yarn进行资源调度</div><div class="line"># 2) --deploy-mode client：client环境运行</div></pre></td></tr></table></figure><p>在执行Spark应用程序前，要启动Hadoop的各种服务，由于已经有了资源管理器，所以不需要启动Spark的Master，Worker守护进程。</p><p><strong>运行流程</strong></p><ul><li>Spark Yarn Client向Yarn的ResourceManager申请启动Application Master，同时在SparkContext初始化中创建DAG Scheduler和Task Scheduler，由于我们选择是Yarn Client模式，程序会选择启动YarnClientClusterScheduler和YarnClientSchedulerBackend</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster；与YarnCluster区别是在该ApplicationMaster中不运行SparkContext，只与SparkContext进行联系进行资源的分配</li><li>Client中的SparkContext初始化完成后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便于对应的NodeManager通信，要求它在获得的Container中开始向SparkContext注册并申请执行Task任务</li><li>Client中的SparkContext分配给Container的Task开始执行，并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而可以在任务失败时重启任务</li><li>应用程序完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</li></ul><h4 id="on-yarn-cluster模式（集群）"><a href="#on-yarn-cluster模式（集群）" class="headerlink" title="on yarn cluster模式（集群）"></a>on yarn cluster模式（集群）</h4><p><strong>运行实例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">--master yarn \</div><div class="line">--deploy-mode cluster \</div><div class="line">--driver-memory 4g \</div><div class="line">--executor-memory 2g \</div><div class="line">--executor-cores 1 \</div><div class="line">examples/jars/spark-examples*.jar \</div><div class="line">10</div><div class="line"></div><div class="line"># --driver-memory 4g：集群模式下Yarn Application Master的内存大小</div></pre></td></tr></table></figure><p><strong>运行流程</strong></p><ul><li>SparkYarnClient向YARN中ResourceManager提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster中进行SparkContext的初始化</li><li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便于对应的NodeManager通信，要求它在获得的Container中开始向SparkContext注册并申请执行Task任务</li><li>SparkContext分配给Container的Task开始执行，并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而可以在任务失败时重启任务</li><li>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己</li></ul><h4 id="Mesos模式"><a href="#Mesos模式" class="headerlink" title="Mesos模式"></a>Mesos模式</h4><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核。Mesos最初是由加州大学伯克利分校的AMPLab开发的，后在Twitter得到广泛使用。Apache Mesos是一个通用的集群管理器，起源于 Google 的数据中心资源管理系统Borg。</p><p>Mesos模式接触较少，这里不作为展开。</p><h4 id="Kubernetes模式（K8S）"><a href="#Kubernetes模式（K8S）" class="headerlink" title="Kubernetes模式（K8S）"></a>Kubernetes模式（K8S）</h4><p>Kubernetes调度器目前是实验性的。在未来的版本中，可能会出现配置，容器图像和入口点的行为变化。 （Spark2.3.0）</p><p>Kubernetes模式接触较少，这里不作为展开。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Spark是新一代基于内存的计算框架，是用于大规模数据处理的同意分析引擎。相比于Hadoop MapReduce计算框架，Spark将中间计算结果保留
      
    
    </summary>
    
    
      <category term="Spark" scheme="https://prosscode.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>HDFS NameNode内存全景</title>
    <link href="https://prosscode.github.io/archives/2018/05/07/"/>
    <id>https://prosscode.github.io/archives/2018/05/07/</id>
    <published>2018-05-07T00:37:22.000Z</published>
    <updated>2018-05-29T12:57:11.272Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><h3 id="NameNode概述"><a href="#NameNode概述" class="headerlink" title="NameNode概述"></a>NameNode概述</h3><p>在HDFS系统架构中，NameNode管理着整个文件系统的元数据，维护整个集群的机架感知信息和DataNode和Block的信息，Lease管理以及集中式缓存引入的缓存管理等等。从整个HDFS系统架构上来看，NameNode是最重要、最复杂也是最容易出现问题的地方。</p><p>NameNode管理的HDFS文件系统的元数据分为两个层次：NameSpace管理层，负责管理文件系统中的树状目录结构以及文件与数据块之间的映射关系；块管理层，负责管理文件系统中文件的物理块与实际存储位置的映射关系（BlockMap）。</p><p>NameSpace管理的元数据除内存常驻外，也会周期Flush到持久化设备fsimage文件上（core-site.xml中配置<code>hadoop.tmp.dir</code>目录下的dfs/name/current中）；BlockMap元数据只存在于内存中；当NameNode发生重启，首先从持久化设备中读取fsimage，构建NameSpace的元数据信息，之后根据DataNode的汇报信息重新构造BlockMap，这两部分数据是占据NamNode大部分JVM Heap空间。</p><h3 id="NameNode内存结构"><a href="#NameNode内存结构" class="headerlink" title="NameNode内存结构"></a>NameNode内存结构</h3><p><img src="/archives/2018/05/07/namenode.png" alt="namenode"></p><p>NameNode常驻内存主要被NameSpace和BlockManager使用，二者使用占比分别接近50%。其它部分内存开销较小且相对固定，与NameSpace和BlockManager相比基本可以忽略。</p><h3 id="NameNode内存分析"><a href="#NameNode内存分析" class="headerlink" title="NameNode内存分析"></a>NameNode内存分析</h3><h4 id="NameSpace"><a href="#NameSpace" class="headerlink" title="NameSpace"></a>NameSpace</h4><p><img src="/archives/2018/05/07/namespace.png" alt="namespace"></p><p>HDFS文件系统的目录结构也是按照树状结构维护，NameSpace保存了目录树以及每个目录/文件节点的属性。在整个NameSpace目录树中存在两种不同类型的INode数据结构：<code>INodeDirectory</code>和<code>INodeFile</code>。其中INodeDirectory表示的是目录树中的目录，INodeFile表示的是目录树中的文件。<br><img src="/archives/2018/05/07/inode.png" alt="inode"></p><p>INodeDirectory和INodeFile均继承自INode，所以具备大部分相同的公共信息INodeWithAddititionalFields，除常用基础属性外，其中还提供了扩展属性features（如Quota，Snapshot等），如果以后出现新属性也可以通过feature扩展。不同的是，INodeFile特有的标识副本数和数据块大小组合的<code>header</code>（2.61之后新增了标识存储策略ID的信息）以及该文件包含的有序Block的数组；INodeDirectory特有的是列表<code>children</code>，children默认是大小为5的ArrayList类型，按照子节点name有序存储，在插入时会损失一部分写入的性能，但是可以方便后续快速二分查找提高读的性能，对于一般的存储系统，读操作比写操作占比要高。</p><h4 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h4><p>NameNode概述中介绍的负责管理文件系统中文件的物理块与实际存储位置的映射关系（BlockMap）就是由BlockManager来统一管理。NameSpace和BlockMap之间通过前面提到的INodeFile有序Blocks数组关联到一起。<br><img src="/archives/2018/05/07/blockinfo.png" alt="blockinfo"></p><p>每一个INodeFile都会包含数量不等的Block，具体数量由文件大小及每一个Block的大小比值决定，这些Block按照所在的文件的先后顺序组成BlockInfo数组，BlockInfo维护的是Block的元数据，数据本身是由DataNode管理，所以BlockInfo需要包含实际数据且由DataNode管理的信息是名为triplets的Object数组，大小为3*replicas（replicas是Block副本数量，默认为3），从图中可以知道，BlockInfo包含了哪些Block，这些Block分别存储在哪些DataNode上。</p><p>如何快速的通过BlockId快速定位到Block，这里还需要BlocksMap。</p><p>BlocksMap底层通过LightWeightGSet实现，本质是一个链式解决冲突的Hash表。事实上，BlocksMap里所有的BlockInfo就是INodeFile中对应BlockInfo的引用，通过Block查找对应BlockInfo时，也是先对Block计算HashCode，根据结果快速定位到对应的BlockInfo信息。</p><p><img src="/archives/2018/05/07/blocksMap.png" alt="blocksMap"></p><p>这里还涉及到几个核心的数据结构：excessReplicateMap（多余的副本存放） ，neededReplications（需要补充Block的副本存放处，是一个优先级队列，缺少副本数越多的Block会优先处理），invalidateBlocks（删除的副本存放处） ，corruptReplicas（某些Block不可用暂存处）等。</p><p>相比Namespace，BlockManager管理的数据要复杂的多。</p><h4 id="NetworkTopology"><a href="#NetworkTopology" class="headerlink" title="NetworkTopology"></a>NetworkTopology</h4><p>Hadoop在设计考虑到数据的安全和高效，默认存放三份副本。存储策略是本地一份，同机架内其他某一个节点上一份，不同机架的某一节点上一份，这样如果本地数据损坏了，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿到的数据要快；为了降低整体的带宽消耗和读取延时时间，HDFS会尽快让读取程序读取离它最近的副本。那么Hadoop确定任意两个节点是位于同一机架还是不同的机架呢？这就需要机架拓扑NetworkTopology，也叫作机架感知。<br>默认情况下，NameNode启动时日志信息是这样的：<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-05-09 19:27:26,423 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /default-rack/ 192.168.123.102:50010</div></pre></td></tr></table></figure><p></p><p>每个IP对应的机架ID都是/default-rack，说明Hadoop的机架感知没有被启用。<br><strong>配置机架感知</strong><br>配置机架感知也很简单，NameNode所在的节点中，在<code>core-site.xml</code>文件中配置<code>topology.script.name</code>，value通常是一个shell脚本，该脚本接受一个参数，输出一个值。接受的参数通常为某台DataNode的IP地址，而输出的值通常为该IP地址对应的DataNode所在的rack。<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.script.file.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/apps/hadoop-2.6.5/etc/hadoop/topology.sh<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p></p><p><code>topology.sh</code><br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>!/bin/bash  </div><div class="line">HADOOP_CONF=/home/bigdata/apps/hadoop/etc/hadoop  </div><div class="line">while [ $# -gt 0 ] ; do  </div><div class="line">  nodeArg=$1  </div><div class="line">  exec&lt;$&#123;HADOOP_CONF&#125;/topology.data</div><div class="line">  result=""  </div><div class="line">  while read line ; do  </div><div class="line">    ar=( $line )  </div><div class="line">    if [ "$&#123;ar[0]&#125;" = "$nodeArg" ]||[ "$&#123;ar[1]&#125;" = "$nodeArg" ]; then  </div><div class="line">      result="$&#123;ar[2]&#125;"  </div><div class="line">    fi  </div><div class="line">  done  </div><div class="line">  shift  </div><div class="line">  if [ -z "$result" ] ; then  </div><div class="line">    echo -n "/default-rack"  </div><div class="line">  else  </div><div class="line">    echo -n "$result"  </div><div class="line">  fi  </div><div class="line">  done</div></pre></td></tr></table></figure><p></p><p><code>topology.data</code>格式为：节点（IP或主机名） /交换机xx/机架xx<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">192.168.123.102 hadoop02 /switch/rack2</div><div class="line">...</div></pre></td></tr></table></figure><p></p><p>配置后，NameNode启动时日志信息是这样的：<br></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-05-09 19:27:26,423 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node:  /switch/rack2/ 192.168.123.102:50010</div></pre></td></tr></table></figure><p></p><p>说明Hadoop的机架感知已经被启用了。查看Hadoop机架信息的命令<code>hadoop dfsadmin -printTopology</code></p><h4 id="LeaseManager"><a href="#LeaseManager" class="headerlink" title="LeaseManager"></a>LeaseManager</h4><p>Lease 机制是重要的分布式协议，广泛应用于各种实际的分布式系统中。HDFS支持Write-Once-Read-Many，对文件写操作的互斥同步靠Lease实现。<br>Lease实际上是时间约束锁，其主要特点是排他性。客户端写文件时需要先申请一个Lease，一旦有客户端持有了某个文件的Lease，其它客户端就不可能再申请到该文件的Lease，这就保证了同一时刻对一个文件的写操作只能发生在一个客户端。<br>NameNode的LeaseManager是Lease机制的核心，维护了文件与Lease、客户端与Lease的对应关系，这类信息会随写数据的变化实时发生对应改变。</p><blockquote><p>本文是根据<a href="https://tech.meituan.com/namenode.html?_blank" target="_blank" rel="external">美团点评技术团队中《HDFS NameNode内存全景》</a>整理总结</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;h3 id=&quot;NameNode概述&quot;&gt;&lt;a href=&quot;#NameNode概述&quot; class=&quot;headerlink&quot; title=&quot;NameNode概述
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop HA集群搭建</title>
    <link href="https://prosscode.github.io/archives/2018/04/25/"/>
    <id>https://prosscode.github.io/archives/2018/04/25/</id>
    <published>2018-04-25T00:58:34.000Z</published>
    <updated>2018-05-29T12:56:50.662Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>HA：High Available，高可用</p><h3 id="为什么会有Hadoop-HA机制"><a href="#为什么会有Hadoop-HA机制" class="headerlink" title="为什么会有Hadoop HA机制"></a>为什么会有Hadoop HA机制</h3><p>在HDFS集群中NameNode会存在单点故障（SPOF：A Single Point of Failure）问题：对于只有一个NameNode的集群，如果唯一的NameNode机器出现故障，比如宕机、软件硬件升级等。那么整个集群将无法使用，直到NameNode重新启动才会恢复。</p><p>所以在hadoop2.0之前，出现这种单节点故障问题是无法解决的；但是Hadoop HA机制的出现就很好的解决了这个问题，在一个典型的Hadoop HA集群中，使用两台单独的机器配置为NameNodes节点。在任何时间点，确保NameNodes中只有一个处于Active状态，另一个处在Standby状态。其中ActiveNameNode负责集群中所有的客户端的操作，StandbyNameNode仅仅充当备机，保证一旦ActiveNameNode出现问题能够快速切换。</p><p>准备两台NameNode解决单节点故障问题是完全可行，但是又出现一个问题：Active和Standby两个 NameNodes的元数据信息(editlog)如何同步才能让Standby节点切换后”无缝对接工作”？那么就需要一个共享存储系统，这个系统就是Zookeeper，Active NameNode会将数据写入共享存储系统，而Standby便可以快速切为Active NameNode接替其工作。为了实现这一目标，DataNode需要配置NameNodes的位置，并同时给他们发送文件块信息以及心跳检测。</p><p><img src="/archives/2018/04/25/zookeeper.jpg" alt=""></p><h3 id="集群规划和准备"><a href="#集群规划和准备" class="headerlink" title="集群规划和准备"></a>集群规划和准备</h3><p>基于Hadoop集群环境搭建的准备条件，集群规划：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">hadoop02</th><th style="text-align:center">hadoop03</th><th style="text-align:center">hadoop04</th><th style="text-align:center">hadoop05</th></tr></thead><tbody><tr><td style="text-align:center">namenode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">datanode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">resourcemanager</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">nodemanager</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td></tr><tr><td style="text-align:center">zookeeper</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">journalnode</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">zkfc</td><td style="text-align:center">✔</td><td style="text-align:center">✔</td><td style="text-align:center"></td></tr></tbody></table><h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><p><code>hadoop-env.sh</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME= /usr/local/jdk1.8.0_73</div></pre></td></tr></table></figure><p><code>core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 hdfs 的 nameservice 为 myha01 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://myha01/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 hadoop 工作目录 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 zookeeper 集群访问地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定副本数 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!--指定 hdfs 的 nameservice 为 myha01，需要和 core-site.xml 中保持一致--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>myha01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- myha01 下面有两个 NameNode，分别是 nn1，nn2 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.myha01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn1 的 RPC 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.myha01.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn1 的 http 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.myha01.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn2 的 RPC 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.myha01.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- nn2 的 http 通信地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.myha01.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 NameNode 的 edits 元数据在 JournalNode 上的存放位置 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/myha01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 JournalNode 在本地磁盘存放数据的位置 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 NameNode 失败自动切换 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 value值不要换行--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.myha01<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></div><div class="line">          sshfence</div><div class="line">          shell(/bin/true)</div><div class="line">      <span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 使用 sshfence 隔离机制时需要 ssh 免登陆 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 配置 sshfence 隔离机制超时时间 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 mr 框架为 yarn 方式 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 设置 mapreduce 的历史服务器地址和端口号 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- mapreduce 历史服务器的 web 访问地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 RM 高可用 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 RM 的 cluster id --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 RM 的名字 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 分别指定 RM 的地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop04<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop05<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 指定 zk 集群地址 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:2181,hadoop03:2181,hadoop04:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 要运行 MapReduce 程序必须配置的附属服务 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 开启 YARN 集群的日志聚合功能 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- YARN 集群的聚合日志最长保留时长 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 启用自动恢复 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 制定 resourcemanager 的状态信息存储在 zookeeper 集群上--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>slaves</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hadoop02</div><div class="line">hadoop03</div><div class="line">hadoop04</div><div class="line">hadoop05</div></pre></td></tr></table></figure><p><strong>配置环境变量和分发安装包</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vim ~/.bashrc  #如果没找到，先执行 ll -a ~/</div><div class="line">添加两行：</div><div class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div><div class="line"></div><div class="line">分发安装包和环境变量：</div><div class="line">scp -r ~/apps/hadoop-2.7.5 hadoop@hadoop03:~/apps/</div><div class="line">...</div><div class="line">scp ~/.bashrc hadoop@hadoop03:~/</div><div class="line">...</div></pre></td></tr></table></figure><h3 id="集群初始化和启动"><a href="#集群初始化和启动" class="headerlink" title="集群初始化和启动"></a>集群初始化和启动</h3><p>HA集群的启动是有一定的顺序的，第一次启动严格按照以下步骤:</p><p><strong>启动zookeeper集群</strong></p><p>配置zookeeper集群的节点都需要启动</p><p>启动命令：<code>zkServer.sh start</code></p><p>查看状态：<code>zkServer.sh status</code></p><p><strong>启动journalnode进程：</strong></p><p>配置journalnode集群的节点都需要启动</p><p>启动命令：<code>hadoop-daemon.sh start journalnode</code></p><p><strong>格式化NameNode：</strong></p><p>在hadoop02主节点上格式化操作</p><p>命令：<code>hadoop namenode -format</code></p><p>在<code>~/data/hadoopdata/</code>中会生成集群信息，把hadoopdata文件发送到hadoop03上</p><p><code>scp -r ~/data/hadoopdata/ hadoop@hadoop03:~/data/</code></p><p><strong>格式化ZKFC</strong></p><p>在第一台机器上操作即可</p><p>命令：<code>hdfs zkfc -formatZK</code></p><p><strong>启动HDFS和YARN集群</strong></p><p>命令：<code>start-dfs.sh</code> <code>start-yarn.sh</code></p><p><strong>查看进程情况</strong></p><p>命令：<code>jps</code></p><h3 id="验证和测试"><a href="#验证和测试" class="headerlink" title="验证和测试"></a>验证和测试</h3><p><strong>验证</strong></p><p>访问HDFSWeb页面：<code>http://hadoop02:50070</code> 、<code>http://hadoop03:50070</code></p><p>hadoop02是Actice状态，hadoop03是standby状态</p><p>访问YARNweb页面：<code>http://hadoop04:8088</code> 、<code>http://hadoop05:8088</code></p><p>访问hadoop05 是 standby resourcemanager，会自动跳转到 hadoop04</p><p><strong>测试</strong></p><p>干掉active namenode的进程，看看集群变化</p><p>在上传文件的时候干掉 active namenode， 看看集群有什么变化</p><p>干掉 active resourcemanager， 看看集群有什么变化</p><p>在执行任务的时候干掉 active resourcemanager，看看集群有什么变化</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;HA：High Available，高可用&lt;/p&gt;&lt;h3 id=&quot;为什么会有Hadoop-HA机制&quot;&gt;&lt;a href=&quot;#为什么会有Hadoop-HA
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Decorator Pattern（装饰者模式）</title>
    <link href="https://prosscode.github.io/archives/2018/04/16/"/>
    <id>https://prosscode.github.io/archives/2018/04/16/</id>
    <published>2018-04-16T14:59:59.000Z</published>
    <updated>2018-05-29T12:58:14.293Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><h3 id="装饰者模式"><a href="#装饰者模式" class="headerlink" title="装饰者模式"></a>装饰者模式</h3><p>首先明确一点，所为”设计模式”，全称是”面对对象设计模式”的简称。其次需要明白，设计模式是与语言无关，意思就是说它并不是属于某种编程语言。最后，因为Java对面向对象支持的比较好，并且在JDK当中有很多实现就是依靠装饰者模式，所以下面举例采用Java代码来说明。</p><p>装饰者模式是在不改变原类文件和使用继承的情况下，动态的扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象。其目的是：对原类的功能增强。</p><p>在装饰模式中的各个角色有：</p><p>抽象构件（Component）：给出一个抽象接口，以规范准备接收附加责任的对象。</p><p>具体构件（Concrete Component）：定义一个将要接收附加责任的类</p><p>装饰（Decorator）角色：持有一个构建对象的实例，并实现一个与抽象构件接口一致的接口</p><p>具体装饰（Concrete Decorator）：负责给构件对象添加上附加的责任</p><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>需求：定义一个字符串数组，让字符串数组中的元素进行大小写转换。</p><p>分析：字符串数组中是没有直接可以实现大小写转换的方法，当然，字符串数组遍历出来转化为单个字符进行大小写的转换是可以，但是用这样的写法就和装饰者模式没啥事了。返回之前，因为没有直接转换方法，所以需要对其进行功能增强。</p><p>代码一现，其义自见：</p><p><code>ListTest.java</code></p><p><img src="/archives/2018/04/16/ListTest.png" alt=""></p><p><code>MyList.java</code></p><p><img src="/archives/2018/04/16/MyList.png" alt=""></p><p><code>Operator.java</code></p><p><img src="/archives/2018/04/16/Operator.png" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;h3 id=&quot;装饰者模式&quot;&gt;&lt;a href=&quot;#装饰者模式&quot; class=&quot;headerlink&quot; title=&quot;装饰者模式&quot;&gt;&lt;/a&gt;装饰者模式&lt;/h3
      
    
    </summary>
    
    
      <category term="设计模式" scheme="https://prosscode.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程案例（下）</title>
    <link href="https://prosscode.github.io/archives/2018/04/05/"/>
    <id>https://prosscode.github.io/archives/2018/04/05/</id>
    <published>2018-04-05T14:18:53.000Z</published>
    <updated>2018-05-29T13:29:04.569Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><ul><li>Versions变动版本记录</li><li>数值累加</li></ul><a id="more"></a><h4 id="Version变动版本记录"><a href="#Version变动版本记录" class="headerlink" title="Version变动版本记录"></a>Version变动版本记录</h4><p>在所有有版本变动的记录后面追加一条字段信息：该信息就是上一个版本的版本号，只限同用户</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">数据格式：</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">8</span>,<span class="number">360</span>手机助手,<span class="number">0.1</span>版本,北京</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">22</span>,<span class="number">360</span>手机助手,<span class="number">0.2</span>版本,北京</div><div class="line"><span class="number">20170308</span>,黄渤,光环斗地主,<span class="number">14</span>,<span class="number">360</span>手机助手,<span class="number">0.3</span>版本,北京</div><div class="line">...</div><div class="line">字段信息：</div><div class="line">用户ID，用户名，游戏名，小时，数据来源，游戏版本，用户所在地</div><div class="line">id, name, game, hour, source, version, city</div></pre></td></tr></table></figure><p>思路：把id、name、game、hours、source、version、city封装成<code>Version</code>对象，实现<code>WritableComparable</code>接口，并根据id、name、version先后排序。排序后判断相邻的数据版本号是否一致来进行版本号的添加。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.version;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.DataInput;</div><div class="line"><span class="keyword">import</span> java.io.DataOutput;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月19日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：Version实体类</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Version</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">Version</span>&gt;</span>&#123;</div><div class="line"><span class="keyword">private</span> String id;</div><div class="line"><span class="keyword">private</span> String name;</div><div class="line"><span class="keyword">private</span> String game;</div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> hour;</div><div class="line"><span class="keyword">private</span> String source;</div><div class="line"><span class="keyword">private</span> String version;</div><div class="line"><span class="keyword">private</span> String city;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> id;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.id = id;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> name;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getGame</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> game;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setGame</span><span class="params">(String game)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.game = game;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getHour</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> hour;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHour</span><span class="params">(<span class="keyword">int</span> hour)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.hour = hour;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getSource</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> source;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSource</span><span class="params">(String source)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.source = source;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getVersion</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> version;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setVersion</span><span class="params">(String version)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.version = version;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getCity</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCity</span><span class="params">(String city)</span> </span>&#123;</div><div class="line"><span class="keyword">this</span>.city = city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Version</span><span class="params">(String id, String name, String game, <span class="keyword">int</span> hour, String source, String version, String city)</span> </span>&#123;</div><div class="line"><span class="keyword">super</span>();</div><div class="line"><span class="keyword">this</span>.id = id;</div><div class="line"><span class="keyword">this</span>.name = name;</div><div class="line"><span class="keyword">this</span>.game = game;</div><div class="line"><span class="keyword">this</span>.hour = hour;</div><div class="line"><span class="keyword">this</span>.source = source;</div><div class="line"><span class="keyword">this</span>.version = version;</div><div class="line"><span class="keyword">this</span>.city = city;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Version</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">super</span>();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">return</span> id + <span class="string">","</span> + name + <span class="string">","</span> + game + <span class="string">","</span> + hour + <span class="string">","</span> + source+ <span class="string">","</span> + version + <span class="string">","</span> + city;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 序列化与反序列化</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">out.writeUTF(id);</div><div class="line">out.writeUTF(name);</div><div class="line">out.writeUTF(game);</div><div class="line">        out.writeInt(hour);  </div><div class="line">        out.writeUTF(source);  </div><div class="line">        out.writeUTF(version);  </div><div class="line">        out.writeUTF(city);  </div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line"><span class="keyword">this</span>.id = in.readUTF();</div><div class="line"><span class="keyword">this</span>.name = in.readUTF();</div><div class="line"><span class="keyword">this</span>.game = in.readUTF();</div><div class="line"><span class="keyword">this</span>.hour = in.readInt();</div><div class="line"><span class="keyword">this</span>.source = in.readUTF();</div><div class="line"><span class="keyword">this</span>.version = in.readUTF();</div><div class="line"><span class="keyword">this</span>.city = in.readUTF();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 比较器 排序</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Version version)</span> </span>&#123;</div><div class="line"><span class="keyword">int</span> resultID=<span class="keyword">this</span>.id.compareTo(version.getId());</div><div class="line"><span class="keyword">if</span>(resultID==<span class="number">0</span>)&#123;</div><div class="line"><span class="keyword">int</span> resultName=<span class="keyword">this</span>.name.compareTo(version.getName());</div><div class="line"><span class="keyword">if</span>(resultName==<span class="number">0</span>)&#123;</div><div class="line"><span class="keyword">return</span> <span class="keyword">this</span>.version.compareTo(version.getVersion());</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="keyword">return</span> resultName;</div><div class="line">&#125;</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="keyword">return</span> resultID;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在MR程序中，map阶段拆分数据后把数据封装到<code>Version</code>对象中，在reduce阶段进行数据迭代，判断版本号是否一致，当id和name不一致，证明不是两个不同的用户；如果一致，则进行版本号的拼接，放入Key中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.version;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月19日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：在所有有版本变动的记录后面追加一条字段信息：该信息就是上一个版本的版本号，只限同用户</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * 文件路径：E:\BigData\7_Hadoop\hadoop-mapreduce-day5\data\input_version</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMR</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span></span>&#123;</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="keyword">int</span> run=ToolRunner.run(<span class="keyword">new</span> VersionMR(), args);</div><div class="line">System.exit(run);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="comment">//指定HDFS相关参数</span></div><div class="line">Configuration conf=<span class="keyword">new</span> Configuration();</div><div class="line">FileSystem fs=FileSystem.get(conf);</div><div class="line"> </div><div class="line">Job job=Job.getInstance(conf);</div><div class="line">job.setJarByClass(VersionMR.class);</div><div class="line"></div><div class="line"><span class="comment">//指定mapper类和reduce类</span></div><div class="line">job.setMapperClass(VersionMRMapper.class);</div><div class="line">job.setReducerClass(VersionMRReduce.class);</div><div class="line"></div><div class="line"><span class="comment">//指定输出类型</span></div><div class="line">        job.setMapOutputKeyClass(Version.class);  </div><div class="line">        job.setMapOutputValueClass(NullWritable.class);  </div><div class="line">        job.setOutputKeyClass(Text.class);  </div><div class="line">        job.setOutputValueClass(NullWritable.class); </div><div class="line">        </div><div class="line">        <span class="comment">//指定路径</span></div><div class="line">        Path inputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day5\\data\\input_version"</span>);  </div><div class="line">        Path outputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day5\\data\\output_version"</span>); </div><div class="line">        <span class="comment">//如果输出路径存在则删除</span></div><div class="line">       <span class="keyword">if</span>(fs.exists(outputPath))&#123;</div><div class="line">       fs.delete(outputPath,<span class="keyword">true</span>);</div><div class="line">       &#125;</div><div class="line">       FileInputFormat.setInputPaths(job, inputPath);</div><div class="line">       FileOutputFormat.setOutputPath(job, outputPath);</div><div class="line">       </div><div class="line">       <span class="comment">//提交任务</span></div><div class="line">       <span class="keyword">boolean</span> bool = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">       <span class="keyword">return</span> bool? <span class="number">0</span> : <span class="number">1</span> ;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * map组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Version</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 数据格式：</span></div><div class="line"><span class="comment"> * 20170308,黄渤,光环斗地主,8,360手机助手,0.1版本,北京</span></div><div class="line"><span class="comment"> * 20170308,黄渤,光环斗地主,5,360手机助手,0.1版本,北京</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * 字段信息：</span></div><div class="line"><span class="comment"> * 用户ID，用户名，游戏名，小时，数据来源，游戏版本，用户所在地</span></div><div class="line"><span class="comment"> * id, name, game, hour, source, version, city </span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException ,InterruptedException </span>&#123;</div><div class="line"><span class="comment">//拆分数据</span></div><div class="line">String[] split = value.toString().split(<span class="string">","</span>);</div><div class="line"><span class="comment">//把数据封装到对象中</span></div><div class="line">Version version = <span class="keyword">new</span> Version(split[<span class="number">0</span>], split[<span class="number">1</span>],split[<span class="number">2</span>],Integer.parseInt(split[<span class="number">3</span>]), split[<span class="number">4</span>], split[<span class="number">5</span>], split[<span class="number">6</span>]);</div><div class="line">context.write(version, NullWritable.get());</div><div class="line">&#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * reduce组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">VersionMRReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Version</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line">String lastID=<span class="keyword">null</span>;</div><div class="line">String lastName=<span class="keyword">null</span>;</div><div class="line">String lastVersion=<span class="keyword">null</span>;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Version key, Iterable&lt;NullWritable&gt; values,Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">            <span class="keyword">for</span>(NullWritable nvl:values)&#123;</div><div class="line"><span class="keyword">if</span>(lastVersion==<span class="keyword">null</span>)&#123;</div><div class="line"><span class="comment">//第一次进入程序，lastVersion为空，直接打印，因为相当于没有上一条数据</span></div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()), NullWritable.get());</div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line"><span class="comment">//当ID和Name一致时</span></div><div class="line"><span class="keyword">if</span>(lastID.equals(key.getId()) &amp;&amp; lastName.equals(key.getName()))&#123;</div><div class="line"><span class="comment">//判断上个版本号和当前版本号是否一致</span></div><div class="line"><span class="keyword">if</span>(!lastVersion.equals(key.getVersion()))&#123;</div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()+<span class="string">"-"</span>+lastVersion), NullWritable.get());</div><div class="line">&#125;</div><div class="line"><span class="comment">//当ID和Name有一个不一致时，证明是两个不同的用户</span></div><div class="line">&#125;<span class="keyword">else</span>&#123;</div><div class="line">context.write(<span class="keyword">new</span> Text(key.toString()), NullWritable.get());</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">//进行数据迭代处理，方便本次和下次的数据进行对比</span></div><div class="line">lastID=key.getId();</div><div class="line">lastName=key.getName();</div><div class="line">lastVersion=key.getVersion();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="数值累加"><a href="#数值累加" class="headerlink" title="数值累加"></a>数值累加</h4><p>求所有数对应位置的叠加和</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">举例：</div><div class="line"><span class="number">0001</span>.txt文件有数据：</div><div class="line"><span class="number">1</span></div><div class="line"><span class="number">2</span></div><div class="line"><span class="number">3</span></div><div class="line"><span class="number">4</span></div><div class="line">...</div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">0002</span>.txt文件有数据：</div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line"><span class="number">10</span></div><div class="line">...</div><div class="line">返回结果是：</div><div class="line"><span class="number">1</span><span class="number">1</span></div><div class="line"><span class="number">2</span><span class="number">3</span></div><div class="line"><span class="number">3</span><span class="number">6</span></div><div class="line"><span class="number">4</span><span class="number">10</span></div><div class="line">...</div><div class="line"><span class="number">10</span><span class="number">55</span></div><div class="line">也就是，每一行数字后面都追加一个累加到该数字的和值</div></pre></td></tr></table></figure><p>思路：先求出每个文件的总和，然后按照文件的顺序进行叠加</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;Versions变动版本记录&lt;/li&gt;&lt;li&gt;数值累加&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程框架之Shuffle详述</title>
    <link href="https://prosscode.github.io/archives/2018/03/28/"/>
    <id>https://prosscode.github.io/archives/2018/03/28/</id>
    <published>2018-03-28T10:43:32.000Z</published>
    <updated>2018-06-03T07:16:47.878Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p><img src="/archives/2018/03/28/shuffle.png" alt=""></p><a id="more"></a><h3 id="Shuffle是什麽"><a href="#Shuffle是什麽" class="headerlink" title="Shuffle是什麽"></a>Shuffle是什麽</h3><p>Shuffle：数据混洗，核心机制有数据分区、排序、局部聚合、缓存、拉取、再合并排序。</p><p>Shuffle是MapReduce处理流程中的一个核心过程，由封面图可以看出，它的每一个处理步骤是分散在各个mapTask和ReduceTask节点上完成的，整体分为3个操作：Partitioner（分区，NumReduceTask只有一个或者没有分区操作将不会起作用）、Sort（排序，根据key排序，没有reducer阶段，那么就不会对key排序）、Combiner（合并，局部value的合并，可选组件）</p><h3 id="MapReuce中的Shuffle"><a href="#MapReuce中的Shuffle" class="headerlink" title="MapReuce中的Shuffle"></a>MapReuce中的Shuffle</h3><p>我们先捋一捋MapReduce的执行流程：</p><p><img src="/archives/2018/03/28/mapreduce.png" alt=""></p><p>我们知道，一个大文件需要处理，在HDFS上是以block块的形式存放，Hadop2.x之后的每个block默认为128M，默认副本为3份，运行每个map任务会处理一个split，一般split大小和block相同，那么有多少block就有多少个map任务。</p><p>每个map任务处理完输入的split后会把结果写入到内存的一个环形缓冲区，写入过程中会进行简单的排序，默认大小为100M，当缓冲区的大小使用超过一定的阀值（默认为80%），一个后台的线程就会启动把缓冲区中的数据溢写（spill）到本地磁盘中，同时Mapper继续向环形缓冲区中写入数据。</p><p>数据溢写入到磁盘之前，首先会根据Reducer的数量划分成同数量的分区（默认为HashPartition），每个分区中的数据都会有后台线程根据map任务的输出结果进行内排序（字典数序，自然数顺序或自定义顺序comparator），如果有combiner（作用是使map输出更紧凑，写到本地磁盘和传给reducer的数据更少），Mapper会在溢写到磁盘之前排好序的输出上运行，最后在本地生成分好区排好序的小文件；如果小文件数量达到默认值10（mapreduce.task.io.sort.factor），则会合并成一个大文件，这个结果文件的分区存在一个映射关系，比如 0~1024 字节内容为 0 号分区内容，1025~2048 字节内容为 1 号分区内容；如果map向环形缓冲区写入数据的速度大于向本地写入数据，环形缓冲区就会被写满，向环形缓冲区写入的数据的线程就会阻塞直至缓冲区中的内容全部溢写到磁盘后再次启动，到阀值后会向本地磁盘新建一个溢写文件。</p><p>Reduce任务启动，Reducer个数由mapred-site.xml的mapreduce.job.reducers配置决定，或者初始化job时调用Job.setNumReduceTask来设置；Reducer中的一个线程定期向MRAppMastrer询问Mapper输出结果文件位置，mapper结束后会向MRAppMaster汇报信息；从而Reducer得知Mapper状态，得到map结果文件目录。</p><p>当有一个Mapper结束时，reduce任务进入复制阶段，通过http协议（hadoop内置了netty容器）把所有Mapper结果文件的对应的分区数据复制过来，比如：编号为0的reduce复制maop结果文件中0号分区数据，1号reduce复制map结果文件中1号分区的数据等；Reducer可以并行复制Mappere的结果，默认线程数为5（mapred-site.xml：mapreduce.reduce.shuffle.parallelcoopies）；</p><p>另外：如果map结果文件相当小，则会被直接复制到reduceNodeManager的内存中（缓冲区大小由mapred.site.xml：mapreduce.shuffle.input.buffer.percent指定，默认为0.7），一旦缓冲区达到reduce的阀值大小0.66或写入到reduceNodeManager内存中文件个数达到map输出阀值1000（mapred-site.xml：mapreduce.reduce.merge.inmen.threshold），reduce就会把map结果文件合并溢写到本地。</p><p>复制阶段完成后，Ruducer进入到Merge阶段，循环的合并map结果文件，维持其顺序排列，合并因子默认为10（mapred-site.xml：mapreduce.task.io.start.factor），经过不断的Merge后得到一个”最终文件”，可能存储在磁盘也可能存在内存中。</p><p>“最终文件”输入到reduce进行计算，计算结果输入到HDFS文件系统中存储。</p><p><strong>shuffle流程</strong></p><p><img src="/archives/2018/03/28/shuffle-rim.png" alt=""></p><p>回到map阶段，mapTask 是收集 map()方法输出的 K-V 对，放到内存缓冲区 kvbuffer中（环形缓冲区：内存中的一种首尾相连的数据结构，kvbuffer 包含数据区和索引区）</p><p>从内存缓冲区中的数据区的数据不断溢出本地磁盘文件 file.out，可能会溢出多次，则会有多个文件，相应的内存缓冲区中的索引区数据溢出为磁盘索引文件 file.out.index</p><p>多个溢出文件会被合并成大的溢出文件</p><p>在溢出过程中，及合并的过程中，都要调用 partitoner 进行分区和针对 key 进行排序</p><p>在数据量大的时候，可以对 mapTask 结果启用压缩，将 mapreduce.map.output.compress设为 true，并使用 mapreduce.map.output.compress.codec 设置使用的压缩算法，可以提高数据传输到 reducer 端的效率</p><p>reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区数据，取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文件再进行合并（归并排序）</p><p>合并成大文件后，shuffle 的过程也就结束了，后面进入 reduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法）</p><p>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快</p><blockquote><p>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M</p><p>缓冲区的溢写比也可以通过参数调整，参数：mapreduce.map.sort.spill.percent 默认 0.8</p></blockquote><p><strong>带上shuffle后mapreduce执行流程图</strong></p><p><img src="/archives/2018/03/28/mapreduce-shuffle.png" alt=""></p><p><strong>为什么需要环形缓冲区</strong></p><p>Map过程中环形缓冲区是指数据被map处理之后会先放入内存，内存中的这片区域就是环形缓冲区。数据从内存要写入磁盘中时，数据会被先写入到磁盘缓冲区，磁盘缓冲区满了再把数据写入磁盘。</p><blockquote><p>磁盘缓冲区是为了平滑不同I/O设备的速度差。</p></blockquote><p>磁盘是分区分块存储的。如果是机械硬盘，是分磁道和扇区的。当磁头转到一个扇区的某磁道时，开始读取数据，如果只读取了 100KB 的数据，这时操作系统就想，磁头转到这儿看不容易啊，反正来都来了，顺带多读点数据吧，万一用的着呢。</p><p>所以，读取数据的时候也是通过缓冲区的。</p><blockquote><p>如果应用的数据存放在不同的磁道，不同的扇区，那么读取的效率是很低的，这被称为磁盘碎片，所以 windows 有个操作叫“整理磁盘碎片”。</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;&lt;img src=&quot;/archives/2018/03/28/shuffle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce编程案例（上）</title>
    <link href="https://prosscode.github.io/archives/2018/03/18/"/>
    <id>https://prosscode.github.io/archives/2018/03/18/</id>
    <published>2018-03-18T04:01:05.000Z</published>
    <updated>2018-05-29T13:29:14.280Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><ul><li>单词计数WordCount</li><li>数组排序并加序号</li><li>共同好友</li></ul><a id="more"></a><h4 id="MapRedeuce相关概念"><a href="#MapRedeuce相关概念" class="headerlink" title="MapRedeuce相关概念"></a>MapRedeuce相关概念</h4><p>MapReduce是一个分布式运算程序的编程框架，是用户开发”基于Hadoop的数据分析应用”的核心框架</p><p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上</p><p>FileInputFormat中默认的切片机制：切片大小默认等于block的大小</p><p>用户编写MR程序分为三个部分：Mapper、Reducer、Driver（提交运行MR程序的客户端）</p><p>Mapper的输入数据和输出数据是K-V对的形式，Reducer的输入数据类型对应Mapper的输出类型</p><p>ReduceTask进程对每一组相同K的K-V组调用一次reduce()方法</p><h4 id="单词计数WordCount"><a href="#单词计数WordCount" class="headerlink" title="单词计数WordCount"></a>单词计数WordCount</h4><p>思路：逐行读取文本内容–&gt;把读取到的一行文本内容切割为一个一个的单词–&gt;把每个单词出现一次的信息记录为一个key-value，key=word，value=1–&gt;收集所有相同的单词，然后统计value值的总和</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：例子程序 wordcount</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMR</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"><span class="comment">//指定hdfs相关的参数,没有涉及集群文件可以把set信息注释</span></div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line"><span class="comment">//conf.set("fs.defaultFS", "hdfs://hadoop02:9000");</span></div><div class="line"><span class="comment">//System.setProperty("HADOOP_USER_NAME", "hadoop");</span></div><div class="line">FileSystem fs = FileSystem.get(conf);</div><div class="line"></div><div class="line"><span class="comment">//通过Configuration对象获取job对象，job对象会组织所有的该mapreduce程序所有的各种组件</span></div><div class="line">Job job = Job.getInstance(conf);</div><div class="line"><span class="comment">//设置jar包所在路径，一般即为类名的反射</span></div><div class="line">job.setJarByClass(WordCountMR.class);</div><div class="line"></div><div class="line"><span class="comment">//指定mapper和reducer类</span></div><div class="line">job.setMapperClass(WordCountMRMapper.class);</div><div class="line">job.setReducerClass(WordCountMRReducer.class);</div><div class="line"><span class="comment">//指定mapper和reduce的输入输出类型，如果reducer的输入输出类型和mapper一致可以省略</span></div><div class="line">job.setMapOutputKeyClass(Text.class);</div><div class="line">job.setMapOutputValueClass(IntWritable.class);</div><div class="line">job.setOutputKeyClass(Text.class);</div><div class="line">job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line"><span class="comment">//设置程序的输入路径和输出路径</span></div><div class="line">Path inputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day2\\data\\score\\input"</span>);</div><div class="line">Path outputPath = <span class="keyword">new</span> Path(<span class="string">"E:\\BigData\\7_Hadoop\\hadoop-mapreduce-day2\\data\\score\\output_wordcount"</span>);</div><div class="line">FileInputFormat.setInputPaths(job, inputPath);</div><div class="line"><span class="comment">//判断输出路径是否存在，存在则删除</span></div><div class="line"><span class="keyword">if</span>(fs.exists(outputPath))&#123;</div><div class="line">fs.delete(outputPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job, outputPath);</div><div class="line"></div><div class="line"><span class="comment">//提交任务，布尔值决定要不要将运行进度信息输出给用户</span></div><div class="line"><span class="keyword">boolean</span> isDone = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line"><span class="comment">//主线程根据mapreduce程序的运行结果成功与否决定是否退出</span></div><div class="line">System.exit(isDone ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * mapper组件</span></div><div class="line"><span class="comment"> * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量 </span></div><div class="line"><span class="comment"> * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本 </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMRMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">//定义输出类型，避免重复实例化其对象</span></div><div class="line">Text outKey=<span class="keyword">new</span> Text();</div><div class="line">IntWritable outValue=<span class="keyword">new</span> IntWritable();</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">//切分单词</span></div><div class="line">String[] words=value.toString().split(<span class="string">" "</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span>(String word:words)&#123;</div><div class="line"><span class="comment">//每个单词计数一次，也就是把单词组织成&lt;hello，1&gt;这样的K-V</span></div><div class="line">outKey.set(word);</div><div class="line">outValue.set(<span class="number">1</span>);</div><div class="line">context.write(outKey, outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * reduce组件</span></div><div class="line"><span class="comment"> * 输入类型就是Map阶段的处理结果，输出类型就是Reduce最后的输出 </span></div><div class="line"><span class="comment"> * reducetask将这些收到K-V数据拿来处理时，是这样调用我们的reduce方法的： </span></div><div class="line"><span class="comment"> * 先将自己收到的所有的K-V对按照K分组（根据K是否相同） 将某一组K-V中的第一个K-V中的K传给reduce方法的key变量</span></div><div class="line"><span class="comment"> * 把这一组kv中所有的v用一个迭代器传给reduce方法的变量values </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMRReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">//结果汇总</span></div><div class="line"><span class="keyword">int</span> sum=<span class="number">0</span>;</div><div class="line"><span class="keyword">for</span>(IntWritable v:values)&#123;</div><div class="line">sum+=v.get();</div><div class="line">&#125;</div><div class="line"><span class="comment">//输出</span></div><div class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="数组排序并加序号"><a href="#数组排序并加序号" class="headerlink" title="数组排序并加序号"></a>数组排序并加序号</h4><p>思路：利用shuffle阶段会把K-V对中的key值自动排序功能，先把数组元素放到key中进行排序，然后根据排序的顺序在reduce阶段为元素编上序号</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：数字排序并加序号</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySort</span> </span>&#123;</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span>  number=<span class="number">0</span>;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">Job job = Job.getInstance(conf);</div><div class="line">job.setJarByClass(ArraySort.class);</div><div class="line"></div><div class="line">job.setMapperClass(ArraySortMapper.class);</div><div class="line">job.setReducerClass(ArraySortReducer.class);</div><div class="line"></div><div class="line">job.setOutputKeyClass(IntWritable.class);</div><div class="line">job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/array/input"</span>));</div><div class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/array/output2"</span>));</div><div class="line"></div><div class="line"><span class="keyword">boolean</span> isDone = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">System.exit(isDone ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * Mapper组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"><span class="comment">//定义输出类型，避免重复实例化其对象</span></div><div class="line">IntWritable outKey=<span class="keyword">new</span> IntWritable();</div><div class="line">IntWritable outValue=<span class="keyword">new</span> IntWritable();</div><div class="line">      </div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">           <span class="comment">//去除到元素的前后空格</span></div><div class="line">String tempNumber = value.toString().trim();</div><div class="line"><span class="keyword">int</span> outKeyTemp=Integer.parseInt(tempNumber);</div><div class="line">           outKey.set(outKeyTemp);</div><div class="line">           outValue.set(<span class="number">0</span>);</div><div class="line">context.write(outKey,outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * Ruduce组件</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ArraySortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>,<span class="title">IntWritable</span>, <span class="title">IntWritable</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;IntWritable&gt; values,Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="keyword">for</span>(IntWritable t:values)&#123;</div><div class="line">number++;</div><div class="line">context.write(<span class="keyword">new</span> IntWritable(number),key);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>思考：当本地文件中的数据过大时，这样的排序并加编号的方案是否可行 ?</p><h4 id="求共同好友"><a href="#求共同好友" class="headerlink" title="求共同好友"></a>求共同好友</h4><p>数据格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">A:B,C,D,F,E,O (表示B,C,D,E,F,O是A用户的好友)</div><div class="line">B:A,C,E,K</div><div class="line">C:F,A,D,I</div><div class="line">D:A,E,F,L</div><div class="line">E:B,C,D,M,L</div><div class="line">F:A,B,C,D,E,O,M</div><div class="line">G:A,C,D,E,F</div><div class="line">H:A,C,D,E,O</div><div class="line">I:A,O</div><div class="line">J:B,O</div><div class="line">K:A,C,D</div><div class="line">L:D,E,F</div><div class="line">M:E,F,G</div><div class="line">O:A,H,I,J,K</div><div class="line">...</div></pre></td></tr></table></figure><p>输出格式：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">A-B:C E  (A和B的共同还有是E,C)</div><div class="line">A-C:D F</div></pre></td></tr></table></figure><p>逆推法：结果为A-B:C E，那么key:A-B；value:C E，得知A-B:C；A-B:E，意思是E是A和B的共同好友；继续可以转换A:E，B:E；那么单看A的好友可以拆分为，A:B，A:C，A:D。</p><p>我们整理来一下，从源数据格式推出结果：A:B,C,D,F,E,O / B:A,C,E,K –&gt; A:C,E / B:C,E –&gt;A-B:E / A-B:C –&gt; A-B:C E，这样推理过来就需要两个MR来解决，拆分来写，只需要后一个MR程序调用前一个MR程序的结果即可；多job串联，可以使用JobControl这个类把具有依赖关系的多个job串联起来，然后调度先后执行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> org.pross.friend;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Collections;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> *         create time：2018年3月17日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> *         content：找出共同好友</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend</span></span>&#123;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">Configuration conf1 = <span class="keyword">new</span> Configuration();</div><div class="line">Configuration conf2 = <span class="keyword">new</span> Configuration();</div><div class="line">FileSystem fs = FileSystem.get(conf1);</div><div class="line"></div><div class="line"><span class="comment">// 第一个job任务</span></div><div class="line">Job job1 = Job.getInstance(conf1);</div><div class="line">job1.setJarByClass(CommonFriend.class);</div><div class="line">job1.setMapperClass(CommonFriend1Mapper.class);</div><div class="line">job1.setReducerClass(CommonFriend1Reduce.class);</div><div class="line"></div><div class="line">job1.setMapOutputKeyClass(Text.class);</div><div class="line">job1.setMapOutputValueClass(Text.class);</div><div class="line">job1.setOutputKeyClass(Text.class);</div><div class="line">job1.setOutputValueClass(Text.class);</div><div class="line"></div><div class="line">Path inputPath1 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/input"</span>);</div><div class="line">Path outputPath1 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_merge"</span>);</div><div class="line">FileInputFormat.setInputPaths(job1, inputPath1);</div><div class="line"><span class="keyword">if</span> (fs.exists(outputPath1)) &#123;</div><div class="line">fs.delete(outputPath1, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job1, outputPath1);</div><div class="line"></div><div class="line"><span class="comment">//第二个job任务</span></div><div class="line">Job job2 =Job.getInstance(conf2);</div><div class="line">job2.setMapperClass(CommonFriend2Mapper.class);</div><div class="line">job2.setReducerClass(CommonFriend2Reducer.class);</div><div class="line">job2.setMapOutputKeyClass(Text.class);</div><div class="line">job2.setMapOutputValueClass(Text.class);</div><div class="line">job2.setOutputKeyClass(Text.class);</div><div class="line">job2.setOutputValueClass(Text.class);</div><div class="line">Path inputPath2 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_merge"</span>);</div><div class="line">Path outputPath2 = <span class="keyword">new</span> Path(<span class="string">"E:/BigData/7_Hadoop/hadoop-mapreduce-day2/data/friend/output_lastMerge"</span>);</div><div class="line">FileInputFormat.setInputPaths(job2, inputPath2);</div><div class="line"><span class="keyword">if</span>(fs.exists(outputPath2))&#123;</div><div class="line">fs.delete(outputPath2, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">FileOutputFormat.setOutputPath(job2, outputPath2);</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 多个job串联</span></div><div class="line"><span class="comment"> * 使用JobControl把具有依赖的多个job串联起来</span></div><div class="line"><span class="comment"> */</span></div><div class="line">JobControl control=<span class="keyword">new</span> JobControl(<span class="string">"CommonFriend"</span>);</div><div class="line">ControlledJob ajob=<span class="keyword">new</span> ControlledJob(job1.getConfiguration());</div><div class="line">ControlledJob bjob=<span class="keyword">new</span> ControlledJob(job2.getConfiguration());</div><div class="line"><span class="comment">// bjob的执行依赖ajob</span></div><div class="line">bjob.addDependingJob(ajob);</div><div class="line"></div><div class="line">control.addJob(ajob);</div><div class="line">control.addJob(bjob);</div><div class="line"></div><div class="line">Thread t=<span class="keyword">new</span> Thread(control);</div><div class="line">t.start();</div><div class="line"></div><div class="line"><span class="comment">//等待0.5秒执行下一个</span></div><div class="line"><span class="keyword">while</span>(!control.allFinished())&#123;</div><div class="line">Thread.sleep(<span class="number">500</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line">System.exit(<span class="number">0</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 第一个MR程序</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend1Mapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 数据源格式 </span></div><div class="line"><span class="comment"> * A:B,C,D,F,E,O B:A,C,E,K</span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line">Text outkey = <span class="keyword">new</span> Text();</div><div class="line">Text outValue = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line"><span class="comment">// 拆分</span></div><div class="line">String[] user_friends = value.toString().split(<span class="string">":"</span>);</div><div class="line">String user = user_friends[<span class="number">0</span>];</div><div class="line">String[] friends = user_friends[<span class="number">1</span>].split(<span class="string">","</span>);</div><div class="line"><span class="keyword">for</span> (String friend : friends) &#123;</div><div class="line">outkey.set(friend);</div><div class="line">outValue.set(user);</div><div class="line">context.write(outkey, outValue);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend1Reduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text keyOut = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">ArrayList&lt;String&gt; userList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"><span class="keyword">for</span> (Text t : values) &#123;</div><div class="line">userList.add(t.toString());</div><div class="line">&#125;</div><div class="line">Collections.sort(userList);</div><div class="line"><span class="keyword">int</span> size = userList.size();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size - <span class="number">1</span>; i++) &#123;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; size; j++) &#123;</div><div class="line">String outKey = userList.get(i) + <span class="string">"-"</span> + userList.get(j);</div><div class="line">keyOut.set(outKey);</div><div class="line">context.write(keyOut, key);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 第二个MR程序</span></div><div class="line"><span class="comment"> * 数据格式：</span></div><div class="line"><span class="comment"> * B-CA</span></div><div class="line"><span class="comment"> * B-DA</span></div><div class="line"><span class="comment"> * B-FA</span></div><div class="line"><span class="comment"> * B-GA</span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend2Mapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text keyOut = <span class="keyword">new</span> Text();</div><div class="line">Text valueOut = <span class="keyword">new</span> Text();</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">String[] split = value.toString().split(<span class="string">"\t"</span>);</div><div class="line">keyOut.set(split[<span class="number">0</span>]);</div><div class="line">valueOut.set(split[<span class="number">1</span>]);</div><div class="line">context.write(keyOut, valueOut);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonFriend2Reducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line">Text valueOut = <span class="keyword">new</span> Text();</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span></span></div><div class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</div><div class="line"><span class="keyword">for</span> (Text t : values) &#123;</div><div class="line">sb.append(t.toString()).append(<span class="string">" "</span>);</div><div class="line">&#125;</div><div class="line">String outValueStr = sb.toString();</div><div class="line">valueOut.set(outValueStr);</div><div class="line">context.write(key, valueOut);</div><div class="line">&#125;</div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * 输出数据：</span></div><div class="line"><span class="comment"> * A-BE C </span></div><div class="line"><span class="comment"> * A-CD F </span></div><div class="line"><span class="comment"> * ...</span></div><div class="line"><span class="comment"> */</span></div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;单词计数WordCount&lt;/li&gt;&lt;li&gt;数组排序并加序号&lt;/li&gt;&lt;li&gt;共同好友&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HDFS核心API编程案例</title>
    <link href="https://prosscode.github.io/archives/2018/03/17/"/>
    <id>https://prosscode.github.io/archives/2018/03/17/</id>
    <published>2018-03-17T14:57:38.000Z</published>
    <updated>2018-05-29T12:57:32.688Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><ul><li>删除HDFS集群中所有的空文件和空目录</li><li>使用流的方式上传下载文件</li><li>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</li><li>统计出HDFS文件系统中平均副本数</li></ul><a id="more"></a><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p><a href="https://prosscode.github.io/2018/Hadoop集群环境搭建/">Hadoop集群环境搭建</a>–&gt;将”windows平台编译hadoop安装包”解压，并配置环境变量–&gt;准备hadoop-eclipse-plugin.jar插件，配置到eclipse–&gt;eclipse中进入Map/Reduce Locations配置集群信息–&gt;Add User Library–&gt;添加common、hdfs、mapreduce、yarn相关依赖库–&gt;新建Java项目开始编写代码</p><p>做那么多操作，无非是要做到<strong>在本地eclipse中编写的程序能够操作HDFS集群中的文件</strong></p><h4 id="公共工具类"><a href="#公共工具类" class="headerlink" title="公共工具类"></a>公共工具类</h4><p><code>HDFSUtils.java</code>：初始化FileSystem对象和关闭FileSystem</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Random;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * <span class="doctag">@author</span> pross shawn</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * create time：2018年3月14日</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * content：初始化FileSystem对象和关闭FileSystem</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSUtils</span> </span>&#123;</div><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> FileSystem fs=<span class="keyword">null</span>;</div><div class="line">  </div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 初始化FileSystem对象</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception </span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">Configuration conf=<span class="keyword">new</span> Configuration();</div><div class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop02:9000"</span>);</div><div class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</div><div class="line">fs=FileSystem.get(conf);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 关闭FileSystem的连接</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">fs.close();</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>所需要的依赖包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.FileInputStream;</div><div class="line"><span class="keyword">import</span> java.io.FileOutputStream;</div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.io.OutputStream;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div></pre></td></tr></table></figure><h4 id="删除HDFS集群中所有空文件和空目录"><a href="#删除HDFS集群中所有空文件和空目录" class="headerlink" title="删除HDFS集群中所有空文件和空目录"></a>删除HDFS集群中所有空文件和空目录</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 删除HDFS集群中的所有空文件和空目录</span></div><div class="line"><span class="comment"> * </span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteEmptyDir</span><span class="params">(Path path)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line"><span class="comment">// 当前路径就是空目录时</span></div><div class="line">FileStatus[] listFile = HDFSUtils.fs.listStatus(path);</div><div class="line"><span class="keyword">if</span> (listFile.length == <span class="number">0</span>) &#123;</div><div class="line"><span class="comment">//删除空目录</span></div><div class="line">HDFSUtils.fs.delete(path, <span class="keyword">true</span>);</div><div class="line"><span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//如果不是空文件，先获取指定目录下的文件和子目录</span></div><div class="line">RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = HDFSUtils.fs.listLocatedStatus(path);</div><div class="line"></div><div class="line"><span class="keyword">while</span> (listLocatedStatus.hasNext()) &#123;</div><div class="line">LocatedFileStatus next = listLocatedStatus.next();</div><div class="line">          <span class="comment">//获取当前目录和其父目录</span></div><div class="line">Path currentPath = next.getPath();</div><div class="line">Path parentPath=next.getPath().getParent();</div><div class="line"></div><div class="line"><span class="comment">// 如果是文件夹，继续往下遍历</span></div><div class="line"><span class="keyword">if</span> (next.isDirectory()) &#123;</div><div class="line"></div><div class="line"><span class="comment">// 如果是空目录，删除</span></div><div class="line"><span class="keyword">if</span> (HDFSUtils.fs.listStatus(currentPath).length == <span class="number">0</span>) &#123;</div><div class="line">HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</div><div class="line"><span class="keyword">if</span>(HDFSUtils.fs.listStatus(parentPath).length==<span class="number">0</span>)&#123;</div><div class="line">HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line"><span class="comment">// 不是空目录，那么则重新遍历</span></div><div class="line"><span class="keyword">if</span> (HDFSUtils.fs.exists(currentPath)) &#123;</div><div class="line">AchieveClass.deleteEmptyDir(currentPath);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 如果是文件</span></div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line"><span class="comment">// 获取文件的长度</span></div><div class="line"><span class="keyword">long</span> fileLength = next.getLen();</div><div class="line"><span class="comment">// 当文件是空文件时， 删除</span></div><div class="line"><span class="keyword">if</span> (fileLength == <span class="number">0</span>) &#123;</div><div class="line">HDFSUtils.fs.delete(currentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹，这里需要判断一下</span></div><div class="line"><span class="keyword">int</span> length = HDFSUtils.fs.listStatus(parentPath).length;</div><div class="line"><span class="keyword">if</span>(length == <span class="number">0</span>)&#123;</div><div class="line">HDFSUtils.fs.delete(parentPath, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">     HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="使用流的方式上传下载文件"><a href="#使用流的方式上传下载文件" class="headerlink" title="使用流的方式上传下载文件"></a>使用流的方式上传下载文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 使用流的方式上传文件</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath  上传的本地路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> desPath  上传到HDFS上后的文件名称路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putFileByStream</span><span class="params">(String srcPath,String desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">InputStream in = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(srcPath));</div><div class="line">      <span class="comment">//Path是HDFS上的文件路径</span></div><div class="line">FSDataOutputStream out = HDFSUtils.fs.create(<span class="keyword">new</span> Path(desPath));</div><div class="line">IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</div><div class="line">System.out.println(<span class="string">"put successfully"</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 使用流的方式下载文件 </span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> srcPath HDFS上的下载文件的路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@param</span> desPath 下载到本地的文件路径</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getFileByStream</span><span class="params">(Path srcPath,File desPath)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">FSDataInputStream in=HDFSUtils.fs.open(srcPath);</div><div class="line">OutputStream out=<span class="keyword">new</span> FileOutputStream(desPath);</div><div class="line">IOUtils.copyBytes(in, out,<span class="number">4096</span>,<span class="keyword">true</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"><a href="#统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比" class="headerlink" title="统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比"></a>统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * 统计出 HDFS文件系统中文件大小小于 HDFS集群中的默认块大小的文件占比 </span></div><div class="line"><span class="comment"> * 默认块大小为128MB</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">lessBlockSizeOfFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line">FileStatus[] listStatus = HDFSUtils.fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</div><div class="line"><span class="comment">// 文件总数</span></div><div class="line"><span class="keyword">int</span> count = listStatus.length;</div><div class="line"><span class="comment">// 小于block大小的文件数个数</span></div><div class="line"><span class="keyword">int</span> lessBlock = <span class="number">0</span>;</div><div class="line">      <span class="comment">// 遍历</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; i++) &#123;</div><div class="line">         <span class="comment">//如果文件大小小于128M，这lessBlock+1</span></div><div class="line"><span class="keyword">if</span> (listStatus[i].getLen() &lt;= <span class="number">134217728</span>) &#123;</div><div class="line">lessBlock += <span class="number">1</span>;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">System.out.println(<span class="string">"文件总数量为："</span> + count + <span class="string">"个\n小于默认block的文件数量为："</span> + lessBlock + <span class="string">"个"</span> + <span class="string">"\n文件大小小于默认块大小的文件占比:"</span></div><div class="line">+ (lessBlock*<span class="number">1</span>D / count) * <span class="number">100</span> + <span class="string">"%"</span>);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="统计出HDFS文件系统中平均副本数"><a href="#统计出HDFS文件系统中平均副本数" class="headerlink" title="统计出HDFS文件系统中平均副本数"></a>统计出HDFS文件系统中平均副本数</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * HDFS文件系统中的平均副本数（副本总数/总数据块数）</span></div><div class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">avgRepofBlock</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">HDFSUtils.initFileSystem();</div><div class="line"><span class="comment">// 副本总数</span></div><div class="line"><span class="keyword">int</span> repCount = <span class="number">0</span>;</div><div class="line"><span class="comment">// 数据块总数</span></div><div class="line"><span class="keyword">int</span> blockCount = <span class="number">0</span>;</div><div class="line">     </div><div class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HDFSUtils.fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</div><div class="line"><span class="keyword">while</span> (listFiles.hasNext()) &#123;</div><div class="line">LocatedFileStatus next = listFiles.next();</div><div class="line"><span class="keyword">int</span> BlockNum = next.getBlockLocations().length;</div><div class="line"><span class="keyword">if</span> (BlockNum != <span class="number">0</span>) &#123;</div><div class="line"><span class="keyword">int</span> repNum = next.getReplication();</div><div class="line"><span class="keyword">int</span> oneRepCount = BlockNum * repNum;</div><div class="line">repCount += oneRepCount;</div><div class="line">blockCount += BlockNum;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">System.out.println(<span class="string">"副本总数："</span> + repCount + <span class="string">"\n数据块总数："</span> + blockCount + <span class="string">"\n平均副本数："</span> + repCount*<span class="number">1</span>D / blockCount);</div><div class="line">HDFSUtils.closeFileSystem();</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;删除HDFS集群中所有的空文件和空目录&lt;/li&gt;&lt;li&gt;使用流的方式上传下载文件&lt;/li&gt;&lt;li&gt;统计HDFS文件系统中文件大小小于HDFS集群中默认块大小的文件占比&lt;/li&gt;&lt;li&gt;统计出HDFS文件系统中平均副本数&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HDFS核心设计</title>
    <link href="https://prosscode.github.io/archives/2018/03/11/"/>
    <id>https://prosscode.github.io/archives/2018/03/11/</id>
    <published>2018-03-11T00:43:04.000Z</published>
    <updated>2018-05-29T12:57:58.601Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><ul><li>心跳机制</li><li>安全模式</li><li>副本存放策略</li><li>负载均衡</li></ul><a id="more"></a><h3 id="HDFS相关概念"><a href="#HDFS相关概念" class="headerlink" title="HDFS相关概念"></a>HDFS相关概念</h3><p>HDFS（Hadoop Distributed File System Hadoop）分布式文件系统，主要用来解决海量数据的存储问题</p><p>HDFS中的文件在物理上是分块（block）存储，块的大小可以通过配置参数（dfs.blocksize）来规定，默认在Hadoop2.x版本中是128MB。</p><p>HDFS文件的各个block的存储管理由DataNode节点承担，DataNode是HDFS集群从节点，每一个block都可以在多个DataNode上存储多个副本（副本数量可以通过参数设置dfs.replication，默认是3）。</p><p>HDFS是设计成适应一次写入，多次读出的场景，不支持文件的修改。</p><p>HDFS核心API：Configuration、FileSystem。</p><h3 id="HDFS架构解释"><a href="#HDFS架构解释" class="headerlink" title="HDFS架构解释"></a>HDFS架构解释</h3><p>在上一篇的<a href="https://prosscode.github.io/2018/Hadoop集群环境搭建/">Hadoop集群环境搭建</a>中对集群规划分为：NameNode、DataNode、SecondaryNameNode以及ResourceManager和NodeManager。我们只知道需要这样去规划一个基本的分布式集群，并不知其所以然，那么这里对HDFS部分名词阐释：</p><p>主节点Namenode：掌管文件系统目录树，管理文件系统的元数据（一个block元信息消耗大约150byte的内存），负责保持和分配文件副本的数量，并处理客户端读写的请求。客户端请求访问HDFS都是通过向NameNode申请来进行的。</p><p>从节点DataNode：存储整个集群的所有的数据块，处理真正的数据读写。通过心跳机制定期汇报给NameNode有关block的信息。</p><p>SecondaryNameNode：严格说并不是NameNode备份节点，而是NameNode的助手，主要给NameNode分担压力之用。</p><p>下图可以很好的帮助理解</p><p><img src="/archives/2018/03/11/1.jpg" alt=""></p><h3 id="HDFS核心设计"><a href="#HDFS核心设计" class="headerlink" title="HDFS核心设计"></a>HDFS核心设计</h3><p><strong>心跳机制（HearBeat）</strong></p><p>在上面架构解释中说到，DataNode通过心跳机制定期汇报给NameNode有关block的信息，那么HDFS的心跳机制是什么原理呢？</p><p>我们知道，Hadoop是Master（NameNode、ResourceManager）/Slave（DataNode、NodeManager）结构，Master启动的时候会启动一个IPC（Inter-Process Comunocation，进程通信）server服务，等待Slave的连接；而Slave启动时，会主动连接master的IPC server服务，并且是每隔3秒连接一次master，这个每隔一段时间去连接一次的机智，我们形象的称为<strong>心跳</strong>。</p><p>Slave通过心跳汇报自己的信息给Master，Master也通过心跳给Slave下达命令；NameNode通过心跳得知DataNode的状态，ResourceManager 通过心跳得知 NodeManager 的状态。如果Master长时间都没有收到slave的心跳，就认为slave挂掉了，那么NameNode感知到Data挂掉死亡的时长是怎么计算的呢？</p><p>原理是这样的：DataNode启动好了之后，会专门启动一个线程来负责给NameNode发送心跳数据包，如果整个DataNode没有任何问题，但是仅仅只是当前负责发送信息的数据包线程挂掉了，那么NameNode会发送命名向这个DataNode进行确认，如果第一次没有返回结果，仅且只会检查第二次。如果发送数据包线程没有问题，是DataNode出现了某些问题，就没有DataNode的汇报；HDFS的标准： 如果连续10次没有收到DataNode的汇报，那么NameNode就会认为该DataNode存在宕机的可能。</p><p>这里需要查看hdfs-site.xml配置文件中的两个相关设置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>heartbeat.recheck.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p>那么计算公示就可以出来：<code>timeout=2*heartbeat.recheck.interval+10*dfs.heartbeat.interval</code></p><p>需要注意的是，<code>heartbeat.recheck.interval</code>时间单位是ms（毫秒，默认为5分钟），<code>dfs.heartbeat.interval</code>时间单位是s（秒，默认为3秒）。</p><p><strong>安全模式</strong></p><p>安全模式是HDFS的自我保护数据安全的措施，当NameNode发现集群中的block丢失率（默认为0.999f，可修改dfs.safemode.threshold.pct手动配置）达到一定比例时，NameNode就会进入安全模式，在安全模式下，客户端不能对HDFS上的任何数据进行操作，只能查看元数据信息。</p><p>安全模式常用操作命令：</p><p>强制NameNode退出安全模式：<code>hadoop dfsadmin -safemode leave</code></p><p>进入安全模式：<code>hadoop dfsadmin -safemode enter</code></p><p>查看安全模式状态：<code>hadoop dfsadmin -safemode get</code></p><p>等待安全模式结束：<code>hadoop dfsadmin -safemode wait</code></p><p><strong>副本存放策略</strong></p><p>数据分块存储和副本的存放是保证可靠性和高性能的关键</p><p>副本存放策略说明：HDFS默认的副本数是3，第一个Block副本放在客户端所在的Node里，如果客户端不在集群范围，则第一个Node随机选取（不会选择太满和太忙的Node），第二个副本放置到与第一个节点不同的机架中的其中一个Node，第三个副本在和第二个副本在同一个机架，随机放在不同的Node中。</p><p>下图Block1-3表示三个副本。</p><p><img src="/archives/2018/03/11/replication.png" alt=""></p><p>修改副本数：</p><p>修改集群文件hdfs-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><p>命令设置：命令+副本数+文件夹路径或某个文件路径</p><p><code>bin/hadoop fs -setrep -R 2 /</code></p><p><strong>负载均衡</strong></p><p>节点与节点之间磁盘利用率不平衡是HDFS集群非常容易出现的情况（集群内新增，删除节点，某个节点机器内一个盘存储达到饱和等）当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，数据均衡过程的核心是一个数据均衡算法。进行数据的负载均衡调整必须满足以下原则：</p><ul><li>数据平衡不能导致数据块减少，数据备份的丢失</li><li>管理员可以终止数据平衡进程</li><li>每次移动的数据量以及占用的网络资源必须是可控的</li><li>数据均衡过程中，不能影响NameNode的正常工作</li></ul><p>影响Balancer的几个参数：</p><p><code>- threshold</code>：默认设置是10，参数范围0-100，判断集群是否平衡的阀值，理论上设置的越小，整个集群越平衡</p><p><code>dfs.balance.bandwidthPerSec</code>：默认值是1048576（1M/S），Balancer运行时允许占用的带宽</p><p>用命令设置：</p><p><code>hadoop dfsadmin -setBalanacerBandwidth 10485760</code></p><p><code>sbin/start-balancer.sh -t(threshold) 10%</code></p><p>在hdfs-site.xml配置文件中设置bandwidthPerSec：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.balance.bandwidthPerSec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>负载均衡时的带宽大小设置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;ul&gt;&lt;li&gt;心跳机制&lt;/li&gt;&lt;li&gt;安全模式&lt;/li&gt;&lt;li&gt;副本存放策略&lt;/li&gt;&lt;li&gt;负载均衡&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群环境搭建</title>
    <link href="https://prosscode.github.io/archives/2018/03/03/"/>
    <id>https://prosscode.github.io/archives/2018/03/03/</id>
    <published>2018-03-03T09:33:25.000Z</published>
    <updated>2018-05-29T12:57:04.787Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><ul><li><p>虚拟机（VMware Workstation）</p></li><li><p>Xshell5</p></li><li><p>Linux系统（CentOS-6.7-x86_64-bin-iso）</p></li><li><p>Hadoop编译后的安装包（hadoop-2.7.5-centos-6.7.tar.gz）</p></li><li><p>JDK（jdk-8u73-linux-x64.tar.gz）、</p><a id="more"></a></li></ul><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th></th><th>HDFS</th><th>YRAN</th></tr></thead><tbody><tr><td>Hadoop02</td><td>NameNode+DataNode</td><td>NodeManager</td></tr><tr><td>Hadoop03</td><td>DataNode+SecondaryNameNode</td><td>NodeManager</td></tr><tr><td>Hadoop04</td><td>DataNode</td><td>NodeManager</td></tr><tr><td>Hadoop05</td><td>DataNode</td><td>ResourceManager+NodeManager</td></tr></tbody></table><p>集群共四个节点，HDFS主节点为Hadoop02，YRAN主节点为Hadoop05</p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><strong>集群搭建：Haddoop02、Hadoop03、Hadoop04、Hadoop05</strong></p><ul><li>各个节点必须固定IP地址，并互相配置集群所有的主机映射</li><li>安装JDK，配置SSH免密登录（相互持有对方的公钥，就算是自己也需要持有）</li><li>关闭防火墙，关闭防火墙自动开启（关系到web管理页面是否能访问成功）</li><li>除root用户外，统一增加用户名：hadoop</li><li>可以配置一个节点后，克隆其余三个节点</li></ul><p><strong>解压Hadoop安装包，这里指定路径：/home/hadoop/apps/hadoop-2.7.5</strong></p><ul><li><p>修改hadoop的环境变量：普通用户（~/etc/.bashrc），root用户（/etc/profile）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</div><div class="line">  </div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div></pre></td></tr></table></figure></li></ul><p><strong>配置Hadoop配置文件（见下方详情）</strong></p><ul><li>先配置一个节点中的配置文件，然后通过scp分发到其余的节点</li><li>所有节点的Hadoop安装路径和配置文件必须一致</li></ul><p><strong>分发安装包</strong></p><p><code>scp</code>命令，需要配置SSH</p><p><strong>启动Hadoop集群</strong></p><ul><li>初始化</li><li>启动HDFS</li><li>启动YARN</li></ul><p><strong>检测验证是否成功</strong></p><ul><li>JPS命令查看各个节点进程</li><li>查看集群状态：<code>hdfs dfsadmin -report</code> 、<code>hadoop dfsadmin -report</code></li><li>HDFSweb管理页面：<a href="https://hadoop02:50070" target="_blank" rel="external">https://hadoop02:50070</a></li><li>YARNweb管理页面：<a href="https://hadoop05:8088" target="_blank" rel="external">https://hadoop05:8088</a></li></ul><h3 id="修改Hadoop配置文件"><a href="#修改Hadoop配置文件" class="headerlink" title="修改Hadoop配置文件"></a>修改Hadoop配置文件</h3><p>Hadoop配置文件需要修改六个，路径在：hadoop-2.7.5/etc/hadoop/</p><p><img src="/archives/2018/03/03/conf.png" alt=""></p><p><strong>hadoop-env.sh</strong></p><ul><li><p>默认的JAVA_HOME变量，建议修改JAVA_HOME的路径为jdk的原始路径</p><p><img src="/archives/2018/03/03/java.png" alt="hadoop-env.sh"></p></li></ul><p><strong>core-site.xml</strong></p><ul><li><p>添加hdfs配置路径，文件上传端口，临时文件存放的目录等</p><p><img src="/archives/2018/03/03/core.png" alt="core-site.xml"></p></li></ul><p><strong>hdfs-site.xml</strong></p><ul><li><p>namenode、datanode数据存储的目录，数据备份副本的个数，以及第二主节点</p><p><img src="/archives/2018/03/03/hdfs.png" alt="hdfs-site.xml"></p></li></ul><p><strong>mapred-site.xml</strong></p><ul><li><p>配置名mapreduce-yarn管理</p><p><img src="/archives/2018/03/03/mapred.png" alt="mapred-site.xml"></p></li></ul><p><strong>yarn-site.xml</strong></p><ul><li><p>yarn的主机名等</p><p><img src="/archives/2018/03/03/yarn.png" alt="yarn-site.xml"></p></li></ul><p><strong>slaves</strong></p><ul><li><p>集群的节点列表。slaves文件中配置的是DataNode的所在节点服务，方便Hadoop启动时去寻找当前集群的节点，从而命令对应的服务器启动对应的进程</p><p><img src="/archives/2018/03/03/slaves.png" alt="slaves"></p></li></ul><h3 id="分发"><a href="#分发" class="headerlink" title="分发"></a>分发</h3><p>通过<code>scp</code>，命令，将hadoop-2.7.5安装包分发到各个节点的相同位置上；</p><p><code>scp local_file remote_username@remote_ip:remote_folder</code>，<code>-r</code>递归复制</p><p>例：<code>scp -r /apps/hadoop-2.7.5 hadoop@hadoo02:~/apps/</code></p><h3 id="启动Hadoop集群"><a href="#启动Hadoop集群" class="headerlink" title="启动Hadoop集群"></a>启动Hadoop集群</h3><ul><li><p>初始化只能在主节点中进行：<code>（/home/hadoop/apps/hadoop-2.7.5/）bin/hadoop namenode -format</code></p></li><li><p>哪个节点启动HDFS均可：<code>（/home/hadoop/apps/hadoop-2.7.5/）sbin/start-dfs.sh</code></p></li><li><p>YARN启动必须在主节点：<code>（/home/hadoop/apps/hadoop-2.7.5/）sbin/start-yarn.sh</code></p></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;h3 id=&quot;前期准备&quot;&gt;&lt;a href=&quot;#前期准备&quot; class=&quot;headerlink&quot; title=&quot;前期准备&quot;&gt;&lt;/a&gt;前期准备&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;虚拟机（VMware Workstation）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Xshell5&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Linux系统（CentOS-6.7-x86_64-bin-iso）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hadoop编译后的安装包（hadoop-2.7.5-centos-6.7.tar.gz）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;JDK（jdk-8u73-linux-x64.tar.gz）、&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop生态体系</title>
    <link href="https://prosscode.github.io/archives/2018/02/27/"/>
    <id>https://prosscode.github.io/archives/2018/02/27/</id>
    <published>2018-02-27T03:49:58.000Z</published>
    <updated>2018-05-29T12:56:58.436Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><h2 id="Hadoop体系架构图"><a href="#Hadoop体系架构图" class="headerlink" title="Hadoop体系架构图"></a>Hadoop体系架构图</h2><a id="more"></a><p><strong>hadoop1.0</strong></p><p><img src="/archives/2018/02/27/hadoop1.png" alt="hadoop1.x"></p><p><strong>hadoop2.0</strong></p><p><img src="/archives/2018/02/27/hadoop2.png" alt="hadoop2.x"></p><p><strong>演变过程</strong></p><p><img src="/archives/2018/02/27/hadoop1to2.png" alt="hadoop1to2"></p><p><strong>Hadoop生态系统部分组件导图</strong></p><p><img src="/archives/2018/02/27/hadoopall.png" alt="hadoopAll"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;Hadoop体系架构图&quot;&gt;&lt;a href=&quot;#Hadoop体系架构图&quot; class=&quot;headerlink&quot; title=&quot;Hadoop体系架构图&quot;&gt;&lt;/a&gt;Hadoop体系架构图&lt;/h2&gt;
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://prosscode.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Merge Two Sorted Lists(#21)</title>
    <link href="https://prosscode.github.io/archives/2018/01/21/"/>
    <id>https://prosscode.github.io/archives/2018/01/21/</id>
    <published>2018-01-21T12:38:05.000Z</published>
    <updated>2018-04-04T08:34:57.099Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>LeetCode编号：21</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.</p><p><strong>Example</strong></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4</div><div class="line">Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4</div></pre></td></tr></table></figure><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"><span class="comment"> * Definition for singly-linked list.</span></div><div class="line"><span class="comment"> * public class ListNode &#123;</span></div><div class="line"><span class="comment"> *     int val;</span></div><div class="line"><span class="comment"> *     ListNode next;</span></div><div class="line"><span class="comment"> *     ListNode(int x) &#123; val = x; &#125;</span></div><div class="line"><span class="comment"> * &#125;</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">mergeTwoLists</span><span class="params">(ListNode l1, ListNode l2)</span> </span>&#123;</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：21&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://prosscode.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Vaild Parentheses(#20)</title>
    <link href="https://prosscode.github.io/archives/2018/01/16/"/>
    <id>https://prosscode.github.io/archives/2018/01/16/</id>
    <published>2018-01-16T02:16:51.000Z</published>
    <updated>2018-04-04T08:34:57.127Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>LeetCode编号：20</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Given a string containing just the characters <code>&#39;(&#39;</code>, <code>&#39;)&#39;</code>, <code>&#39;{&#39;</code>, <code>&#39;}&#39;</code>, <code>&#39;[&#39;</code> and <code>&#39;]&#39;</code>, determine if the input string is valid.</p><p>The brackets must close in the correct order, <code>&quot;( )&quot;</code> and <code>&quot;( )[ ]{ }&quot;</code> are all valid but <code>&quot;( ]&quot;</code> and <code>&quot;( [ ) ]&quot;</code> are not.</p><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">      </div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>题目要求</strong></p><p>根据LeetCode给出的题目说明：给出一个字符串包含一对小括号、中括号、大括号。需要判断是否是按照正确的的顺序排列的。即需要括号需要成对的在一起。</p><p><strong>解题思路</strong></p><p>可以通过截取字符串后压入栈中，然后做连续索引下的值是否比较进行判断。也可以通过字符串转化为字符数组，利用栈类的特性做匹配，进而做出判断。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>Solution one</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">        Stack&lt;Integer&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</div><div class="line">            <span class="keyword">int</span> q = <span class="string">"()&#123;&#125;[]"</span>.indexOf(s.substring(i, i + <span class="number">1</span>));</div><div class="line">            <span class="keyword">if</span>(q % <span class="number">2</span> == <span class="number">1</span>) &#123;</div><div class="line">                <span class="keyword">if</span>(p.isEmpty() || stack.pop() != q - <span class="number">1</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                stack.push(q);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> stack.isEmpty();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>Solution two</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">(String s)</span> </span>&#123;</div><div class="line">Stack&lt;Character&gt; stack = <span class="keyword">new</span> Stack&lt;Character&gt;();</div><div class="line"><span class="keyword">for</span> (<span class="keyword">char</span> c : s.toCharArray()) &#123;</div><div class="line"><span class="keyword">if</span> (c == <span class="string">'('</span>)&#123;</div><div class="line">            stack.push(<span class="string">')'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">'&#123;'</span>)&#123;</div><div class="line">            stack.push(<span class="string">'&#125;'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (c == <span class="string">'['</span>)&#123;</div><div class="line">            stack.push(<span class="string">']'</span>);</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> ((stack.isEmpty() || stack.pop() != c))&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> stack.isEmpty();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Stack中的两个方法：</p><ul><li>pop()：移除堆栈顶部的对象，并作为此函数的值返回该对象</li><li>push(E item)：将一个项目推到这个堆栈的顶部</li><li>栈是Vector的一个子类，它实现了一个标准的后进先出的栈</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：20&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://prosscode.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Palindrome Number(#9)</title>
    <link href="https://prosscode.github.io/archives/2018/01/02/"/>
    <id>https://prosscode.github.io/archives/2018/01/02/</id>
    <published>2018-01-02T12:50:47.000Z</published>
    <updated>2018-04-04T08:34:57.107Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>LeetCode编号：9</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Determine whether an integer is a palindrome. Do this without extra space.</p><p><strong>Some hints:</strong></p><p>Could negative integers be palindromes? (ie, -1)</p><p>If you are thinking of converting the integer to string, note the restriction of using extra space.</p><p>You could also try reversing an integer. However, if you have solved the problem “Reverse Integer”, you know that the reversed integer might overflow. How would you handle such case?</p><p>There is a more generic way of solving this problem.</p><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>题目要求</strong></p><p>题目要求很简单，判断是否为回文数。</p><p>LeetCode给出了提示：需要判断当<code>x</code>是负数、超过int的范围、反转后超过int的范围这三种情况。</p><p><strong>解题思路</strong></p><p>依照上次反转数字一样，当<code>x</code>是负数和超过int范围时，我们返回<code>false</code>，那么怎么解决反转后超过int类型范围呢，这里考虑用<code>long</code>来接收<code>x</code>的值再进行判断，那么第一种解法就出来了。</p><p>继续思考，反转的方式有哪些？第一种，利用StringBuffer的reverse()反转。第二种，采用数学的方式直接取模赋值，注意当<code>x</code>的值是<code>10</code>的倍数时判断。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>Solution one</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">       Long l=(<span class="keyword">long</span>) x;</div><div class="line">       <span class="keyword">if</span> (l&gt; Integer.MAX_VALUE || l&lt;<span class="number">0</span>) &#123;</div><div class="line">          <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">       &#125; <span class="keyword">else</span> &#123;</div><div class="line">         <span class="keyword">int</span> rex=Math.abs(x);</div><div class="line">         String str = Integer.toString(rex);</div><div class="line">         StringBuffer strb = <span class="keyword">new</span> StringBuffer(str);</div><div class="line">         strb.reverse();</div><div class="line">         Long re = Long.valueOf(strb.toString());</div><div class="line">         <span class="keyword">if</span> (re == rex) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">         &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>Solution two</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">       <span class="keyword">if</span> (x&lt;<span class="number">0</span> || (x!=<span class="number">0</span> &amp;&amp; x%<span class="number">10</span>==<span class="number">0</span>)) <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">       <span class="keyword">int</span> rev = <span class="number">0</span>;</div><div class="line">       <span class="keyword">while</span> (x&gt;rev)&#123;</div><div class="line">         rev = rev*<span class="number">10</span> + x%<span class="number">10</span>;</div><div class="line">         x = x/<span class="number">10</span>;</div><div class="line">       &#125;</div><div class="line">       <span class="keyword">return</span> (x==rev || x==rev/<span class="number">10</span>);</div><div class="line">     &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>第一种方式执行的时间明显比第二种方式长</li><li>在第二种解决办法中<code>while</code>循环进行反转后赋值的方法，以及<code>while</code>条件的判断，极大且有效的降低了0(n)</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：9&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://prosscode.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Reverse Integer(#7)</title>
    <link href="https://prosscode.github.io/archives/2017/12/24/"/>
    <id>https://prosscode.github.io/archives/2017/12/24/</id>
    <published>2017-12-24T07:06:11.000Z</published>
    <updated>2018-04-04T08:34:57.119Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>LeetCode编号：7</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Given a 32-bit signed integer, reverse digits of an integer.</p><p><strong>Example 1</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input： 123</div><div class="line">Output： 321</div></pre></td></tr></table></figure><p><strong>Example 2</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input：-123</div><div class="line">Output：-321</div></pre></td></tr></table></figure><p><strong>Example 3</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input：120</div><div class="line">Output：21</div></pre></td></tr></table></figure><p><strong>note</strong></p><p>Assume we are dealing with an environment which could only hold integers within the 32-bit signed integer range. For the purpose of this problem, assume that your function returns 0 when the reversed integer overflows.</p><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span></span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverse</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>数字的反转，需要考虑并处理负数、个位为0以及反转后数值超过int类型的赋值范围的问题。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverse</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line"><span class="keyword">int</span> m;</div><div class="line"><span class="keyword">long</span> sum = <span class="number">0</span>;</div><div class="line"><span class="keyword">while</span> (x != <span class="number">0</span>) &#123;</div><div class="line">m = x % <span class="number">10</span>;</div><div class="line">sum = sum * <span class="number">10</span> + m;</div><div class="line">x = x / <span class="number">10</span>;</div><div class="line">&#125;</div><div class="line">        <span class="keyword">if</span>(sum&gt;Integer.MAX_VALUE || sum&lt;Integer.MIN_VALUE)&#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">        &#125;</div><div class="line"><span class="keyword">return</span> (<span class="keyword">int</span>)sum;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：7&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://prosscode.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode：Two Sum(#1)</title>
    <link href="https://prosscode.github.io/archives/2017/12/17/"/>
    <id>https://prosscode.github.io/archives/2017/12/17/</id>
    <published>2017-12-17T05:26:33.000Z</published>
    <updated>2018-04-04T08:34:57.123Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><p>LeetCode编号：1</p><a id="more"></a><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>Given an array of integers, return <strong>indices</strong> of the two numbers such that they add up to a specific target.</p><p>You may assume that each input would have <strong>exactly</strong> one solution, and you may not use the <em>same</em> element twice.</p><p><strong>Example:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Given nums = [2, 7, 11, 15], target = 9,</div><div class="line"></div><div class="line">Because nums[0] + nums[1] = 2 + 7 = 9,</div><div class="line">return [0, 1].</div></pre></td></tr></table></figure><p><strong>Code Format</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</div><div class="line">      </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>题目要求</strong></p><p>给定int型的数组nums，int型的target，该函数要求返回一个int型的数组。返回的数组是nums数组中两个元素的下标索引组成，该两个元素相加的和等于target，且一个元素只能使用一次。</p><p><strong>解题思路</strong></p><p>数组nums和值target已经确定，我们需要从nums中找出两个元素出来，且这两个元素的值和target相等。由此可以想到遍历数组两次，取出的元素和target进行比较。但是数组内的元素只能使用一次，那么我们需要去重后取出符合的元素，组合成新的数组并返回。这样逻辑思路就可以出来了。</p><p>继续思考，遍历两遍数组是不是增大了时间复杂度。还有没有和数组相类似的特性，可以解决找出其元素对应其数组索引的问题呢。答案是有的：集合。我们定义一个数组存放最终的返回值，定义一个集合来寻找符合的元素的索引值，这样就避免了再一次的循环数组。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><strong>Solution one</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] twoSum1(<span class="keyword">int</span> nums,<span class="keyword">int</span> target)&#123;</div><div class="line">  <span class="keyword">int</span>[] twoNum=<span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">2</span>];</div><div class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.length;i++)&#123;</div><div class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;nums.length;j++)&#123;</div><div class="line">        <span class="comment">//索引值不相等且其对应的元素值的和和target值相等，即元素符合题目要求</span></div><div class="line">          <span class="keyword">if</span>(i!=j &amp;&amp; (num[i]+num[j])==target)&#123;</div><div class="line">            twoNum[<span class="number">0</span>]=i;</div><div class="line">            twoNum[<span class="number">1</span>]=j;</div><div class="line">          &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> twoNum;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>Solution two</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] twoSum2(<span class="keyword">int</span> nums,<span class="keyword">int</span> target)&#123;</div><div class="line">  <span class="comment">//数组返回值，集合取出值</span></div><div class="line">  <span class="keyword">int</span>[] result=<span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">2</span>];</div><div class="line">  Map&lt;Integer,Integer&gt; map=<span class="keyword">new</span> HashMap&lt;Integer,Integer&gt;();</div><div class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.length;i++)&#123;</div><div class="line">    <span class="comment">//如果集合中存在某个元素+nums[i]=target ，进if循环</span></div><div class="line">      <span class="keyword">if</span>(map.containsKey(target-nums[i]))&#123;</div><div class="line">         result[<span class="number">1</span>]=i;</div><div class="line">         result[<span class="number">0</span>]=(<span class="keyword">int</span>)map.get(target-num[i])-<span class="number">1</span>;</div><div class="line">         <span class="keyword">return</span> result;</div><div class="line">       &#125;</div><div class="line">    <span class="comment">//把nums数组内元素传到集合内,对应键和值</span></div><div class="line">     map.put(num[i],i+<span class="number">1</span>);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>两种方式在时间复杂度上区别还是很大的，在Submit Solution后，集合方式的Runtime时间大大减少。这里用到了集合中的containsKey()方法，get()方法，put()方法。</p><p>get(Object key)：返回指定键映射到的值，如果此映射不包含键映射，则返回null。</p><p>containsKey(Object key)：如果此映射包含指定键的映射，则返回true。</p><p>put(K key , V value)：将指定的值与此映射中指定的键关联。即存储元素在集合中。</p><p>for循环遍历数组，初始化数组。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;LeetCode编号：1&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://prosscode.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>Linux下创建和删除用户</title>
    <link href="https://prosscode.github.io/archives/2017/11/27/"/>
    <id>https://prosscode.github.io/archives/2017/11/27/</id>
    <published>2017-11-27T12:40:56.000Z</published>
    <updated>2018-05-29T12:58:05.697Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><blockquote><p>Linux_CentOS7环境</p><p>Linux内置终端</p><p>管理员Root权限</p></blockquote><a id="more"></a><h3 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h3><p>在学习Linux基本命令操作过程中，在终端使用<code>useradd</code>命令可用来建立用户账号，用<code>passwd</code>设定账号的密码，可用userdel删除账号。不考虑缓冲天数，群组，备注，登入目录等设置。具体步骤如下：</p><ul><li><code># useradd maxpross</code> - 创建一个名字为maxpross的用户</li><li><code># ls /home</code> - 查看home目录下是否创建</li></ul><ul><li><code># userdel maxpross</code> - 删除用户</li><li><code># useradd maxpross</code> - 查看是否能重新创建用户</li></ul><p>然而事实并不能如我们所想的那样。</p><p><img src="/archives/2017/11/27/01.png" alt=""></p><p>useradd警告： 此目录已经存在。</p><h3 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a>思考问题</h3><p>既然useradd是添加用户，userdel是删除目录，那么为什么不能按照我们的操作的预想创建成功呢，哪里有不对的地方呢？既然是已经存在 此目录，那是不是问题出现在没有删除干净的原因呢，我们来查找一下涉及到<code>maxpross</code>名字的文件有哪些：</p><p><code># find / -name &#39;*maxpross*&#39;</code></p><p>结果如下：</p><p><img src="/archives/2017/11/27/02.png" alt=""></p><p>那我们终于找到问题所在了，在/home目录和/var/spool/mail邮件池中存在着maxpross有关文件，<code>userdel</code>删除只是删除用户帐号，而没有删除相关文件，那么下次继续创建时候就会就会涉及到相关文件，系统就会提示，文件已经存在。</p><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>既然问题找到了，我们就需要来解决问题。先前我们已经删除了用户账号，那我们就手动删除查找出来的路径下的有关文件后，再执行添加操作。</p><p><code># rm -rf /var/spool/mail/maxpross /home/maxpross</code></p><p><code># useradd maxpross</code></p><p><code># ls /home</code></p><p><img src="/archives/2017/11/27/03.png" alt=""></p><p>创建成功，圆满解决。</p><h3 id="拓展问题"><a href="#拓展问题" class="headerlink" title="拓展问题"></a>拓展问题</h3><p>通过查找资料，我们知道<code>userdel</code>有一个参数<code>-r</code>：删除用户登入目录以及目录中所有文件，这下就很好解决用户的创建和删除的问题了！</p><p><code># userdel -r maxpross</code></p><p><code># useradd maxpross</code></p><p><img src="/archives/2017/11/27/04.png" alt=""></p><p>大家发现更好的解决办法欢迎联系我一起交流！</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;blockquote&gt;&lt;p&gt;Linux_CentOS7环境&lt;/p&gt;&lt;p&gt;Linux内置终端&lt;/p&gt;&lt;p&gt;管理员Root权限&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://prosscode.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hibernate核心配置文件总结</title>
    <link href="https://prosscode.github.io/archives/2017/10/16/"/>
    <id>https://prosscode.github.io/archives/2017/10/16/</id>
    <published>2017-10-16T13:14:38.000Z</published>
    <updated>2018-05-29T12:57:41.426Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --><h2 id="Hibernate核心配置文件步骤"><a href="#Hibernate核心配置文件步骤" class="headerlink" title="Hibernate核心配置文件步骤"></a>Hibernate核心配置文件步骤</h2><ul><li><p>引入约束</p></li><li><p>连接数据库（重点）</p></li><li><p>Hibernate中其他配置</p></li><li><p>引入映射文件（必须）</p><a id="more"></a></li></ul><p>按照hibernate核心文件配置的顺序是以上的排序。我现在以步骤的重要的先后顺序来按点讲解，顺便也当自己复习和回顾。</p><h3 id="连接数据库"><a href="#连接数据库" class="headerlink" title="连接数据库"></a>连接数据库</h3><p>现在主要使用三种数据库：<a href="#mysql">MySQL</a>，<a href="#oracle">Oracle</a>，<a href="#sqlserver">SQL Server</a>。</p><h4 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;hibernate-configuration&gt;</div><div class="line">&lt;session-factory&gt;</div><div class="line">&lt;!--</div><div class="line">加载驱动</div><div class="line">连接数据库</div><div class="line">数据库用户名</div><div class="line">数据库密码</div><div class="line">  --&gt;</div><div class="line">&lt;property name="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/property&gt;</div><div class="line">&lt;property name=<span class="string">"hibernate.connection.url"</span>&gt;jdbc:mysql:<span class="comment">//localhost:3306/DataBaseName&lt;/property&gt;</span></div><div class="line">&lt;property name="hibernate.connection.username"&gt;mysqlusername&lt;/property&gt;</div><div class="line">&lt;property name="hibernate.connection.password"&gt;mysqlpassword&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;/session-factory&gt;</div><div class="line">&lt;/hibernate-configuration&gt;</div></pre></td></tr></table></figure><h4 id="Oracle"><a href="#Oracle" class="headerlink" title="Oracle"></a>Oracle</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;hibernate-configuration&gt;</div><div class="line">&lt;session-factory&gt;</div><div class="line"></div><div class="line">&lt;property name="hibernate.connection.driver_class"&gt;oracle.jdbc.driver.OracleDriver&lt;/property&gt;</div><div class="line">&lt;property name=<span class="string">"hibernate.connection.url"</span>&gt;</div><div class="line">jdbc:oracle:thin:@localhost:1521:dataBaseName&lt;/property&gt;</div><div class="line">&lt;property name="hibernate.connection.username"&gt;username&lt;/property&gt;</div><div class="line">&lt;property name="hibernate.connection.password"&gt;password&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;/session-factory&gt;</div><div class="line">&lt;/hibernate-configuration&gt;</div></pre></td></tr></table></figure><h4 id="SQL-Server"><a href="#SQL-Server" class="headerlink" title="SQL Server"></a>SQL Server</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&lt;hibernate-configuration&gt;</div><div class="line">&lt;session-factory&gt;</div><div class="line"></div><div class="line">&lt;property name=<span class="string">"hibernate.connection.driver_class"</span>&gt;</div><div class="line">com.microsoft.sqlserver.jdbc.SQLServerDriver</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property name=<span class="string">"hibernate.connection.url"</span>&gt;jdbc:sqlserver:<span class="comment">//localhost:1433;databaseName="xx"</span></div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property name="hibernate.connection.username"&gt;username&lt;/property&gt;</div><div class="line">&lt;property name="hibernate.connection.password"&gt;password&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;/session-factory&gt;</div><div class="line">&lt;/hibernate-configuration&gt;</div></pre></td></tr></table></figure><blockquote><p>官方给出配置信息可查看hibernate-release\project\etc内的hibernate.properties文件</p></blockquote><h3 id="引入映射文件"><a href="#引入映射文件" class="headerlink" title="引入映射文件"></a>引入映射文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;mapping resource=<span class="string">"org/itcast/entity/User.hbm.xml"</span>/&gt;</div></pre></td></tr></table></figure><blockquote><p>org.itcast.entity：包名。User.hbm.xml：映射文件名。</p></blockquote><h3 id="引入约束"><a href="#引入约束" class="headerlink" title="引入约束"></a>引入约束</h3><p>在配置文件头引入dtd约束</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;!DOCTYPE hibernate-configuration PUBLIC</div><div class="line"><span class="string">"-//Hibernate/Hibernate Configuration DTD 3.0//EN"</span></div><div class="line"><span class="string">"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"</span>&gt;</div></pre></td></tr></table></figure><h3 id="Hibernate中其他配置"><a href="#Hibernate中其他配置" class="headerlink" title="Hibernate中其他配置"></a>Hibernate中其他配置</h3><ul><li>输出底层sql语句并格式化</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&lt;property name=&quot;hibernate.show_sql&quot;&gt;true&lt;/property&gt;</div><div class="line">&lt;property name=&quot;hibernate.format_sql&quot;&gt;true&lt;/property&gt;</div></pre></td></tr></table></figure><ul><li>hibernate创建表</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//update：如果已经有表则更新，如果没有则创建</span></div><div class="line">&lt;property name="hibernate.hbm2ddl.auto"&gt;update&lt;/property&gt;</div></pre></td></tr></table></figure><ul><li>配置数据库方言</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// SQL Server </span></div><div class="line">&lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.SQLServerDialect&lt;/property&gt;</div><div class="line"><span class="comment">//MySQL三种：MySQLDialect、MySQLInnoDBDialect、MySQLMyISAMDialect</span></div><div class="line">&lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt;</div><div class="line"><span class="comment">//Oracle三种：Oracle8iDialect、Oracle9iDialect、Oracle10gDialect</span></div><div class="line">&lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.Oracle8iDialect&lt;/property&gt;</div></pre></td></tr></table></figure><blockquote><p>数据库方言：在数据库里面相同的功可能是需要不同的关键字才能实现。比如：实现分页，mysql关键字是limit，oracle是rownum。为了区别并执行你写的底层sql语句，需要配置数据库相应的方言</p></blockquote><hr><p>简单的介绍一下这三种数据库：</p><p><span id="mysql">MySQL，最初的核心思想主要是开源、简便、易用。经过几次改版之后蜕变成一个成熟的关系型数据库系统，由于MySQL的早期定位，其主要应用场景就是互联网开发。基本上，互联网的爆发成就了MySQL，LAMP架构风靡天下。</span></p><p><span id="oracle">Oracle，基于运算架构了一种新型的数据存储模型，基于这种模型Oracle成为了一个非常典型的关系数据库，其主要特点是结构严谨、高性能、高可用。由于诞生的早，使其在传统数据库应用中大杀四方，金融、通信、能源、运输、零售、制造等各个行业的大型公司基本都是用了Oracle。</span></p><p><span id="sqlserver">SQL Server，一般是指MS SQL Server，其最大的优势在于集成了MS公司各类产品及资源，提供了强大了可视化界面、高度集成的管理工具。MS SQL Server是MS公司在软件集成方案中的重要一环，也为WIN系统在企业级应用中的普及做出了很大贡献。</span></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Aug 26 2018 22:41:59 GMT+0800 (中国标准时间) --&gt;&lt;h2 id=&quot;Hibernate核心配置文件步骤&quot;&gt;&lt;a href=&quot;#Hibernate核心配置文件步骤&quot; class=&quot;headerlink&quot; title=&quot;Hibernate核心配置文件步骤&quot;&gt;&lt;/a&gt;Hibernate核心配置文件步骤&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;引入约束&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;连接数据库（重点）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hibernate中其他配置&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;引入映射文件（必须）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://prosscode.github.io/tags/Java/"/>
    
  </entry>
  
</feed>
